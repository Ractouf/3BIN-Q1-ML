{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJ0lRTHxtm9O"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6dPWqajUAGe"
      },
      "source": [
        "#Théorie\n",
        "\n",
        "##Vidéos obligatoires\n",
        "Nous vous demandons de regarder et de comprendre les vidéos suivantes :\n",
        "\n",
        "1.   [BINV3100 2 IA 1 Introduction au réseau de neurones et deep-learning](https://www.youtube.com/watch?v=CNJ942E7UyQ)\n",
        "2.   [BINV3100 2 IA 2 Réseau de neurones](https://www.youtube.com/watch?v=FIfByp7qc90)\n",
        "3.   [BINV3100 2 IA 3 Régression sur PyTorch](https://www.youtube.com/watch?v=Fnjj2QBpUM4)\n",
        "4.   [BINV3100 2 IA 3 Classification sur PyTorch](https://www.youtube.com/watch?v=cOMagXizAtk)\n",
        "5.   [BINV3100 2 IA 5 Enoncé exercices](https://www.youtube.com/watch?v=Y95L3CJZ5TQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2RydpwBVrP5"
      },
      "source": [
        "#Partie 1: Exercices relatifs aux Vidéos\n",
        "\n",
        "[BINV3100 2 IA 5 Enoncé exercices](https://www.youtube.com/watch?v=Y95L3CJZ5TQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT3fRSJH-SB1"
      },
      "source": [
        "**Nous utilisons pas encore le GPU, il ne faut pas changer le runtime**\n",
        "\n",
        "Merci de commenter un maximum votre code.\n",
        "\n",
        "Objectifs :\n",
        "\n",
        "Exercice 1 (sur 4):\n",
        "- Layer ok et bons paramètres\n",
        "- Bonne forward pass / calcul de loss / retropropagation du gradient\n",
        "- Bonne accuracy sur le test (100% escompté)\n",
        "- Divers (bonne gestion de type, bonne création de batch, bon arrêt d'entrainement, tracking de la training loss)\n",
        "\n",
        "\n",
        "Exercice 2 (sur 2) :\n",
        "- Bonne modification du réseau\n",
        "- Bonne accuracy sur le test (100% escompté)\n",
        "\n",
        "Exercice 3 (sur 2):\n",
        "- Bonne déclaration de la classe Network\n",
        "- Bonne modification de optimizer et de la forward pass\n",
        "\n",
        "Exercice 4 (sur 2):\n",
        "- Bonne déclaration de la classe Dataset\n",
        "- Bon calcul de l'accuracy\n",
        "\n",
        "Bonus (2 points):\n",
        "- Exercice 4: plot de l'évolution de la training loss par epoch\n",
        "- Un truc cool qui va au dela des énnoncés (merci de le dire explicitement dans cette cellule pour que je le trouve facilement)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvotnuWx-moM"
      },
      "source": [
        "## **Exercice 1 : Classification linéaire**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxF1Oeo7yDu7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles, make_blobs\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rWut0TTtKax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "423bd14f-d513-40b4-f62d-a7a1c6f39096"
      },
      "source": [
        "X, y = make_blobs(n_samples=200, centers=2, n_features=2, center_box=(0, 10))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#0000FF']),\n",
        "               edgecolors='k')"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7a6ac2547eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 170
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz13bEknV9VA"
      },
      "source": [
        "# Procédure pour générer un batch\n",
        "def batch(X,y,batch_size):\n",
        "  #X,y sont les features et target dont on veut prendre un batch de taille batch size\n",
        "  #Attention !! X et y doivent être des array numpy et batch, batch y en seront aussi\n",
        "  batch_x = []\n",
        "  batch_y = []\n",
        "  indices = []\n",
        "  while (len(indices)<batch_size) :\n",
        "    random_index = randrange(len(X))\n",
        "    if (not random_index in indices):\n",
        "      batch_x.append(np.array(X[random_index]))\n",
        "      batch_y.append(np.array(y[random_index]))\n",
        "      indices.append(random_index)\n",
        "  return batch_x,batch_y"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqqfcWtzzk2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04915cf5-b8ee-488b-f920-86cd1e1cc457"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from random import randrange\n",
        "import torch.optim as optim\n",
        "\n",
        "## Créez les layers ici\n",
        "layer1 = nn.Linear(2, 3)\n",
        "layer2 = nn.Linear(3, 2)\n",
        "\n",
        "# Déclarez les parametres de l'expérience (batch_size, learning rate, ...)\n",
        "batch_size = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Déclarez la loss et l'optimisation (SGD)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(layer1.parameters()) + list(layer2.parameters()), lr = learning_rate)\n",
        "\n",
        "# Split test/train selon https://playground.tensorflow.org/\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)\n",
        "\n",
        "# Training\n",
        "while True:\n",
        "  X_batch, y_batch = batch(X_train, y_train, batch_size = batch_size)\n",
        "\n",
        "  y_batch = np.array(y_batch)\n",
        "  X_batch = np.array(X_batch)\n",
        "\n",
        "  X_batch_tensor = torch.tensor(X_batch, dtype=torch.float32)\n",
        "  y_batch_tensor = torch.tensor(y_batch, dtype=torch.long)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  out = layer2(layer1(X_batch_tensor))\n",
        "\n",
        "  loss = loss_func(out, y_batch_tensor)\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"Training Loss: {loss.item()}\")\n",
        "  if loss.item() < 0.1:\n",
        "        break\n",
        "\n",
        "# Eval sur test\n",
        "with torch.no_grad():\n",
        "    test_inputs = torch.Tensor(X_test)\n",
        "    test_labels = torch.LongTensor(y_test)\n",
        "\n",
        "    test_output = layer2(layer1(test_inputs))\n",
        "\n",
        "    predicted = np.argmax(test_output.numpy(), axis = 1)\n",
        "    accuracy = np.mean(predicted == test_labels.numpy())\n",
        "\n",
        "    print(f\"Accuracy on Test Data: {accuracy * 100}%\")"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.6544089913368225\n",
            "Training Loss: 0.5898969173431396\n",
            "Training Loss: 0.6176874041557312\n",
            "Training Loss: 0.5018899440765381\n",
            "Training Loss: 0.4523540139198303\n",
            "Training Loss: 0.26815012097358704\n",
            "Training Loss: 0.4716562330722809\n",
            "Training Loss: 0.34216630458831787\n",
            "Training Loss: 0.14536716043949127\n",
            "Training Loss: 0.19392769038677216\n",
            "Training Loss: 0.09617818146944046\n",
            "Accuracy on Test Data: 99.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9bk4Q0tFOpP"
      },
      "source": [
        "## **Exercice 2 : Classification non-linéaire**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUj6kMnU8AjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "87711c5f-23ff-4eb1-89b3-fb0bd1773246"
      },
      "source": [
        "X, y = make_circles(n_samples=200, noise=0.05, factor=0.5, random_state=1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#0000FF']),\n",
        "               edgecolors='k')\n",
        "plt.show()\n"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36hWyZLhFrxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45357929-3ecd-42c2-90f1-e3f56e58c6fd"
      },
      "source": [
        "# Modifiez votre reseau (la couche intermédiaire) pour obtenir 100% sur ce dataset\n",
        "\n",
        "## Créez les layers ici\n",
        "layer1 = nn.Linear(2, 3)\n",
        "tanh = nn.Tanh()\n",
        "layer2 = nn.Linear(3, 2)\n",
        "\n",
        "# Déclarez les parametres de l'expérience (batch_size, learning rate, ...)\n",
        "batch_size = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Déclarez la loss et l'optimisation (SGD)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(layer1.parameters())+list(layer2.parameters()), lr = learning_rate)\n",
        "\n",
        "# Split test/train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)\n",
        "\n",
        "# Training\n",
        "while True:\n",
        "  X_batch, y_batch = batch(X_train, y_train, batch_size = batch_size)\n",
        "\n",
        "  y_batch = np.array(y_batch)\n",
        "  X_batch = np.array(X_batch)\n",
        "\n",
        "  X_batch_tensor = torch.tensor(X_batch, dtype=torch.float32)\n",
        "  y_batch_tensor = torch.tensor(y_batch, dtype=torch.long)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  out = layer2(tanh(layer1(X_batch_tensor)))\n",
        "\n",
        "  loss = loss_func(out, y_batch_tensor)\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"Training Loss: {loss.item()}\")\n",
        "  if loss.item() < 0.1:\n",
        "        break\n",
        "\n",
        "# Eval sur test\n",
        "with torch.no_grad():\n",
        "    test_inputs = torch.tensor(X_test, dtype=torch.float32)\n",
        "    test_labels = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    test_output = layer2(tanh(layer1(test_inputs)))\n",
        "\n",
        "    predicted = np.argmax(test_output.numpy(), axis = 1)\n",
        "    accuracy = np.mean(predicted == test_labels.numpy())\n",
        "\n",
        "    print(f\"Accuracy on Test Data: {accuracy * 100}%\")"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.704660177230835\n",
            "Training Loss: 0.68018639087677\n",
            "Training Loss: 0.7616987824440002\n",
            "Training Loss: 0.6966195702552795\n",
            "Training Loss: 0.6628320217132568\n",
            "Training Loss: 0.7050206065177917\n",
            "Training Loss: 0.7035135626792908\n",
            "Training Loss: 0.7280953526496887\n",
            "Training Loss: 0.6901119947433472\n",
            "Training Loss: 0.6823526620864868\n",
            "Training Loss: 0.7326990365982056\n",
            "Training Loss: 0.6836972236633301\n",
            "Training Loss: 0.6519570350646973\n",
            "Training Loss: 0.7262388467788696\n",
            "Training Loss: 0.6759316325187683\n",
            "Training Loss: 0.6800246238708496\n",
            "Training Loss: 0.6385734677314758\n",
            "Training Loss: 0.6932806968688965\n",
            "Training Loss: 0.7184414267539978\n",
            "Training Loss: 0.6366139650344849\n",
            "Training Loss: 0.6965634226799011\n",
            "Training Loss: 0.5746013522148132\n",
            "Training Loss: 0.6602312326431274\n",
            "Training Loss: 0.6922243237495422\n",
            "Training Loss: 0.6485801935195923\n",
            "Training Loss: 0.7630088329315186\n",
            "Training Loss: 0.631030797958374\n",
            "Training Loss: 0.6782124042510986\n",
            "Training Loss: 0.7004178166389465\n",
            "Training Loss: 0.7833188772201538\n",
            "Training Loss: 0.6941962242126465\n",
            "Training Loss: 0.7270008325576782\n",
            "Training Loss: 0.721011221408844\n",
            "Training Loss: 0.7582718133926392\n",
            "Training Loss: 0.6844015121459961\n",
            "Training Loss: 0.7227440476417542\n",
            "Training Loss: 0.7086135745048523\n",
            "Training Loss: 0.6593402624130249\n",
            "Training Loss: 0.678497850894928\n",
            "Training Loss: 0.670456051826477\n",
            "Training Loss: 0.719872772693634\n",
            "Training Loss: 0.6668040752410889\n",
            "Training Loss: 0.6181603670120239\n",
            "Training Loss: 0.6944464445114136\n",
            "Training Loss: 0.704195499420166\n",
            "Training Loss: 0.7299780249595642\n",
            "Training Loss: 0.6264709234237671\n",
            "Training Loss: 0.6886323690414429\n",
            "Training Loss: 0.6461080312728882\n",
            "Training Loss: 0.7075169086456299\n",
            "Training Loss: 0.6614730954170227\n",
            "Training Loss: 0.6511656045913696\n",
            "Training Loss: 0.6780235171318054\n",
            "Training Loss: 0.7322134971618652\n",
            "Training Loss: 0.6403946280479431\n",
            "Training Loss: 0.5840431451797485\n",
            "Training Loss: 0.6681196093559265\n",
            "Training Loss: 0.7338763475418091\n",
            "Training Loss: 0.5777572989463806\n",
            "Training Loss: 0.6830880641937256\n",
            "Training Loss: 0.5773751735687256\n",
            "Training Loss: 0.6733606457710266\n",
            "Training Loss: 0.7385669946670532\n",
            "Training Loss: 0.658110499382019\n",
            "Training Loss: 0.7692559957504272\n",
            "Training Loss: 0.6315401792526245\n",
            "Training Loss: 0.7925766110420227\n",
            "Training Loss: 0.6662850975990295\n",
            "Training Loss: 0.6399825215339661\n",
            "Training Loss: 0.6733149290084839\n",
            "Training Loss: 0.7466583847999573\n",
            "Training Loss: 0.756078839302063\n",
            "Training Loss: 0.6739911437034607\n",
            "Training Loss: 0.7436642050743103\n",
            "Training Loss: 0.6782433390617371\n",
            "Training Loss: 0.7829722166061401\n",
            "Training Loss: 0.6257182359695435\n",
            "Training Loss: 0.632636547088623\n",
            "Training Loss: 0.7132644057273865\n",
            "Training Loss: 0.718741238117218\n",
            "Training Loss: 0.7165021300315857\n",
            "Training Loss: 0.720999002456665\n",
            "Training Loss: 0.6816264986991882\n",
            "Training Loss: 0.7196403741836548\n",
            "Training Loss: 0.6309905648231506\n",
            "Training Loss: 0.705132782459259\n",
            "Training Loss: 0.6200298070907593\n",
            "Training Loss: 0.7124212384223938\n",
            "Training Loss: 0.7056668996810913\n",
            "Training Loss: 0.6792207360267639\n",
            "Training Loss: 0.6562113165855408\n",
            "Training Loss: 0.643904447555542\n",
            "Training Loss: 0.7302307486534119\n",
            "Training Loss: 0.7003741264343262\n",
            "Training Loss: 0.6735810041427612\n",
            "Training Loss: 0.6684969663619995\n",
            "Training Loss: 0.759817898273468\n",
            "Training Loss: 0.7100566625595093\n",
            "Training Loss: 0.657089114189148\n",
            "Training Loss: 0.6508811712265015\n",
            "Training Loss: 0.6735979914665222\n",
            "Training Loss: 0.6123734712600708\n",
            "Training Loss: 0.6292160749435425\n",
            "Training Loss: 0.6673292517662048\n",
            "Training Loss: 0.7918603420257568\n",
            "Training Loss: 0.6351401209831238\n",
            "Training Loss: 0.7275798320770264\n",
            "Training Loss: 0.6974702477455139\n",
            "Training Loss: 0.6747640371322632\n",
            "Training Loss: 0.6430081129074097\n",
            "Training Loss: 0.7056772708892822\n",
            "Training Loss: 0.5915535688400269\n",
            "Training Loss: 0.6161266565322876\n",
            "Training Loss: 0.6967979073524475\n",
            "Training Loss: 0.6837722063064575\n",
            "Training Loss: 0.5456126928329468\n",
            "Training Loss: 0.6670417785644531\n",
            "Training Loss: 0.7577127814292908\n",
            "Training Loss: 0.5931819677352905\n",
            "Training Loss: 0.6219603419303894\n",
            "Training Loss: 0.7262097597122192\n",
            "Training Loss: 0.722489595413208\n",
            "Training Loss: 0.7207556962966919\n",
            "Training Loss: 0.7061039209365845\n",
            "Training Loss: 0.6887878179550171\n",
            "Training Loss: 0.6351683735847473\n",
            "Training Loss: 0.6293780207633972\n",
            "Training Loss: 0.6742557287216187\n",
            "Training Loss: 0.6713601350784302\n",
            "Training Loss: 0.6747161149978638\n",
            "Training Loss: 0.6636320948600769\n",
            "Training Loss: 0.7608352899551392\n",
            "Training Loss: 0.7211973667144775\n",
            "Training Loss: 0.8111060261726379\n",
            "Training Loss: 0.644940197467804\n",
            "Training Loss: 0.6458979845046997\n",
            "Training Loss: 0.797956645488739\n",
            "Training Loss: 0.659318208694458\n",
            "Training Loss: 0.6161966323852539\n",
            "Training Loss: 0.7301626801490784\n",
            "Training Loss: 0.6469715237617493\n",
            "Training Loss: 0.7040554285049438\n",
            "Training Loss: 0.6918858289718628\n",
            "Training Loss: 0.6537531018257141\n",
            "Training Loss: 0.7016507387161255\n",
            "Training Loss: 0.6798455119132996\n",
            "Training Loss: 0.6538103818893433\n",
            "Training Loss: 0.7310736775398254\n",
            "Training Loss: 0.678142786026001\n",
            "Training Loss: 0.7448612451553345\n",
            "Training Loss: 0.788785457611084\n",
            "Training Loss: 0.6455187797546387\n",
            "Training Loss: 0.6799371242523193\n",
            "Training Loss: 0.7100452780723572\n",
            "Training Loss: 0.7020431756973267\n",
            "Training Loss: 0.7146172523498535\n",
            "Training Loss: 0.6602007150650024\n",
            "Training Loss: 0.6788848042488098\n",
            "Training Loss: 0.6676989793777466\n",
            "Training Loss: 0.6618326306343079\n",
            "Training Loss: 0.6427072286605835\n",
            "Training Loss: 0.6199848651885986\n",
            "Training Loss: 0.7075425386428833\n",
            "Training Loss: 0.6357741951942444\n",
            "Training Loss: 0.7438294887542725\n",
            "Training Loss: 0.6691762208938599\n",
            "Training Loss: 0.7303733825683594\n",
            "Training Loss: 0.7348949313163757\n",
            "Training Loss: 0.6734678149223328\n",
            "Training Loss: 0.6781572103500366\n",
            "Training Loss: 0.5894119143486023\n",
            "Training Loss: 0.7067127823829651\n",
            "Training Loss: 0.6376596093177795\n",
            "Training Loss: 0.7370620965957642\n",
            "Training Loss: 0.6534386277198792\n",
            "Training Loss: 0.6690842509269714\n",
            "Training Loss: 0.6413669586181641\n",
            "Training Loss: 0.6705244779586792\n",
            "Training Loss: 0.5968276262283325\n",
            "Training Loss: 0.7560895085334778\n",
            "Training Loss: 0.6325578689575195\n",
            "Training Loss: 0.745856523513794\n",
            "Training Loss: 0.6840214729309082\n",
            "Training Loss: 0.6629145741462708\n",
            "Training Loss: 0.7409030199050903\n",
            "Training Loss: 0.6315009593963623\n",
            "Training Loss: 0.6994868516921997\n",
            "Training Loss: 0.6734513640403748\n",
            "Training Loss: 0.7020053267478943\n",
            "Training Loss: 0.717067301273346\n",
            "Training Loss: 0.7183602452278137\n",
            "Training Loss: 0.642082929611206\n",
            "Training Loss: 0.7494475841522217\n",
            "Training Loss: 0.6938353776931763\n",
            "Training Loss: 0.6522741913795471\n",
            "Training Loss: 0.5911737680435181\n",
            "Training Loss: 0.6448875069618225\n",
            "Training Loss: 0.6340974569320679\n",
            "Training Loss: 0.625386118888855\n",
            "Training Loss: 0.5856154561042786\n",
            "Training Loss: 0.7248873710632324\n",
            "Training Loss: 0.6727712750434875\n",
            "Training Loss: 0.632023811340332\n",
            "Training Loss: 0.6641585826873779\n",
            "Training Loss: 0.712432861328125\n",
            "Training Loss: 0.6595948338508606\n",
            "Training Loss: 0.6667395830154419\n",
            "Training Loss: 0.6315833330154419\n",
            "Training Loss: 0.6333914995193481\n",
            "Training Loss: 0.6213963627815247\n",
            "Training Loss: 0.7570232152938843\n",
            "Training Loss: 0.7474877238273621\n",
            "Training Loss: 0.6587737202644348\n",
            "Training Loss: 0.6648403406143188\n",
            "Training Loss: 0.7032850384712219\n",
            "Training Loss: 0.6436119079589844\n",
            "Training Loss: 0.6685859560966492\n",
            "Training Loss: 0.7016723155975342\n",
            "Training Loss: 0.58690345287323\n",
            "Training Loss: 0.6355751752853394\n",
            "Training Loss: 0.6604893803596497\n",
            "Training Loss: 0.7148202061653137\n",
            "Training Loss: 0.6347601413726807\n",
            "Training Loss: 0.6585518717765808\n",
            "Training Loss: 0.6780857443809509\n",
            "Training Loss: 0.6038233041763306\n",
            "Training Loss: 0.5726010203361511\n",
            "Training Loss: 0.6145287752151489\n",
            "Training Loss: 0.6866766810417175\n",
            "Training Loss: 0.614676833152771\n",
            "Training Loss: 0.617472231388092\n",
            "Training Loss: 0.664699375629425\n",
            "Training Loss: 0.7791484594345093\n",
            "Training Loss: 0.7303049564361572\n",
            "Training Loss: 0.6682173609733582\n",
            "Training Loss: 0.6827788949012756\n",
            "Training Loss: 0.7157008051872253\n",
            "Training Loss: 0.624778151512146\n",
            "Training Loss: 0.6083489656448364\n",
            "Training Loss: 0.7130647897720337\n",
            "Training Loss: 0.8402705192565918\n",
            "Training Loss: 0.7557506561279297\n",
            "Training Loss: 0.6538094282150269\n",
            "Training Loss: 0.687969446182251\n",
            "Training Loss: 0.6644008159637451\n",
            "Training Loss: 0.7111358046531677\n",
            "Training Loss: 0.6143149137496948\n",
            "Training Loss: 0.6665846705436707\n",
            "Training Loss: 0.7402071952819824\n",
            "Training Loss: 0.7564285397529602\n",
            "Training Loss: 0.7168766260147095\n",
            "Training Loss: 0.6720424890518188\n",
            "Training Loss: 0.664808452129364\n",
            "Training Loss: 0.7344831228256226\n",
            "Training Loss: 0.6718336939811707\n",
            "Training Loss: 0.7738667726516724\n",
            "Training Loss: 0.6584199666976929\n",
            "Training Loss: 0.6617271304130554\n",
            "Training Loss: 0.6376775503158569\n",
            "Training Loss: 0.6367019414901733\n",
            "Training Loss: 0.6445581316947937\n",
            "Training Loss: 0.6460270881652832\n",
            "Training Loss: 0.7400408387184143\n",
            "Training Loss: 0.6957629919052124\n",
            "Training Loss: 0.7149470448493958\n",
            "Training Loss: 0.6889697909355164\n",
            "Training Loss: 0.6393505334854126\n",
            "Training Loss: 0.6991575956344604\n",
            "Training Loss: 0.7397502660751343\n",
            "Training Loss: 0.667815625667572\n",
            "Training Loss: 0.7015719413757324\n",
            "Training Loss: 0.6647772192955017\n",
            "Training Loss: 0.6916355490684509\n",
            "Training Loss: 0.7186976671218872\n",
            "Training Loss: 0.6788381338119507\n",
            "Training Loss: 0.6910644769668579\n",
            "Training Loss: 0.6703524589538574\n",
            "Training Loss: 0.7177289128303528\n",
            "Training Loss: 0.6767445802688599\n",
            "Training Loss: 0.7158260345458984\n",
            "Training Loss: 0.6886508464813232\n",
            "Training Loss: 0.7451887726783752\n",
            "Training Loss: 0.7248884439468384\n",
            "Training Loss: 0.6866339445114136\n",
            "Training Loss: 0.7131450772285461\n",
            "Training Loss: 0.699495255947113\n",
            "Training Loss: 0.6622810363769531\n",
            "Training Loss: 0.6979700922966003\n",
            "Training Loss: 0.7420607805252075\n",
            "Training Loss: 0.6525284647941589\n",
            "Training Loss: 0.7168746590614319\n",
            "Training Loss: 0.6577248573303223\n",
            "Training Loss: 0.7061737179756165\n",
            "Training Loss: 0.7156302332878113\n",
            "Training Loss: 0.6525849103927612\n",
            "Training Loss: 0.7024621367454529\n",
            "Training Loss: 0.6672208309173584\n",
            "Training Loss: 0.6134551763534546\n",
            "Training Loss: 0.694729208946228\n",
            "Training Loss: 0.6737746000289917\n",
            "Training Loss: 0.5762099027633667\n",
            "Training Loss: 0.6154371500015259\n",
            "Training Loss: 0.6595850586891174\n",
            "Training Loss: 0.6650058627128601\n",
            "Training Loss: 0.6488779783248901\n",
            "Training Loss: 0.7369310259819031\n",
            "Training Loss: 0.7016041874885559\n",
            "Training Loss: 0.7086807489395142\n",
            "Training Loss: 0.6631876826286316\n",
            "Training Loss: 0.6631032824516296\n",
            "Training Loss: 0.6997174024581909\n",
            "Training Loss: 0.7019520401954651\n",
            "Training Loss: 0.6448615193367004\n",
            "Training Loss: 0.7539696097373962\n",
            "Training Loss: 0.6562175154685974\n",
            "Training Loss: 0.7003722786903381\n",
            "Training Loss: 0.6593440175056458\n",
            "Training Loss: 0.6573757529258728\n",
            "Training Loss: 0.6294633150100708\n",
            "Training Loss: 0.6951035261154175\n",
            "Training Loss: 0.7023101449012756\n",
            "Training Loss: 0.6142656207084656\n",
            "Training Loss: 0.7409977316856384\n",
            "Training Loss: 0.764022707939148\n",
            "Training Loss: 0.7447382211685181\n",
            "Training Loss: 0.6715693473815918\n",
            "Training Loss: 0.5617967844009399\n",
            "Training Loss: 0.7207965850830078\n",
            "Training Loss: 0.6701871752738953\n",
            "Training Loss: 0.6974349617958069\n",
            "Training Loss: 0.6547752022743225\n",
            "Training Loss: 0.7282344102859497\n",
            "Training Loss: 0.6621235609054565\n",
            "Training Loss: 0.7843940258026123\n",
            "Training Loss: 0.6547964811325073\n",
            "Training Loss: 0.6704684495925903\n",
            "Training Loss: 0.7037359476089478\n",
            "Training Loss: 0.6466489434242249\n",
            "Training Loss: 0.729216992855072\n",
            "Training Loss: 0.6789259314537048\n",
            "Training Loss: 0.6837397813796997\n",
            "Training Loss: 0.7168978452682495\n",
            "Training Loss: 0.6340958476066589\n",
            "Training Loss: 0.6809729337692261\n",
            "Training Loss: 0.6400212049484253\n",
            "Training Loss: 0.6173439621925354\n",
            "Training Loss: 0.6794227361679077\n",
            "Training Loss: 0.7239317297935486\n",
            "Training Loss: 0.6830083131790161\n",
            "Training Loss: 0.6796042919158936\n",
            "Training Loss: 0.564689040184021\n",
            "Training Loss: 0.712892472743988\n",
            "Training Loss: 0.7097684144973755\n",
            "Training Loss: 0.6292473077774048\n",
            "Training Loss: 0.5912686586380005\n",
            "Training Loss: 0.7397453188896179\n",
            "Training Loss: 0.6488088369369507\n",
            "Training Loss: 0.706253170967102\n",
            "Training Loss: 0.6578070521354675\n",
            "Training Loss: 0.6335991621017456\n",
            "Training Loss: 0.7850276231765747\n",
            "Training Loss: 0.7198356986045837\n",
            "Training Loss: 0.6712539792060852\n",
            "Training Loss: 0.6551693677902222\n",
            "Training Loss: 0.7204768061637878\n",
            "Training Loss: 0.6433686017990112\n",
            "Training Loss: 0.6577948331832886\n",
            "Training Loss: 0.6657460331916809\n",
            "Training Loss: 0.7141584157943726\n",
            "Training Loss: 0.6985965967178345\n",
            "Training Loss: 0.6978940963745117\n",
            "Training Loss: 0.6687207818031311\n",
            "Training Loss: 0.5959233045578003\n",
            "Training Loss: 0.6233335733413696\n",
            "Training Loss: 0.6242263913154602\n",
            "Training Loss: 0.7036631107330322\n",
            "Training Loss: 0.593594491481781\n",
            "Training Loss: 0.737352192401886\n",
            "Training Loss: 0.5512251853942871\n",
            "Training Loss: 0.7201276421546936\n",
            "Training Loss: 0.5463624000549316\n",
            "Training Loss: 0.5030209422111511\n",
            "Training Loss: 0.8272476196289062\n",
            "Training Loss: 0.6801131367683411\n",
            "Training Loss: 0.5864771604537964\n",
            "Training Loss: 0.7297757863998413\n",
            "Training Loss: 0.7699869871139526\n",
            "Training Loss: 0.6942557096481323\n",
            "Training Loss: 0.6923807859420776\n",
            "Training Loss: 0.665520191192627\n",
            "Training Loss: 0.7693656086921692\n",
            "Training Loss: 0.653082549571991\n",
            "Training Loss: 0.589626133441925\n",
            "Training Loss: 0.7610604763031006\n",
            "Training Loss: 0.6525181531906128\n",
            "Training Loss: 0.5989837646484375\n",
            "Training Loss: 0.6171444654464722\n",
            "Training Loss: 0.7143136262893677\n",
            "Training Loss: 0.7351018190383911\n",
            "Training Loss: 0.6691257357597351\n",
            "Training Loss: 0.7155667543411255\n",
            "Training Loss: 0.673571765422821\n",
            "Training Loss: 0.700287401676178\n",
            "Training Loss: 0.7151018381118774\n",
            "Training Loss: 0.6606898307800293\n",
            "Training Loss: 0.7067378759384155\n",
            "Training Loss: 0.5866344571113586\n",
            "Training Loss: 0.5695694088935852\n",
            "Training Loss: 0.6762813329696655\n",
            "Training Loss: 0.6979060173034668\n",
            "Training Loss: 0.710507333278656\n",
            "Training Loss: 0.7009872198104858\n",
            "Training Loss: 0.7675470113754272\n",
            "Training Loss: 0.6749475598335266\n",
            "Training Loss: 0.8102377653121948\n",
            "Training Loss: 0.6886107921600342\n",
            "Training Loss: 0.6723743081092834\n",
            "Training Loss: 0.7297303080558777\n",
            "Training Loss: 0.6293467283248901\n",
            "Training Loss: 0.6717686653137207\n",
            "Training Loss: 0.661163330078125\n",
            "Training Loss: 0.6278387308120728\n",
            "Training Loss: 0.7160443067550659\n",
            "Training Loss: 0.607620894908905\n",
            "Training Loss: 0.7482863664627075\n",
            "Training Loss: 0.6768521070480347\n",
            "Training Loss: 0.6265178322792053\n",
            "Training Loss: 0.7470299005508423\n",
            "Training Loss: 0.6836104393005371\n",
            "Training Loss: 0.6190859079360962\n",
            "Training Loss: 0.7523496150970459\n",
            "Training Loss: 0.728689968585968\n",
            "Training Loss: 0.660097062587738\n",
            "Training Loss: 0.6921342015266418\n",
            "Training Loss: 0.6733915209770203\n",
            "Training Loss: 0.6634258031845093\n",
            "Training Loss: 0.6998836398124695\n",
            "Training Loss: 0.7128572463989258\n",
            "Training Loss: 0.6533185243606567\n",
            "Training Loss: 0.6068358421325684\n",
            "Training Loss: 0.6870365142822266\n",
            "Training Loss: 0.6526129245758057\n",
            "Training Loss: 0.6684796810150146\n",
            "Training Loss: 0.6957525014877319\n",
            "Training Loss: 0.5573722720146179\n",
            "Training Loss: 0.8115093111991882\n",
            "Training Loss: 0.7023671865463257\n",
            "Training Loss: 0.6591456532478333\n",
            "Training Loss: 0.6624563932418823\n",
            "Training Loss: 0.7663722634315491\n",
            "Training Loss: 0.7071461081504822\n",
            "Training Loss: 0.6986249685287476\n",
            "Training Loss: 0.6972028613090515\n",
            "Training Loss: 0.6961551904678345\n",
            "Training Loss: 0.6509952545166016\n",
            "Training Loss: 0.6112256050109863\n",
            "Training Loss: 0.752609133720398\n",
            "Training Loss: 0.6399334669113159\n",
            "Training Loss: 0.7258411049842834\n",
            "Training Loss: 0.7288193106651306\n",
            "Training Loss: 0.671257734298706\n",
            "Training Loss: 0.7244395017623901\n",
            "Training Loss: 0.7171779274940491\n",
            "Training Loss: 0.7093003988265991\n",
            "Training Loss: 0.633159339427948\n",
            "Training Loss: 0.7140310406684875\n",
            "Training Loss: 0.6210256814956665\n",
            "Training Loss: 0.6924350261688232\n",
            "Training Loss: 0.691592276096344\n",
            "Training Loss: 0.6914078593254089\n",
            "Training Loss: 0.7307228446006775\n",
            "Training Loss: 0.7602254748344421\n",
            "Training Loss: 0.6862953305244446\n",
            "Training Loss: 0.6589654088020325\n",
            "Training Loss: 0.7277653813362122\n",
            "Training Loss: 0.6767728328704834\n",
            "Training Loss: 0.7245797514915466\n",
            "Training Loss: 0.6868528127670288\n",
            "Training Loss: 0.7407304048538208\n",
            "Training Loss: 0.7210239768028259\n",
            "Training Loss: 0.6769376993179321\n",
            "Training Loss: 0.6674576997756958\n",
            "Training Loss: 0.6519774198532104\n",
            "Training Loss: 0.6491376161575317\n",
            "Training Loss: 0.6959401965141296\n",
            "Training Loss: 0.6649832725524902\n",
            "Training Loss: 0.7180256247520447\n",
            "Training Loss: 0.627656102180481\n",
            "Training Loss: 0.7319734692573547\n",
            "Training Loss: 0.6795079112052917\n",
            "Training Loss: 0.6341269612312317\n",
            "Training Loss: 0.6622540950775146\n",
            "Training Loss: 0.6641907095909119\n",
            "Training Loss: 0.6231849193572998\n",
            "Training Loss: 0.7002894282341003\n",
            "Training Loss: 0.6406539678573608\n",
            "Training Loss: 0.6609131693840027\n",
            "Training Loss: 0.7365685701370239\n",
            "Training Loss: 0.6493395566940308\n",
            "Training Loss: 0.7278309464454651\n",
            "Training Loss: 0.6640413999557495\n",
            "Training Loss: 0.7593291997909546\n",
            "Training Loss: 0.6947941780090332\n",
            "Training Loss: 0.7067985534667969\n",
            "Training Loss: 0.7212496399879456\n",
            "Training Loss: 0.6848926544189453\n",
            "Training Loss: 0.6467123627662659\n",
            "Training Loss: 0.6158226728439331\n",
            "Training Loss: 0.7596045732498169\n",
            "Training Loss: 0.6698713302612305\n",
            "Training Loss: 0.7012261152267456\n",
            "Training Loss: 0.6428912878036499\n",
            "Training Loss: 0.6990894079208374\n",
            "Training Loss: 0.6785123944282532\n",
            "Training Loss: 0.7442256212234497\n",
            "Training Loss: 0.708803653717041\n",
            "Training Loss: 0.7020057439804077\n",
            "Training Loss: 0.6350845098495483\n",
            "Training Loss: 0.5647422075271606\n",
            "Training Loss: 0.7689330577850342\n",
            "Training Loss: 0.6412711143493652\n",
            "Training Loss: 0.6448329091072083\n",
            "Training Loss: 0.752118706703186\n",
            "Training Loss: 0.6889594197273254\n",
            "Training Loss: 0.7093722820281982\n",
            "Training Loss: 0.5835526585578918\n",
            "Training Loss: 0.6785616278648376\n",
            "Training Loss: 0.6428155899047852\n",
            "Training Loss: 0.6495373249053955\n",
            "Training Loss: 0.6622844934463501\n",
            "Training Loss: 0.7132484316825867\n",
            "Training Loss: 0.6371726989746094\n",
            "Training Loss: 0.7197647094726562\n",
            "Training Loss: 0.685196042060852\n",
            "Training Loss: 0.6864263415336609\n",
            "Training Loss: 0.639661431312561\n",
            "Training Loss: 0.7235950231552124\n",
            "Training Loss: 0.662930965423584\n",
            "Training Loss: 0.7015601396560669\n",
            "Training Loss: 0.7271498441696167\n",
            "Training Loss: 0.6740326285362244\n",
            "Training Loss: 0.6429083347320557\n",
            "Training Loss: 0.7062831521034241\n",
            "Training Loss: 0.6725229620933533\n",
            "Training Loss: 0.6438757181167603\n",
            "Training Loss: 0.6705040335655212\n",
            "Training Loss: 0.6385006308555603\n",
            "Training Loss: 0.5950416326522827\n",
            "Training Loss: 0.751885712146759\n",
            "Training Loss: 0.684007465839386\n",
            "Training Loss: 0.6873043775558472\n",
            "Training Loss: 0.7364143133163452\n",
            "Training Loss: 0.6346503496170044\n",
            "Training Loss: 0.7235379219055176\n",
            "Training Loss: 0.5932711362838745\n",
            "Training Loss: 0.5731843113899231\n",
            "Training Loss: 0.700924277305603\n",
            "Training Loss: 0.57899010181427\n",
            "Training Loss: 0.646252453327179\n",
            "Training Loss: 0.677519679069519\n",
            "Training Loss: 0.8050163984298706\n",
            "Training Loss: 0.6191312670707703\n",
            "Training Loss: 0.6695241332054138\n",
            "Training Loss: 0.6710547208786011\n",
            "Training Loss: 0.6296914219856262\n",
            "Training Loss: 0.7072545289993286\n",
            "Training Loss: 0.7278738021850586\n",
            "Training Loss: 0.6178014874458313\n",
            "Training Loss: 0.5493050813674927\n",
            "Training Loss: 0.5858895182609558\n",
            "Training Loss: 0.754129946231842\n",
            "Training Loss: 0.7431041598320007\n",
            "Training Loss: 0.6841123104095459\n",
            "Training Loss: 0.7507048845291138\n",
            "Training Loss: 0.7153939008712769\n",
            "Training Loss: 0.707769513130188\n",
            "Training Loss: 0.6725800633430481\n",
            "Training Loss: 0.6345261335372925\n",
            "Training Loss: 0.6706148386001587\n",
            "Training Loss: 0.6319388747215271\n",
            "Training Loss: 0.6133984327316284\n",
            "Training Loss: 0.7271585464477539\n",
            "Training Loss: 0.6362847685813904\n",
            "Training Loss: 0.6809231638908386\n",
            "Training Loss: 0.626198947429657\n",
            "Training Loss: 0.7056489586830139\n",
            "Training Loss: 0.6957763433456421\n",
            "Training Loss: 0.6477438807487488\n",
            "Training Loss: 0.6578844785690308\n",
            "Training Loss: 0.6904077529907227\n",
            "Training Loss: 0.6770263910293579\n",
            "Training Loss: 0.6867499351501465\n",
            "Training Loss: 0.6466401815414429\n",
            "Training Loss: 0.6472181081771851\n",
            "Training Loss: 0.7051934599876404\n",
            "Training Loss: 0.6675971150398254\n",
            "Training Loss: 0.7035199403762817\n",
            "Training Loss: 0.6433656811714172\n",
            "Training Loss: 0.6798607707023621\n",
            "Training Loss: 0.6554678678512573\n",
            "Training Loss: 0.6305274963378906\n",
            "Training Loss: 0.7273504137992859\n",
            "Training Loss: 0.7367397546768188\n",
            "Training Loss: 0.6003252267837524\n",
            "Training Loss: 0.688299834728241\n",
            "Training Loss: 0.6152749061584473\n",
            "Training Loss: 0.6361011266708374\n",
            "Training Loss: 0.6219242215156555\n",
            "Training Loss: 0.7351880669593811\n",
            "Training Loss: 0.6874443888664246\n",
            "Training Loss: 0.6514734029769897\n",
            "Training Loss: 0.6844154000282288\n",
            "Training Loss: 0.7408113479614258\n",
            "Training Loss: 0.7072375416755676\n",
            "Training Loss: 0.730177104473114\n",
            "Training Loss: 0.700937032699585\n",
            "Training Loss: 0.630513072013855\n",
            "Training Loss: 0.6811944246292114\n",
            "Training Loss: 0.6596137881278992\n",
            "Training Loss: 0.7252520322799683\n",
            "Training Loss: 0.6607809662818909\n",
            "Training Loss: 0.7195637822151184\n",
            "Training Loss: 0.6140135526657104\n",
            "Training Loss: 0.6604188084602356\n",
            "Training Loss: 0.7189418077468872\n",
            "Training Loss: 0.698988139629364\n",
            "Training Loss: 0.6131478548049927\n",
            "Training Loss: 0.693118691444397\n",
            "Training Loss: 0.6109349727630615\n",
            "Training Loss: 0.6793562769889832\n",
            "Training Loss: 0.5403828024864197\n",
            "Training Loss: 0.608500063419342\n",
            "Training Loss: 0.7462683916091919\n",
            "Training Loss: 0.6147361993789673\n",
            "Training Loss: 0.6676180958747864\n",
            "Training Loss: 0.7594262361526489\n",
            "Training Loss: 0.5817392468452454\n",
            "Training Loss: 0.647162139415741\n",
            "Training Loss: 0.7163909077644348\n",
            "Training Loss: 0.6639158725738525\n",
            "Training Loss: 0.579261064529419\n",
            "Training Loss: 0.7044591307640076\n",
            "Training Loss: 0.728570818901062\n",
            "Training Loss: 0.7258148193359375\n",
            "Training Loss: 0.6381060481071472\n",
            "Training Loss: 0.6912410855293274\n",
            "Training Loss: 0.510624349117279\n",
            "Training Loss: 0.6892397999763489\n",
            "Training Loss: 0.6847283840179443\n",
            "Training Loss: 0.7407423853874207\n",
            "Training Loss: 0.695952296257019\n",
            "Training Loss: 0.6120198369026184\n",
            "Training Loss: 0.6061437726020813\n",
            "Training Loss: 0.6820671558380127\n",
            "Training Loss: 0.7545922994613647\n",
            "Training Loss: 0.6242796182632446\n",
            "Training Loss: 0.7330859899520874\n",
            "Training Loss: 0.6547360420227051\n",
            "Training Loss: 0.6141295433044434\n",
            "Training Loss: 0.6604808568954468\n",
            "Training Loss: 0.5829086303710938\n",
            "Training Loss: 0.7865223288536072\n",
            "Training Loss: 0.6327419877052307\n",
            "Training Loss: 0.67864590883255\n",
            "Training Loss: 0.7396429777145386\n",
            "Training Loss: 0.5991109013557434\n",
            "Training Loss: 0.6573470234870911\n",
            "Training Loss: 0.5415717959403992\n",
            "Training Loss: 0.6884487867355347\n",
            "Training Loss: 0.768083930015564\n",
            "Training Loss: 0.7093372941017151\n",
            "Training Loss: 0.6101776957511902\n",
            "Training Loss: 0.667596697807312\n",
            "Training Loss: 0.7066941261291504\n",
            "Training Loss: 0.7516924142837524\n",
            "Training Loss: 0.6577638387680054\n",
            "Training Loss: 0.7563601732254028\n",
            "Training Loss: 0.6985270380973816\n",
            "Training Loss: 0.6138604879379272\n",
            "Training Loss: 0.7058383822441101\n",
            "Training Loss: 0.6501868963241577\n",
            "Training Loss: 0.6250695586204529\n",
            "Training Loss: 0.6961425542831421\n",
            "Training Loss: 0.710147500038147\n",
            "Training Loss: 0.6698625683784485\n",
            "Training Loss: 0.6457301378250122\n",
            "Training Loss: 0.6583356857299805\n",
            "Training Loss: 0.6800051331520081\n",
            "Training Loss: 0.6271487474441528\n",
            "Training Loss: 0.680031955242157\n",
            "Training Loss: 0.6541959047317505\n",
            "Training Loss: 0.6923500895500183\n",
            "Training Loss: 0.5327706336975098\n",
            "Training Loss: 0.7100092172622681\n",
            "Training Loss: 0.6824679374694824\n",
            "Training Loss: 0.7037400007247925\n",
            "Training Loss: 0.6600277423858643\n",
            "Training Loss: 0.7530879974365234\n",
            "Training Loss: 0.6855610609054565\n",
            "Training Loss: 0.6999467611312866\n",
            "Training Loss: 0.689747154712677\n",
            "Training Loss: 0.6724625825881958\n",
            "Training Loss: 0.6517184972763062\n",
            "Training Loss: 0.6209790706634521\n",
            "Training Loss: 0.7549864649772644\n",
            "Training Loss: 0.6793656349182129\n",
            "Training Loss: 0.7041037082672119\n",
            "Training Loss: 0.7837334275245667\n",
            "Training Loss: 0.6743065118789673\n",
            "Training Loss: 0.6565651297569275\n",
            "Training Loss: 0.6528807282447815\n",
            "Training Loss: 0.6234442591667175\n",
            "Training Loss: 0.6530775427818298\n",
            "Training Loss: 0.5993255376815796\n",
            "Training Loss: 0.6238161325454712\n",
            "Training Loss: 0.609054684638977\n",
            "Training Loss: 0.6816116571426392\n",
            "Training Loss: 0.7009481191635132\n",
            "Training Loss: 0.653533935546875\n",
            "Training Loss: 0.6879515051841736\n",
            "Training Loss: 0.639816164970398\n",
            "Training Loss: 0.6380395293235779\n",
            "Training Loss: 0.598212480545044\n",
            "Training Loss: 0.6482913494110107\n",
            "Training Loss: 0.6610608100891113\n",
            "Training Loss: 0.6281131505966187\n",
            "Training Loss: 0.6334328651428223\n",
            "Training Loss: 0.7210735082626343\n",
            "Training Loss: 0.6903234124183655\n",
            "Training Loss: 0.7440884709358215\n",
            "Training Loss: 0.7050145268440247\n",
            "Training Loss: 0.6352945566177368\n",
            "Training Loss: 0.6897382140159607\n",
            "Training Loss: 0.7204881906509399\n",
            "Training Loss: 0.6964067220687866\n",
            "Training Loss: 0.6458350419998169\n",
            "Training Loss: 0.6767932176589966\n",
            "Training Loss: 0.685025155544281\n",
            "Training Loss: 0.7196580171585083\n",
            "Training Loss: 0.680709719657898\n",
            "Training Loss: 0.5710426568984985\n",
            "Training Loss: 0.6231411695480347\n",
            "Training Loss: 0.6080623269081116\n",
            "Training Loss: 0.6597113609313965\n",
            "Training Loss: 0.6854519844055176\n",
            "Training Loss: 0.6494601368904114\n",
            "Training Loss: 0.7144301533699036\n",
            "Training Loss: 0.5794426202774048\n",
            "Training Loss: 0.709040641784668\n",
            "Training Loss: 0.6728577613830566\n",
            "Training Loss: 0.7100427150726318\n",
            "Training Loss: 0.6486011743545532\n",
            "Training Loss: 0.5701338052749634\n",
            "Training Loss: 0.7295895218849182\n",
            "Training Loss: 0.6306224465370178\n",
            "Training Loss: 0.5799908638000488\n",
            "Training Loss: 0.6656549572944641\n",
            "Training Loss: 0.6513475179672241\n",
            "Training Loss: 0.7182315587997437\n",
            "Training Loss: 0.6076509356498718\n",
            "Training Loss: 0.6462197303771973\n",
            "Training Loss: 0.6870315670967102\n",
            "Training Loss: 0.6861740350723267\n",
            "Training Loss: 0.6506129503250122\n",
            "Training Loss: 0.6266885995864868\n",
            "Training Loss: 0.6629644632339478\n",
            "Training Loss: 0.5721281170845032\n",
            "Training Loss: 0.5087987184524536\n",
            "Training Loss: 0.7185822129249573\n",
            "Training Loss: 0.5943711400032043\n",
            "Training Loss: 0.6591476798057556\n",
            "Training Loss: 0.5695995092391968\n",
            "Training Loss: 0.7391315698623657\n",
            "Training Loss: 0.7282753586769104\n",
            "Training Loss: 0.5970104932785034\n",
            "Training Loss: 0.7257336974143982\n",
            "Training Loss: 0.7157626152038574\n",
            "Training Loss: 0.6330008506774902\n",
            "Training Loss: 0.7764341235160828\n",
            "Training Loss: 0.5807162523269653\n",
            "Training Loss: 0.7182931900024414\n",
            "Training Loss: 0.712774932384491\n",
            "Training Loss: 0.5903891324996948\n",
            "Training Loss: 0.7020442485809326\n",
            "Training Loss: 0.6591170430183411\n",
            "Training Loss: 0.674630880355835\n",
            "Training Loss: 0.6265705227851868\n",
            "Training Loss: 0.5162773132324219\n",
            "Training Loss: 0.6549280881881714\n",
            "Training Loss: 0.7182599902153015\n",
            "Training Loss: 0.7872489094734192\n",
            "Training Loss: 0.7214236259460449\n",
            "Training Loss: 0.6517830491065979\n",
            "Training Loss: 0.6417425870895386\n",
            "Training Loss: 0.6360516548156738\n",
            "Training Loss: 0.6459511518478394\n",
            "Training Loss: 0.6301048994064331\n",
            "Training Loss: 0.707893967628479\n",
            "Training Loss: 0.6269344091415405\n",
            "Training Loss: 0.7334745526313782\n",
            "Training Loss: 0.588399350643158\n",
            "Training Loss: 0.6765927076339722\n",
            "Training Loss: 0.638066828250885\n",
            "Training Loss: 0.6042226552963257\n",
            "Training Loss: 0.6386426687240601\n",
            "Training Loss: 0.48971977829933167\n",
            "Training Loss: 0.7112004160881042\n",
            "Training Loss: 0.5068277716636658\n",
            "Training Loss: 0.572579026222229\n",
            "Training Loss: 0.7920976281166077\n",
            "Training Loss: 0.6139827966690063\n",
            "Training Loss: 0.5840336084365845\n",
            "Training Loss: 0.592587411403656\n",
            "Training Loss: 0.6693392395973206\n",
            "Training Loss: 0.6972554922103882\n",
            "Training Loss: 0.6129604578018188\n",
            "Training Loss: 0.7324693202972412\n",
            "Training Loss: 0.6628073453903198\n",
            "Training Loss: 0.7053857445716858\n",
            "Training Loss: 0.5051051378250122\n",
            "Training Loss: 0.6610201597213745\n",
            "Training Loss: 0.5951888561248779\n",
            "Training Loss: 0.6639384627342224\n",
            "Training Loss: 0.6160116195678711\n",
            "Training Loss: 0.6402576565742493\n",
            "Training Loss: 0.6610262989997864\n",
            "Training Loss: 0.6352639198303223\n",
            "Training Loss: 0.754004955291748\n",
            "Training Loss: 0.6505997776985168\n",
            "Training Loss: 0.7711626291275024\n",
            "Training Loss: 0.7519646883010864\n",
            "Training Loss: 0.5675007104873657\n",
            "Training Loss: 0.576320230960846\n",
            "Training Loss: 0.6246815323829651\n",
            "Training Loss: 0.6412755846977234\n",
            "Training Loss: 0.6806383728981018\n",
            "Training Loss: 0.6383106112480164\n",
            "Training Loss: 0.6243063807487488\n",
            "Training Loss: 0.701530396938324\n",
            "Training Loss: 0.6619420051574707\n",
            "Training Loss: 0.585323691368103\n",
            "Training Loss: 0.6449662446975708\n",
            "Training Loss: 0.5983108282089233\n",
            "Training Loss: 0.7276930212974548\n",
            "Training Loss: 0.5683512687683105\n",
            "Training Loss: 0.49467581510543823\n",
            "Training Loss: 0.6162487268447876\n",
            "Training Loss: 0.8147948980331421\n",
            "Training Loss: 0.6400139331817627\n",
            "Training Loss: 0.6802425980567932\n",
            "Training Loss: 0.647652268409729\n",
            "Training Loss: 0.7016615867614746\n",
            "Training Loss: 0.6974622011184692\n",
            "Training Loss: 0.6609180569648743\n",
            "Training Loss: 0.6385236978530884\n",
            "Training Loss: 0.6787616014480591\n",
            "Training Loss: 0.5639451742172241\n",
            "Training Loss: 0.7614744901657104\n",
            "Training Loss: 0.6368740797042847\n",
            "Training Loss: 0.5930837392807007\n",
            "Training Loss: 0.6684907674789429\n",
            "Training Loss: 0.6594566106796265\n",
            "Training Loss: 0.5996734499931335\n",
            "Training Loss: 0.6078107953071594\n",
            "Training Loss: 0.6697850823402405\n",
            "Training Loss: 0.7364344596862793\n",
            "Training Loss: 0.6357799768447876\n",
            "Training Loss: 0.759839653968811\n",
            "Training Loss: 0.6303942203521729\n",
            "Training Loss: 0.5942038297653198\n",
            "Training Loss: 0.6759784817695618\n",
            "Training Loss: 0.5964179635047913\n",
            "Training Loss: 0.7059434056282043\n",
            "Training Loss: 0.6583206653594971\n",
            "Training Loss: 0.7254184484481812\n",
            "Training Loss: 0.6043210029602051\n",
            "Training Loss: 0.5663703680038452\n",
            "Training Loss: 0.6592225432395935\n",
            "Training Loss: 0.6877411603927612\n",
            "Training Loss: 0.641600489616394\n",
            "Training Loss: 0.7022911906242371\n",
            "Training Loss: 0.6761432886123657\n",
            "Training Loss: 0.6022136807441711\n",
            "Training Loss: 0.7588186860084534\n",
            "Training Loss: 0.6022050976753235\n",
            "Training Loss: 0.7130225896835327\n",
            "Training Loss: 0.6016381978988647\n",
            "Training Loss: 0.7573230862617493\n",
            "Training Loss: 0.61277836561203\n",
            "Training Loss: 0.7271506190299988\n",
            "Training Loss: 0.65538489818573\n",
            "Training Loss: 0.5701919794082642\n",
            "Training Loss: 0.6347509622573853\n",
            "Training Loss: 0.6516900658607483\n",
            "Training Loss: 0.5715833902359009\n",
            "Training Loss: 0.7129443883895874\n",
            "Training Loss: 0.5588949918746948\n",
            "Training Loss: 0.6394032835960388\n",
            "Training Loss: 0.5718691945075989\n",
            "Training Loss: 0.5120304822921753\n",
            "Training Loss: 0.6741383671760559\n",
            "Training Loss: 0.6512442231178284\n",
            "Training Loss: 0.6088584065437317\n",
            "Training Loss: 0.6555675864219666\n",
            "Training Loss: 0.5363761186599731\n",
            "Training Loss: 0.7385913133621216\n",
            "Training Loss: 0.6612927913665771\n",
            "Training Loss: 0.7664768099784851\n",
            "Training Loss: 0.5972338318824768\n",
            "Training Loss: 0.6593061685562134\n",
            "Training Loss: 0.6546836495399475\n",
            "Training Loss: 0.631184458732605\n",
            "Training Loss: 0.5747365951538086\n",
            "Training Loss: 0.5734690427780151\n",
            "Training Loss: 0.7132099270820618\n",
            "Training Loss: 0.7686694264411926\n",
            "Training Loss: 0.7701659202575684\n",
            "Training Loss: 0.6189956665039062\n",
            "Training Loss: 0.49768781661987305\n",
            "Training Loss: 0.6675094366073608\n",
            "Training Loss: 0.6086315512657166\n",
            "Training Loss: 0.7166140079498291\n",
            "Training Loss: 0.6708478331565857\n",
            "Training Loss: 0.6293582916259766\n",
            "Training Loss: 0.5572482943534851\n",
            "Training Loss: 0.5028981566429138\n",
            "Training Loss: 0.6431105136871338\n",
            "Training Loss: 0.6631368398666382\n",
            "Training Loss: 0.6516637802124023\n",
            "Training Loss: 0.6678842306137085\n",
            "Training Loss: 0.6129506230354309\n",
            "Training Loss: 0.610075831413269\n",
            "Training Loss: 0.5922755002975464\n",
            "Training Loss: 0.6160820126533508\n",
            "Training Loss: 0.6297540068626404\n",
            "Training Loss: 0.6450894474983215\n",
            "Training Loss: 0.6484397053718567\n",
            "Training Loss: 0.5413581728935242\n",
            "Training Loss: 0.6095360517501831\n",
            "Training Loss: 0.5211354494094849\n",
            "Training Loss: 0.6253048181533813\n",
            "Training Loss: 0.5559890866279602\n",
            "Training Loss: 0.6254632472991943\n",
            "Training Loss: 0.5709270238876343\n",
            "Training Loss: 0.47243833541870117\n",
            "Training Loss: 0.755829930305481\n",
            "Training Loss: 0.6515156626701355\n",
            "Training Loss: 0.6498120427131653\n",
            "Training Loss: 0.5262654423713684\n",
            "Training Loss: 0.5151364207267761\n",
            "Training Loss: 0.6056989431381226\n",
            "Training Loss: 0.7800623178482056\n",
            "Training Loss: 0.6980293393135071\n",
            "Training Loss: 0.5825651288032532\n",
            "Training Loss: 0.5695592164993286\n",
            "Training Loss: 0.6102112531661987\n",
            "Training Loss: 0.6715512275695801\n",
            "Training Loss: 0.5018249750137329\n",
            "Training Loss: 0.5886499285697937\n",
            "Training Loss: 0.7440911531448364\n",
            "Training Loss: 0.6569756269454956\n",
            "Training Loss: 0.6626933813095093\n",
            "Training Loss: 0.7453029155731201\n",
            "Training Loss: 0.6085871458053589\n",
            "Training Loss: 0.7496832609176636\n",
            "Training Loss: 0.6320297718048096\n",
            "Training Loss: 0.533006489276886\n",
            "Training Loss: 0.5667701363563538\n",
            "Training Loss: 0.6886738538742065\n",
            "Training Loss: 0.5808507800102234\n",
            "Training Loss: 0.697371244430542\n",
            "Training Loss: 0.659340500831604\n",
            "Training Loss: 0.6356590390205383\n",
            "Training Loss: 0.6266452670097351\n",
            "Training Loss: 0.5138639807701111\n",
            "Training Loss: 0.6289430260658264\n",
            "Training Loss: 0.6061580181121826\n",
            "Training Loss: 0.4862615168094635\n",
            "Training Loss: 0.6434001922607422\n",
            "Training Loss: 0.6102727651596069\n",
            "Training Loss: 0.5667209029197693\n",
            "Training Loss: 0.5780022144317627\n",
            "Training Loss: 0.6198243498802185\n",
            "Training Loss: 0.5628173351287842\n",
            "Training Loss: 0.5477067232131958\n",
            "Training Loss: 0.5849822163581848\n",
            "Training Loss: 0.6622251272201538\n",
            "Training Loss: 0.46863746643066406\n",
            "Training Loss: 0.5346598625183105\n",
            "Training Loss: 0.5568250417709351\n",
            "Training Loss: 0.5952974557876587\n",
            "Training Loss: 0.6345121264457703\n",
            "Training Loss: 0.5879954099655151\n",
            "Training Loss: 0.6617521047592163\n",
            "Training Loss: 0.501869797706604\n",
            "Training Loss: 0.6777797937393188\n",
            "Training Loss: 0.6440140604972839\n",
            "Training Loss: 0.5756131410598755\n",
            "Training Loss: 0.5865430235862732\n",
            "Training Loss: 0.6005238890647888\n",
            "Training Loss: 0.6349891424179077\n",
            "Training Loss: 0.5639915466308594\n",
            "Training Loss: 0.5686461329460144\n",
            "Training Loss: 0.6118651628494263\n",
            "Training Loss: 0.5914350152015686\n",
            "Training Loss: 0.6780838370323181\n",
            "Training Loss: 0.6431536674499512\n",
            "Training Loss: 0.6285350918769836\n",
            "Training Loss: 0.6449191570281982\n",
            "Training Loss: 0.530353307723999\n",
            "Training Loss: 0.700051486492157\n",
            "Training Loss: 0.5933599472045898\n",
            "Training Loss: 0.6070218682289124\n",
            "Training Loss: 0.5898279547691345\n",
            "Training Loss: 0.6568559408187866\n",
            "Training Loss: 0.6694474220275879\n",
            "Training Loss: 0.6100638508796692\n",
            "Training Loss: 0.6582041382789612\n",
            "Training Loss: 0.5126374959945679\n",
            "Training Loss: 0.47419267892837524\n",
            "Training Loss: 0.5645831823348999\n",
            "Training Loss: 0.6287752389907837\n",
            "Training Loss: 0.717558741569519\n",
            "Training Loss: 0.6122854351997375\n",
            "Training Loss: 0.49976176023483276\n",
            "Training Loss: 0.7198224067687988\n",
            "Training Loss: 0.48384150862693787\n",
            "Training Loss: 0.486687570810318\n",
            "Training Loss: 0.48649606108665466\n",
            "Training Loss: 0.5459277033805847\n",
            "Training Loss: 0.6428947448730469\n",
            "Training Loss: 0.4933122992515564\n",
            "Training Loss: 0.5975872278213501\n",
            "Training Loss: 0.7602398991584778\n",
            "Training Loss: 0.6439558863639832\n",
            "Training Loss: 0.6281737685203552\n",
            "Training Loss: 0.739733099937439\n",
            "Training Loss: 0.5554262399673462\n",
            "Training Loss: 0.541568398475647\n",
            "Training Loss: 0.47044259309768677\n",
            "Training Loss: 0.5407267808914185\n",
            "Training Loss: 0.6727732419967651\n",
            "Training Loss: 0.5332165956497192\n",
            "Training Loss: 0.6310644745826721\n",
            "Training Loss: 0.5122767090797424\n",
            "Training Loss: 0.5647541284561157\n",
            "Training Loss: 0.6512212753295898\n",
            "Training Loss: 0.6380912065505981\n",
            "Training Loss: 0.4929591119289398\n",
            "Training Loss: 0.4797705113887787\n",
            "Training Loss: 0.7402588129043579\n",
            "Training Loss: 0.549407958984375\n",
            "Training Loss: 0.626931369304657\n",
            "Training Loss: 0.5792437791824341\n",
            "Training Loss: 0.5708478689193726\n",
            "Training Loss: 0.4924411177635193\n",
            "Training Loss: 0.5318385362625122\n",
            "Training Loss: 0.5482591390609741\n",
            "Training Loss: 0.5899590253829956\n",
            "Training Loss: 0.6089690923690796\n",
            "Training Loss: 0.6558599472045898\n",
            "Training Loss: 0.5385249257087708\n",
            "Training Loss: 0.5708919167518616\n",
            "Training Loss: 0.7125516533851624\n",
            "Training Loss: 0.5403810739517212\n",
            "Training Loss: 0.4579731523990631\n",
            "Training Loss: 0.7336780428886414\n",
            "Training Loss: 0.5833650827407837\n",
            "Training Loss: 0.5594543218612671\n",
            "Training Loss: 0.5953623652458191\n",
            "Training Loss: 0.4846063554286957\n",
            "Training Loss: 0.44544118642807007\n",
            "Training Loss: 0.5250582098960876\n",
            "Training Loss: 0.4550445079803467\n",
            "Training Loss: 0.6230774521827698\n",
            "Training Loss: 0.5956462025642395\n",
            "Training Loss: 0.5285451412200928\n",
            "Training Loss: 0.5584709048271179\n",
            "Training Loss: 0.5812691450119019\n",
            "Training Loss: 0.4421335756778717\n",
            "Training Loss: 0.6416464447975159\n",
            "Training Loss: 0.7294580340385437\n",
            "Training Loss: 0.4579140245914459\n",
            "Training Loss: 0.5047986507415771\n",
            "Training Loss: 0.501213014125824\n",
            "Training Loss: 0.5982894897460938\n",
            "Training Loss: 0.5623049139976501\n",
            "Training Loss: 0.508872389793396\n",
            "Training Loss: 0.7143825888633728\n",
            "Training Loss: 0.41850346326828003\n",
            "Training Loss: 0.5012668371200562\n",
            "Training Loss: 0.5022573471069336\n",
            "Training Loss: 0.5815932154655457\n",
            "Training Loss: 0.46493300795555115\n",
            "Training Loss: 0.5652285218238831\n",
            "Training Loss: 0.6115404367446899\n",
            "Training Loss: 0.567150354385376\n",
            "Training Loss: 0.48437365889549255\n",
            "Training Loss: 0.4957456588745117\n",
            "Training Loss: 0.4411706328392029\n",
            "Training Loss: 0.4859069883823395\n",
            "Training Loss: 0.5605472326278687\n",
            "Training Loss: 0.48470884561538696\n",
            "Training Loss: 0.4187988340854645\n",
            "Training Loss: 0.611259937286377\n",
            "Training Loss: 0.5886859893798828\n",
            "Training Loss: 0.7273787260055542\n",
            "Training Loss: 0.5536328554153442\n",
            "Training Loss: 0.4438759684562683\n",
            "Training Loss: 0.45322877168655396\n",
            "Training Loss: 0.5502504110336304\n",
            "Training Loss: 0.5668705701828003\n",
            "Training Loss: 0.4230869710445404\n",
            "Training Loss: 0.49222859740257263\n",
            "Training Loss: 0.5249537229537964\n",
            "Training Loss: 0.5715221762657166\n",
            "Training Loss: 0.529398500919342\n",
            "Training Loss: 0.5344046354293823\n",
            "Training Loss: 0.4505063593387604\n",
            "Training Loss: 0.7191833257675171\n",
            "Training Loss: 0.4082016348838806\n",
            "Training Loss: 0.613627552986145\n",
            "Training Loss: 0.575343132019043\n",
            "Training Loss: 0.42406922578811646\n",
            "Training Loss: 0.5920065641403198\n",
            "Training Loss: 0.6196967959403992\n",
            "Training Loss: 0.524056613445282\n",
            "Training Loss: 0.5585896372795105\n",
            "Training Loss: 0.4594062864780426\n",
            "Training Loss: 0.632256805896759\n",
            "Training Loss: 0.6626176834106445\n",
            "Training Loss: 0.4925992488861084\n",
            "Training Loss: 0.6095682978630066\n",
            "Training Loss: 0.5370506644248962\n",
            "Training Loss: 0.6452478170394897\n",
            "Training Loss: 0.6581621170043945\n",
            "Training Loss: 0.5129984021186829\n",
            "Training Loss: 0.47058790922164917\n",
            "Training Loss: 0.5371730327606201\n",
            "Training Loss: 0.4227098524570465\n",
            "Training Loss: 0.6272448301315308\n",
            "Training Loss: 0.6040987372398376\n",
            "Training Loss: 0.507219672203064\n",
            "Training Loss: 0.609259307384491\n",
            "Training Loss: 0.49466022849082947\n",
            "Training Loss: 0.5265234708786011\n",
            "Training Loss: 0.5987076759338379\n",
            "Training Loss: 0.6085940003395081\n",
            "Training Loss: 0.5864503979682922\n",
            "Training Loss: 0.4032987654209137\n",
            "Training Loss: 0.572946310043335\n",
            "Training Loss: 0.5111331343650818\n",
            "Training Loss: 0.7225795984268188\n",
            "Training Loss: 0.5607430934906006\n",
            "Training Loss: 0.5864348411560059\n",
            "Training Loss: 0.49447402358055115\n",
            "Training Loss: 0.43734750151634216\n",
            "Training Loss: 0.39162835478782654\n",
            "Training Loss: 0.46010103821754456\n",
            "Training Loss: 0.593821108341217\n",
            "Training Loss: 0.5530418753623962\n",
            "Training Loss: 0.7065659761428833\n",
            "Training Loss: 0.5099256634712219\n",
            "Training Loss: 0.6645427346229553\n",
            "Training Loss: 0.6073693037033081\n",
            "Training Loss: 0.6163485050201416\n",
            "Training Loss: 0.6807181239128113\n",
            "Training Loss: 0.5461479425430298\n",
            "Training Loss: 0.5969091653823853\n",
            "Training Loss: 0.6181338429450989\n",
            "Training Loss: 0.41007980704307556\n",
            "Training Loss: 0.4592015743255615\n",
            "Training Loss: 0.5692303776741028\n",
            "Training Loss: 0.5572497248649597\n",
            "Training Loss: 0.4332618713378906\n",
            "Training Loss: 0.5360614061355591\n",
            "Training Loss: 0.5830761194229126\n",
            "Training Loss: 0.5150197744369507\n",
            "Training Loss: 0.5431252121925354\n",
            "Training Loss: 0.6830094456672668\n",
            "Training Loss: 0.3939141631126404\n",
            "Training Loss: 0.49966907501220703\n",
            "Training Loss: 0.517559289932251\n",
            "Training Loss: 0.6187591552734375\n",
            "Training Loss: 0.5238850116729736\n",
            "Training Loss: 0.7559539675712585\n",
            "Training Loss: 0.6202949285507202\n",
            "Training Loss: 0.6082807779312134\n",
            "Training Loss: 0.46993565559387207\n",
            "Training Loss: 0.4216165542602539\n",
            "Training Loss: 0.6147743463516235\n",
            "Training Loss: 0.7002782821655273\n",
            "Training Loss: 0.4730805456638336\n",
            "Training Loss: 0.5272316336631775\n",
            "Training Loss: 0.3777470886707306\n",
            "Training Loss: 0.5521775484085083\n",
            "Training Loss: 0.5777243375778198\n",
            "Training Loss: 0.6384025812149048\n",
            "Training Loss: 0.4214721620082855\n",
            "Training Loss: 0.46817702054977417\n",
            "Training Loss: 0.6335427761077881\n",
            "Training Loss: 0.4744941294193268\n",
            "Training Loss: 0.4734254777431488\n",
            "Training Loss: 0.5136348009109497\n",
            "Training Loss: 0.4668639302253723\n",
            "Training Loss: 0.7206581830978394\n",
            "Training Loss: 0.34450480341911316\n",
            "Training Loss: 0.4949895441532135\n",
            "Training Loss: 0.5635348558425903\n",
            "Training Loss: 0.6344773769378662\n",
            "Training Loss: 0.5206350684165955\n",
            "Training Loss: 0.6639806032180786\n",
            "Training Loss: 0.40829557180404663\n",
            "Training Loss: 0.4922408163547516\n",
            "Training Loss: 0.5518907904624939\n",
            "Training Loss: 0.5468366742134094\n",
            "Training Loss: 0.5101317167282104\n",
            "Training Loss: 0.49171486496925354\n",
            "Training Loss: 0.5915034413337708\n",
            "Training Loss: 0.3741975426673889\n",
            "Training Loss: 0.5972532033920288\n",
            "Training Loss: 0.4087791442871094\n",
            "Training Loss: 0.5317721366882324\n",
            "Training Loss: 0.4699954092502594\n",
            "Training Loss: 0.3631060719490051\n",
            "Training Loss: 0.5430364608764648\n",
            "Training Loss: 0.3704208433628082\n",
            "Training Loss: 0.5691255331039429\n",
            "Training Loss: 0.3351685404777527\n",
            "Training Loss: 0.4696142077445984\n",
            "Training Loss: 0.4220767915248871\n",
            "Training Loss: 0.45249098539352417\n",
            "Training Loss: 0.4212230145931244\n",
            "Training Loss: 0.4584159255027771\n",
            "Training Loss: 0.47619423270225525\n",
            "Training Loss: 0.38549694418907166\n",
            "Training Loss: 0.5711033940315247\n",
            "Training Loss: 0.4733197093009949\n",
            "Training Loss: 0.3082875907421112\n",
            "Training Loss: 0.4349098205566406\n",
            "Training Loss: 0.4427510201931\n",
            "Training Loss: 0.6214491128921509\n",
            "Training Loss: 0.6397792100906372\n",
            "Training Loss: 0.5686417818069458\n",
            "Training Loss: 0.4210108816623688\n",
            "Training Loss: 0.7436335682868958\n",
            "Training Loss: 0.5411191582679749\n",
            "Training Loss: 0.4201783239841461\n",
            "Training Loss: 0.5303713083267212\n",
            "Training Loss: 0.5114498734474182\n",
            "Training Loss: 0.5312317609786987\n",
            "Training Loss: 0.46392059326171875\n",
            "Training Loss: 0.5014411807060242\n",
            "Training Loss: 0.5014948844909668\n",
            "Training Loss: 0.4580424427986145\n",
            "Training Loss: 0.4617728590965271\n",
            "Training Loss: 0.38747888803482056\n",
            "Training Loss: 0.5799034237861633\n",
            "Training Loss: 0.54725581407547\n",
            "Training Loss: 0.4951113164424896\n",
            "Training Loss: 0.37420520186424255\n",
            "Training Loss: 0.4268952012062073\n",
            "Training Loss: 0.48335379362106323\n",
            "Training Loss: 0.5439200401306152\n",
            "Training Loss: 0.43354591727256775\n",
            "Training Loss: 0.6993209719657898\n",
            "Training Loss: 0.42349377274513245\n",
            "Training Loss: 0.5167208313941956\n",
            "Training Loss: 0.5608978271484375\n",
            "Training Loss: 0.566766619682312\n",
            "Training Loss: 0.5314798355102539\n",
            "Training Loss: 0.5380488634109497\n",
            "Training Loss: 0.5993731021881104\n",
            "Training Loss: 0.49608293175697327\n",
            "Training Loss: 0.6183263063430786\n",
            "Training Loss: 0.4730544984340668\n",
            "Training Loss: 0.3431146740913391\n",
            "Training Loss: 0.4250493049621582\n",
            "Training Loss: 0.4078851640224457\n",
            "Training Loss: 0.7479811906814575\n",
            "Training Loss: 0.5887371897697449\n",
            "Training Loss: 0.443302720785141\n",
            "Training Loss: 0.4274331033229828\n",
            "Training Loss: 0.5355077981948853\n",
            "Training Loss: 0.482526957988739\n",
            "Training Loss: 0.4842386841773987\n",
            "Training Loss: 0.3531971275806427\n",
            "Training Loss: 0.5770909786224365\n",
            "Training Loss: 0.5772189497947693\n",
            "Training Loss: 0.4465377926826477\n",
            "Training Loss: 0.5546339750289917\n",
            "Training Loss: 0.48537546396255493\n",
            "Training Loss: 0.5161648988723755\n",
            "Training Loss: 0.4842103123664856\n",
            "Training Loss: 0.5025488138198853\n",
            "Training Loss: 0.510651707649231\n",
            "Training Loss: 0.5520203709602356\n",
            "Training Loss: 0.4420056939125061\n",
            "Training Loss: 0.5568714141845703\n",
            "Training Loss: 0.4612838625907898\n",
            "Training Loss: 0.7460532784461975\n",
            "Training Loss: 0.60594642162323\n",
            "Training Loss: 0.5071513652801514\n",
            "Training Loss: 0.4528360366821289\n",
            "Training Loss: 0.48948121070861816\n",
            "Training Loss: 0.478442519903183\n",
            "Training Loss: 0.684239387512207\n",
            "Training Loss: 0.6655885577201843\n",
            "Training Loss: 0.5820134878158569\n",
            "Training Loss: 0.4795425832271576\n",
            "Training Loss: 0.4077931344509125\n",
            "Training Loss: 0.4261295199394226\n",
            "Training Loss: 0.4648398756980896\n",
            "Training Loss: 0.5020700097084045\n",
            "Training Loss: 0.32810065150260925\n",
            "Training Loss: 0.469879150390625\n",
            "Training Loss: 0.5605999231338501\n",
            "Training Loss: 0.6677657961845398\n",
            "Training Loss: 0.364942729473114\n",
            "Training Loss: 0.7451661825180054\n",
            "Training Loss: 0.5455925464630127\n",
            "Training Loss: 0.4201573431491852\n",
            "Training Loss: 0.5333558917045593\n",
            "Training Loss: 0.5476157069206238\n",
            "Training Loss: 0.653170108795166\n",
            "Training Loss: 0.39405906200408936\n",
            "Training Loss: 0.3219214975833893\n",
            "Training Loss: 0.5129914283752441\n",
            "Training Loss: 0.5733753442764282\n",
            "Training Loss: 0.46841058135032654\n",
            "Training Loss: 0.48099708557128906\n",
            "Training Loss: 0.39193564653396606\n",
            "Training Loss: 0.43326535820961\n",
            "Training Loss: 0.40697869658470154\n",
            "Training Loss: 0.556268572807312\n",
            "Training Loss: 0.3401213586330414\n",
            "Training Loss: 0.6457428932189941\n",
            "Training Loss: 0.5080209970474243\n",
            "Training Loss: 0.5415464639663696\n",
            "Training Loss: 0.4994431138038635\n",
            "Training Loss: 0.53716641664505\n",
            "Training Loss: 0.6654549837112427\n",
            "Training Loss: 0.6965993046760559\n",
            "Training Loss: 0.7023663520812988\n",
            "Training Loss: 0.37999746203422546\n",
            "Training Loss: 0.5258550643920898\n",
            "Training Loss: 0.4391317367553711\n",
            "Training Loss: 0.3764115273952484\n",
            "Training Loss: 0.49698692560195923\n",
            "Training Loss: 0.6326537132263184\n",
            "Training Loss: 0.42479023337364197\n",
            "Training Loss: 0.3471671938896179\n",
            "Training Loss: 0.555782675743103\n",
            "Training Loss: 0.5188629627227783\n",
            "Training Loss: 0.5725396275520325\n",
            "Training Loss: 0.41332441568374634\n",
            "Training Loss: 0.3816632628440857\n",
            "Training Loss: 0.43170875310897827\n",
            "Training Loss: 0.5957511067390442\n",
            "Training Loss: 0.31594544649124146\n",
            "Training Loss: 0.40134182572364807\n",
            "Training Loss: 0.6170340180397034\n",
            "Training Loss: 0.4821999967098236\n",
            "Training Loss: 0.5722514986991882\n",
            "Training Loss: 0.534504771232605\n",
            "Training Loss: 0.4383290410041809\n",
            "Training Loss: 0.4262975752353668\n",
            "Training Loss: 0.6072781085968018\n",
            "Training Loss: 0.4263926148414612\n",
            "Training Loss: 0.6442801356315613\n",
            "Training Loss: 0.7920743227005005\n",
            "Training Loss: 0.35272935032844543\n",
            "Training Loss: 0.3480146527290344\n",
            "Training Loss: 0.4179707467556\n",
            "Training Loss: 0.6366602778434753\n",
            "Training Loss: 0.586789071559906\n",
            "Training Loss: 0.4700845777988434\n",
            "Training Loss: 0.33408206701278687\n",
            "Training Loss: 0.8871979713439941\n",
            "Training Loss: 0.533562183380127\n",
            "Training Loss: 0.5322164297103882\n",
            "Training Loss: 0.37224632501602173\n",
            "Training Loss: 0.27203255891799927\n",
            "Training Loss: 0.5212664604187012\n",
            "Training Loss: 0.4324313700199127\n",
            "Training Loss: 0.5077667832374573\n",
            "Training Loss: 0.48600512742996216\n",
            "Training Loss: 0.34918808937072754\n",
            "Training Loss: 0.6791373491287231\n",
            "Training Loss: 0.37315696477890015\n",
            "Training Loss: 0.5924960374832153\n",
            "Training Loss: 0.44832953810691833\n",
            "Training Loss: 0.5030748248100281\n",
            "Training Loss: 0.5142918825149536\n",
            "Training Loss: 0.5227102041244507\n",
            "Training Loss: 0.6282049417495728\n",
            "Training Loss: 0.38125768303871155\n",
            "Training Loss: 0.3919900059700012\n",
            "Training Loss: 0.44635456800460815\n",
            "Training Loss: 0.6650059819221497\n",
            "Training Loss: 0.35368290543556213\n",
            "Training Loss: 0.4577801823616028\n",
            "Training Loss: 0.5248841047286987\n",
            "Training Loss: 0.5537461638450623\n",
            "Training Loss: 0.4201003909111023\n",
            "Training Loss: 0.2944575250148773\n",
            "Training Loss: 0.42219099402427673\n",
            "Training Loss: 0.45829135179519653\n",
            "Training Loss: 0.3483836054801941\n",
            "Training Loss: 0.4333013892173767\n",
            "Training Loss: 0.34958529472351074\n",
            "Training Loss: 0.5080665349960327\n",
            "Training Loss: 0.47543030977249146\n",
            "Training Loss: 0.48607176542282104\n",
            "Training Loss: 0.5760311484336853\n",
            "Training Loss: 0.4029860496520996\n",
            "Training Loss: 0.5182722210884094\n",
            "Training Loss: 0.4232942461967468\n",
            "Training Loss: 0.3948010206222534\n",
            "Training Loss: 0.3930211067199707\n",
            "Training Loss: 0.42187437415122986\n",
            "Training Loss: 0.5598949790000916\n",
            "Training Loss: 0.4861406683921814\n",
            "Training Loss: 0.4863024652004242\n",
            "Training Loss: 0.38906872272491455\n",
            "Training Loss: 0.4588906168937683\n",
            "Training Loss: 0.5299721956253052\n",
            "Training Loss: 0.5541366338729858\n",
            "Training Loss: 0.5570803880691528\n",
            "Training Loss: 0.38010457158088684\n",
            "Training Loss: 0.5100805759429932\n",
            "Training Loss: 0.29501110315322876\n",
            "Training Loss: 0.5107141137123108\n",
            "Training Loss: 0.6378632187843323\n",
            "Training Loss: 0.4561953544616699\n",
            "Training Loss: 0.47424429655075073\n",
            "Training Loss: 0.34680497646331787\n",
            "Training Loss: 0.42908692359924316\n",
            "Training Loss: 0.30168092250823975\n",
            "Training Loss: 0.47007808089256287\n",
            "Training Loss: 0.6402633786201477\n",
            "Training Loss: 0.44624337553977966\n",
            "Training Loss: 0.28402918577194214\n",
            "Training Loss: 0.3900742828845978\n",
            "Training Loss: 0.24108150601387024\n",
            "Training Loss: 0.3877202868461609\n",
            "Training Loss: 0.4236608147621155\n",
            "Training Loss: 0.6046122312545776\n",
            "Training Loss: 0.5196436643600464\n",
            "Training Loss: 0.3596394956111908\n",
            "Training Loss: 0.43605875968933105\n",
            "Training Loss: 0.36310434341430664\n",
            "Training Loss: 0.5906071066856384\n",
            "Training Loss: 0.7905246615409851\n",
            "Training Loss: 0.35157451033592224\n",
            "Training Loss: 0.6595376133918762\n",
            "Training Loss: 0.419626384973526\n",
            "Training Loss: 0.23923340439796448\n",
            "Training Loss: 0.40869975090026855\n",
            "Training Loss: 0.5668858289718628\n",
            "Training Loss: 0.38943997025489807\n",
            "Training Loss: 0.4088119566440582\n",
            "Training Loss: 0.27354440093040466\n",
            "Training Loss: 0.386059045791626\n",
            "Training Loss: 0.4490681290626526\n",
            "Training Loss: 0.30426111817359924\n",
            "Training Loss: 0.40958231687545776\n",
            "Training Loss: 0.7000831961631775\n",
            "Training Loss: 0.4539894163608551\n",
            "Training Loss: 0.2885027825832367\n",
            "Training Loss: 0.40809041261672974\n",
            "Training Loss: 0.44864732027053833\n",
            "Training Loss: 0.6640397906303406\n",
            "Training Loss: 0.5712687373161316\n",
            "Training Loss: 0.3908872604370117\n",
            "Training Loss: 0.485339879989624\n",
            "Training Loss: 0.5175009965896606\n",
            "Training Loss: 0.43755975365638733\n",
            "Training Loss: 0.45101675391197205\n",
            "Training Loss: 0.644970715045929\n",
            "Training Loss: 0.6434561610221863\n",
            "Training Loss: 0.41377463936805725\n",
            "Training Loss: 0.510064423084259\n",
            "Training Loss: 0.5844462513923645\n",
            "Training Loss: 0.3619081676006317\n",
            "Training Loss: 0.43070951104164124\n",
            "Training Loss: 0.48623037338256836\n",
            "Training Loss: 0.4804827570915222\n",
            "Training Loss: 0.39087677001953125\n",
            "Training Loss: 0.5123587250709534\n",
            "Training Loss: 0.6995989084243774\n",
            "Training Loss: 0.49797898530960083\n",
            "Training Loss: 0.5080896615982056\n",
            "Training Loss: 0.4170767664909363\n",
            "Training Loss: 0.41091638803482056\n",
            "Training Loss: 0.45150595903396606\n",
            "Training Loss: 0.3579152822494507\n",
            "Training Loss: 0.3079451322555542\n",
            "Training Loss: 0.3778151571750641\n",
            "Training Loss: 0.4574964940547943\n",
            "Training Loss: 0.4428865313529968\n",
            "Training Loss: 0.5205129384994507\n",
            "Training Loss: 0.5380512475967407\n",
            "Training Loss: 0.36650484800338745\n",
            "Training Loss: 0.45752519369125366\n",
            "Training Loss: 0.3811078667640686\n",
            "Training Loss: 0.4818250238895416\n",
            "Training Loss: 0.47179049253463745\n",
            "Training Loss: 0.45348507165908813\n",
            "Training Loss: 0.4268111288547516\n",
            "Training Loss: 0.575183629989624\n",
            "Training Loss: 0.46629470586776733\n",
            "Training Loss: 0.4109194874763489\n",
            "Training Loss: 0.3761942386627197\n",
            "Training Loss: 0.363157719373703\n",
            "Training Loss: 0.35112300515174866\n",
            "Training Loss: 0.4530337452888489\n",
            "Training Loss: 0.3343260586261749\n",
            "Training Loss: 0.4673220217227936\n",
            "Training Loss: 0.3611697256565094\n",
            "Training Loss: 0.5619422197341919\n",
            "Training Loss: 0.5518607497215271\n",
            "Training Loss: 0.39395269751548767\n",
            "Training Loss: 0.5556062459945679\n",
            "Training Loss: 0.31147056818008423\n",
            "Training Loss: 0.28765279054641724\n",
            "Training Loss: 0.46486836671829224\n",
            "Training Loss: 0.4932588040828705\n",
            "Training Loss: 0.4079800546169281\n",
            "Training Loss: 0.3894740045070648\n",
            "Training Loss: 0.4478452801704407\n",
            "Training Loss: 0.5242165327072144\n",
            "Training Loss: 0.4158664345741272\n",
            "Training Loss: 0.34365352988243103\n",
            "Training Loss: 0.4784272313117981\n",
            "Training Loss: 0.4568091034889221\n",
            "Training Loss: 0.4493272304534912\n",
            "Training Loss: 0.6380594968795776\n",
            "Training Loss: 0.6048895716667175\n",
            "Training Loss: 0.511401355266571\n",
            "Training Loss: 0.4665932059288025\n",
            "Training Loss: 0.5169686675071716\n",
            "Training Loss: 0.38676685094833374\n",
            "Training Loss: 0.4245983958244324\n",
            "Training Loss: 0.5322265625\n",
            "Training Loss: 0.49486130475997925\n",
            "Training Loss: 0.4847966134548187\n",
            "Training Loss: 0.4332234263420105\n",
            "Training Loss: 0.7841508984565735\n",
            "Training Loss: 0.48704665899276733\n",
            "Training Loss: 0.41890400648117065\n",
            "Training Loss: 0.37553855776786804\n",
            "Training Loss: 0.5523932576179504\n",
            "Training Loss: 0.5072986483573914\n",
            "Training Loss: 0.46940574049949646\n",
            "Training Loss: 0.4328348636627197\n",
            "Training Loss: 0.38657742738723755\n",
            "Training Loss: 0.3096888065338135\n",
            "Training Loss: 0.3854570984840393\n",
            "Training Loss: 0.30436190962791443\n",
            "Training Loss: 0.32663944363594055\n",
            "Training Loss: 0.5058806538581848\n",
            "Training Loss: 0.27879974246025085\n",
            "Training Loss: 0.5054944753646851\n",
            "Training Loss: 0.3329315781593323\n",
            "Training Loss: 0.4270761013031006\n",
            "Training Loss: 0.2909711003303528\n",
            "Training Loss: 0.48841315507888794\n",
            "Training Loss: 0.4017275869846344\n",
            "Training Loss: 0.32519617676734924\n",
            "Training Loss: 0.4523100256919861\n",
            "Training Loss: 0.4974132180213928\n",
            "Training Loss: 0.4044172763824463\n",
            "Training Loss: 0.44406193494796753\n",
            "Training Loss: 0.2927800416946411\n",
            "Training Loss: 0.4035791754722595\n",
            "Training Loss: 0.5532343983650208\n",
            "Training Loss: 0.4081514775753021\n",
            "Training Loss: 0.334576815366745\n",
            "Training Loss: 0.456839382648468\n",
            "Training Loss: 0.3401584327220917\n",
            "Training Loss: 0.43049097061157227\n",
            "Training Loss: 0.39689740538597107\n",
            "Training Loss: 0.34027695655822754\n",
            "Training Loss: 0.3557676374912262\n",
            "Training Loss: 0.3734756112098694\n",
            "Training Loss: 0.3549766540527344\n",
            "Training Loss: 0.4054732322692871\n",
            "Training Loss: 0.3819515109062195\n",
            "Training Loss: 0.38671156764030457\n",
            "Training Loss: 0.40166839957237244\n",
            "Training Loss: 0.6034340858459473\n",
            "Training Loss: 0.4459569454193115\n",
            "Training Loss: 0.35094261169433594\n",
            "Training Loss: 0.34278953075408936\n",
            "Training Loss: 0.4169258177280426\n",
            "Training Loss: 0.422384649515152\n",
            "Training Loss: 0.3862965703010559\n",
            "Training Loss: 0.3760778307914734\n",
            "Training Loss: 0.30013811588287354\n",
            "Training Loss: 0.35476669669151306\n",
            "Training Loss: 0.2881762981414795\n",
            "Training Loss: 0.43021875619888306\n",
            "Training Loss: 0.4451926350593567\n",
            "Training Loss: 0.269248902797699\n",
            "Training Loss: 0.5792518258094788\n",
            "Training Loss: 0.3326760232448578\n",
            "Training Loss: 0.36607852578163147\n",
            "Training Loss: 0.5121785402297974\n",
            "Training Loss: 0.4254727363586426\n",
            "Training Loss: 0.38545066118240356\n",
            "Training Loss: 0.270565390586853\n",
            "Training Loss: 0.3759079575538635\n",
            "Training Loss: 0.3092617690563202\n",
            "Training Loss: 0.40739092230796814\n",
            "Training Loss: 0.2769748568534851\n",
            "Training Loss: 0.36436957120895386\n",
            "Training Loss: 0.37718623876571655\n",
            "Training Loss: 0.3209834694862366\n",
            "Training Loss: 0.2953019440174103\n",
            "Training Loss: 0.3505948781967163\n",
            "Training Loss: 0.33534952998161316\n",
            "Training Loss: 0.361806720495224\n",
            "Training Loss: 0.3689020276069641\n",
            "Training Loss: 0.31756797432899475\n",
            "Training Loss: 0.38997188210487366\n",
            "Training Loss: 0.36908721923828125\n",
            "Training Loss: 0.48382216691970825\n",
            "Training Loss: 0.3782978057861328\n",
            "Training Loss: 0.44455185532569885\n",
            "Training Loss: 0.39298680424690247\n",
            "Training Loss: 0.33598050475120544\n",
            "Training Loss: 0.36288708448410034\n",
            "Training Loss: 0.41142019629478455\n",
            "Training Loss: 0.31792476773262024\n",
            "Training Loss: 0.4915499687194824\n",
            "Training Loss: 0.4539887011051178\n",
            "Training Loss: 0.4042500853538513\n",
            "Training Loss: 0.3025161623954773\n",
            "Training Loss: 0.3160932660102844\n",
            "Training Loss: 0.36321085691452026\n",
            "Training Loss: 0.3753806948661804\n",
            "Training Loss: 0.3448217213153839\n",
            "Training Loss: 0.4374193549156189\n",
            "Training Loss: 0.3267321288585663\n",
            "Training Loss: 0.3790995478630066\n",
            "Training Loss: 0.39249950647354126\n",
            "Training Loss: 0.37644124031066895\n",
            "Training Loss: 0.45973554253578186\n",
            "Training Loss: 0.3942597508430481\n",
            "Training Loss: 0.40697336196899414\n",
            "Training Loss: 0.3215221166610718\n",
            "Training Loss: 0.37549644708633423\n",
            "Training Loss: 0.3429885804653168\n",
            "Training Loss: 0.2893444299697876\n",
            "Training Loss: 0.36467352509498596\n",
            "Training Loss: 0.3327294886112213\n",
            "Training Loss: 0.29267168045043945\n",
            "Training Loss: 0.3778802752494812\n",
            "Training Loss: 0.3774343430995941\n",
            "Training Loss: 0.4540468752384186\n",
            "Training Loss: 0.40028849244117737\n",
            "Training Loss: 0.4076969027519226\n",
            "Training Loss: 0.3098680078983307\n",
            "Training Loss: 0.39878907799720764\n",
            "Training Loss: 0.46796584129333496\n",
            "Training Loss: 0.3268352448940277\n",
            "Training Loss: 0.2730981409549713\n",
            "Training Loss: 0.4407216012477875\n",
            "Training Loss: 0.32776835560798645\n",
            "Training Loss: 0.3158903419971466\n",
            "Training Loss: 0.3481787145137787\n",
            "Training Loss: 0.35668274760246277\n",
            "Training Loss: 0.34897080063819885\n",
            "Training Loss: 0.2607055604457855\n",
            "Training Loss: 0.3511061668395996\n",
            "Training Loss: 0.4170350134372711\n",
            "Training Loss: 0.3573932647705078\n",
            "Training Loss: 0.3482514023780823\n",
            "Training Loss: 0.31238648295402527\n",
            "Training Loss: 0.3484339118003845\n",
            "Training Loss: 0.3435879349708557\n",
            "Training Loss: 0.32073092460632324\n",
            "Training Loss: 0.2897055745124817\n",
            "Training Loss: 0.347259521484375\n",
            "Training Loss: 0.28423234820365906\n",
            "Training Loss: 0.3444977402687073\n",
            "Training Loss: 0.3395516872406006\n",
            "Training Loss: 0.3370128571987152\n",
            "Training Loss: 0.3342672288417816\n",
            "Training Loss: 0.27562302350997925\n",
            "Training Loss: 0.36764854192733765\n",
            "Training Loss: 0.3457692563533783\n",
            "Training Loss: 0.349466472864151\n",
            "Training Loss: 0.3245280981063843\n",
            "Training Loss: 0.3561277389526367\n",
            "Training Loss: 0.381417453289032\n",
            "Training Loss: 0.3852669298648834\n",
            "Training Loss: 0.33121585845947266\n",
            "Training Loss: 0.31526511907577515\n",
            "Training Loss: 0.32100725173950195\n",
            "Training Loss: 0.31561869382858276\n",
            "Training Loss: 0.27325138449668884\n",
            "Training Loss: 0.30791541934013367\n",
            "Training Loss: 0.3682488799095154\n",
            "Training Loss: 0.2716672420501709\n",
            "Training Loss: 0.28727781772613525\n",
            "Training Loss: 0.3386009931564331\n",
            "Training Loss: 0.3491378724575043\n",
            "Training Loss: 0.35217946767807007\n",
            "Training Loss: 0.2194242775440216\n",
            "Training Loss: 0.2727389633655548\n",
            "Training Loss: 0.2700682282447815\n",
            "Training Loss: 0.31640344858169556\n",
            "Training Loss: 0.28348779678344727\n",
            "Training Loss: 0.2585538625717163\n",
            "Training Loss: 0.3505958616733551\n",
            "Training Loss: 0.27571409940719604\n",
            "Training Loss: 0.33633866906166077\n",
            "Training Loss: 0.3503710925579071\n",
            "Training Loss: 0.2976340353488922\n",
            "Training Loss: 0.23808176815509796\n",
            "Training Loss: 0.30785882472991943\n",
            "Training Loss: 0.299734890460968\n",
            "Training Loss: 0.25683388113975525\n",
            "Training Loss: 0.2971930205821991\n",
            "Training Loss: 0.29763609170913696\n",
            "Training Loss: 0.2929965853691101\n",
            "Training Loss: 0.30852657556533813\n",
            "Training Loss: 0.2872171998023987\n",
            "Training Loss: 0.34618908166885376\n",
            "Training Loss: 0.335600882768631\n",
            "Training Loss: 0.30262279510498047\n",
            "Training Loss: 0.27127936482429504\n",
            "Training Loss: 0.2306351661682129\n",
            "Training Loss: 0.27470633387565613\n",
            "Training Loss: 0.24763818085193634\n",
            "Training Loss: 0.2792072296142578\n",
            "Training Loss: 0.2530899941921234\n",
            "Training Loss: 0.31285539269447327\n",
            "Training Loss: 0.30788707733154297\n",
            "Training Loss: 0.3044191002845764\n",
            "Training Loss: 0.2997063100337982\n",
            "Training Loss: 0.22939184308052063\n",
            "Training Loss: 0.2798807621002197\n",
            "Training Loss: 0.2998589873313904\n",
            "Training Loss: 0.2816426157951355\n",
            "Training Loss: 0.30800920724868774\n",
            "Training Loss: 0.2936151623725891\n",
            "Training Loss: 0.25209158658981323\n",
            "Training Loss: 0.31220489740371704\n",
            "Training Loss: 0.23355886340141296\n",
            "Training Loss: 0.2545631527900696\n",
            "Training Loss: 0.2714473605155945\n",
            "Training Loss: 0.2519041895866394\n",
            "Training Loss: 0.26214781403541565\n",
            "Training Loss: 0.28731876611709595\n",
            "Training Loss: 0.2720624804496765\n",
            "Training Loss: 0.28015607595443726\n",
            "Training Loss: 0.31777071952819824\n",
            "Training Loss: 0.28558745980262756\n",
            "Training Loss: 0.252859890460968\n",
            "Training Loss: 0.22264495491981506\n",
            "Training Loss: 0.3033736050128937\n",
            "Training Loss: 0.30596011877059937\n",
            "Training Loss: 0.28753894567489624\n",
            "Training Loss: 0.258811354637146\n",
            "Training Loss: 0.2590124011039734\n",
            "Training Loss: 0.32029104232788086\n",
            "Training Loss: 0.26435306668281555\n",
            "Training Loss: 0.30349674820899963\n",
            "Training Loss: 0.23763108253479004\n",
            "Training Loss: 0.28837209939956665\n",
            "Training Loss: 0.28563761711120605\n",
            "Training Loss: 0.27496394515037537\n",
            "Training Loss: 0.2659061551094055\n",
            "Training Loss: 0.23863866925239563\n",
            "Training Loss: 0.29862481355667114\n",
            "Training Loss: 0.32293087244033813\n",
            "Training Loss: 0.23269498348236084\n",
            "Training Loss: 0.2341955453157425\n",
            "Training Loss: 0.2325294464826584\n",
            "Training Loss: 0.268604576587677\n",
            "Training Loss: 0.20483234524726868\n",
            "Training Loss: 0.22718045115470886\n",
            "Training Loss: 0.29663389921188354\n",
            "Training Loss: 0.2602549195289612\n",
            "Training Loss: 0.25975191593170166\n",
            "Training Loss: 0.2659679055213928\n",
            "Training Loss: 0.2681024372577667\n",
            "Training Loss: 0.3014495074748993\n",
            "Training Loss: 0.32095232605934143\n",
            "Training Loss: 0.24971583485603333\n",
            "Training Loss: 0.28138467669487\n",
            "Training Loss: 0.2346714287996292\n",
            "Training Loss: 0.2440100610256195\n",
            "Training Loss: 0.24884340167045593\n",
            "Training Loss: 0.20436760783195496\n",
            "Training Loss: 0.23573315143585205\n",
            "Training Loss: 0.2472263127565384\n",
            "Training Loss: 0.23401913046836853\n",
            "Training Loss: 0.23830528557300568\n",
            "Training Loss: 0.23185274004936218\n",
            "Training Loss: 0.2706655263900757\n",
            "Training Loss: 0.2168962061405182\n",
            "Training Loss: 0.25724172592163086\n",
            "Training Loss: 0.24955499172210693\n",
            "Training Loss: 0.19878901541233063\n",
            "Training Loss: 0.29892367124557495\n",
            "Training Loss: 0.2058853656053543\n",
            "Training Loss: 0.20058313012123108\n",
            "Training Loss: 0.21089057624340057\n",
            "Training Loss: 0.2829952836036682\n",
            "Training Loss: 0.26067882776260376\n",
            "Training Loss: 0.260344535112381\n",
            "Training Loss: 0.2958568036556244\n",
            "Training Loss: 0.23354902863502502\n",
            "Training Loss: 0.23396846652030945\n",
            "Training Loss: 0.22366270422935486\n",
            "Training Loss: 0.20931001007556915\n",
            "Training Loss: 0.2697734534740448\n",
            "Training Loss: 0.2816084027290344\n",
            "Training Loss: 0.2600038945674896\n",
            "Training Loss: 0.2674857974052429\n",
            "Training Loss: 0.26092439889907837\n",
            "Training Loss: 0.2547866404056549\n",
            "Training Loss: 0.22009341418743134\n",
            "Training Loss: 0.26979750394821167\n",
            "Training Loss: 0.21042780578136444\n",
            "Training Loss: 0.2073037326335907\n",
            "Training Loss: 0.2631191611289978\n",
            "Training Loss: 0.2467230260372162\n",
            "Training Loss: 0.21517130732536316\n",
            "Training Loss: 0.2741941809654236\n",
            "Training Loss: 0.26346006989479065\n",
            "Training Loss: 0.26581913232803345\n",
            "Training Loss: 0.2711016535758972\n",
            "Training Loss: 0.22066092491149902\n",
            "Training Loss: 0.2272588461637497\n",
            "Training Loss: 0.18275399506092072\n",
            "Training Loss: 0.24419467151165009\n",
            "Training Loss: 0.22175459563732147\n",
            "Training Loss: 0.20671498775482178\n",
            "Training Loss: 0.23608095943927765\n",
            "Training Loss: 0.2825582027435303\n",
            "Training Loss: 0.2023947685956955\n",
            "Training Loss: 0.22000309824943542\n",
            "Training Loss: 0.24688291549682617\n",
            "Training Loss: 0.22040843963623047\n",
            "Training Loss: 0.20777520537376404\n",
            "Training Loss: 0.18731114268302917\n",
            "Training Loss: 0.22039520740509033\n",
            "Training Loss: 0.2712516188621521\n",
            "Training Loss: 0.25210148096084595\n",
            "Training Loss: 0.252707302570343\n",
            "Training Loss: 0.3020652234554291\n",
            "Training Loss: 0.19859197735786438\n",
            "Training Loss: 0.21097266674041748\n",
            "Training Loss: 0.20901675522327423\n",
            "Training Loss: 0.20550093054771423\n",
            "Training Loss: 0.23404157161712646\n",
            "Training Loss: 0.22069939970970154\n",
            "Training Loss: 0.2060530185699463\n",
            "Training Loss: 0.2609671950340271\n",
            "Training Loss: 0.1921313852071762\n",
            "Training Loss: 0.28000521659851074\n",
            "Training Loss: 0.21631844341754913\n",
            "Training Loss: 0.2483394593000412\n",
            "Training Loss: 0.1931518167257309\n",
            "Training Loss: 0.24333567917346954\n",
            "Training Loss: 0.28008776903152466\n",
            "Training Loss: 0.2458568513393402\n",
            "Training Loss: 0.21214289963245392\n",
            "Training Loss: 0.2559514343738556\n",
            "Training Loss: 0.19094237685203552\n",
            "Training Loss: 0.1981492042541504\n",
            "Training Loss: 0.23403385281562805\n",
            "Training Loss: 0.21730920672416687\n",
            "Training Loss: 0.2368149757385254\n",
            "Training Loss: 0.23985691368579865\n",
            "Training Loss: 0.21421703696250916\n",
            "Training Loss: 0.2153191864490509\n",
            "Training Loss: 0.2500883638858795\n",
            "Training Loss: 0.20585021376609802\n",
            "Training Loss: 0.1829071342945099\n",
            "Training Loss: 0.21213248372077942\n",
            "Training Loss: 0.24519994854927063\n",
            "Training Loss: 0.2622716426849365\n",
            "Training Loss: 0.1660388708114624\n",
            "Training Loss: 0.1678028404712677\n",
            "Training Loss: 0.23655343055725098\n",
            "Training Loss: 0.22559376060962677\n",
            "Training Loss: 0.2101573497056961\n",
            "Training Loss: 0.20675702393054962\n",
            "Training Loss: 0.24667075276374817\n",
            "Training Loss: 0.2052585631608963\n",
            "Training Loss: 0.22870317101478577\n",
            "Training Loss: 0.20410379767417908\n",
            "Training Loss: 0.23378512263298035\n",
            "Training Loss: 0.21243128180503845\n",
            "Training Loss: 0.2204076498746872\n",
            "Training Loss: 0.19108515977859497\n",
            "Training Loss: 0.23903724551200867\n",
            "Training Loss: 0.23649826645851135\n",
            "Training Loss: 0.21417251229286194\n",
            "Training Loss: 0.24836322665214539\n",
            "Training Loss: 0.19491097331047058\n",
            "Training Loss: 0.1972414255142212\n",
            "Training Loss: 0.26658114790916443\n",
            "Training Loss: 0.18472735583782196\n",
            "Training Loss: 0.19016863405704498\n",
            "Training Loss: 0.22325214743614197\n",
            "Training Loss: 0.22806739807128906\n",
            "Training Loss: 0.27978771924972534\n",
            "Training Loss: 0.19589856266975403\n",
            "Training Loss: 0.22391918301582336\n",
            "Training Loss: 0.15444889664649963\n",
            "Training Loss: 0.25098031759262085\n",
            "Training Loss: 0.2264155149459839\n",
            "Training Loss: 0.1914227157831192\n",
            "Training Loss: 0.25719210505485535\n",
            "Training Loss: 0.17946918308734894\n",
            "Training Loss: 0.19315192103385925\n",
            "Training Loss: 0.20699629187583923\n",
            "Training Loss: 0.19743295013904572\n",
            "Training Loss: 0.2356976568698883\n",
            "Training Loss: 0.23037084937095642\n",
            "Training Loss: 0.21633020043373108\n",
            "Training Loss: 0.2049468457698822\n",
            "Training Loss: 0.16612929105758667\n",
            "Training Loss: 0.19579370319843292\n",
            "Training Loss: 0.14854088425636292\n",
            "Training Loss: 0.22085094451904297\n",
            "Training Loss: 0.24395053088665009\n",
            "Training Loss: 0.19634340703487396\n",
            "Training Loss: 0.1881738305091858\n",
            "Training Loss: 0.200903981924057\n",
            "Training Loss: 0.23190638422966003\n",
            "Training Loss: 0.19152267277240753\n",
            "Training Loss: 0.15912748873233795\n",
            "Training Loss: 0.23549509048461914\n",
            "Training Loss: 0.21347975730895996\n",
            "Training Loss: 0.21786203980445862\n",
            "Training Loss: 0.1826760619878769\n",
            "Training Loss: 0.200374573469162\n",
            "Training Loss: 0.18283972144126892\n",
            "Training Loss: 0.17678578197956085\n",
            "Training Loss: 0.21623504161834717\n",
            "Training Loss: 0.23852232098579407\n",
            "Training Loss: 0.21489934623241425\n",
            "Training Loss: 0.18411333858966827\n",
            "Training Loss: 0.18856677412986755\n",
            "Training Loss: 0.20599429309368134\n",
            "Training Loss: 0.23654699325561523\n",
            "Training Loss: 0.1794058382511139\n",
            "Training Loss: 0.182978093624115\n",
            "Training Loss: 0.1431923359632492\n",
            "Training Loss: 0.22534818947315216\n",
            "Training Loss: 0.18760253489017487\n",
            "Training Loss: 0.1930883824825287\n",
            "Training Loss: 0.2340056449174881\n",
            "Training Loss: 0.19459271430969238\n",
            "Training Loss: 0.17024283111095428\n",
            "Training Loss: 0.2568994164466858\n",
            "Training Loss: 0.2070862352848053\n",
            "Training Loss: 0.18700167536735535\n",
            "Training Loss: 0.164696604013443\n",
            "Training Loss: 0.18042992055416107\n",
            "Training Loss: 0.23490580916404724\n",
            "Training Loss: 0.16285918653011322\n",
            "Training Loss: 0.19014859199523926\n",
            "Training Loss: 0.17709565162658691\n",
            "Training Loss: 0.16517966985702515\n",
            "Training Loss: 0.19523116946220398\n",
            "Training Loss: 0.18764209747314453\n",
            "Training Loss: 0.21299631893634796\n",
            "Training Loss: 0.17354747653007507\n",
            "Training Loss: 0.16468827426433563\n",
            "Training Loss: 0.19300433993339539\n",
            "Training Loss: 0.20557384192943573\n",
            "Training Loss: 0.17343148589134216\n",
            "Training Loss: 0.17760851979255676\n",
            "Training Loss: 0.18748901784420013\n",
            "Training Loss: 0.23025746643543243\n",
            "Training Loss: 0.18268170952796936\n",
            "Training Loss: 0.17815937101840973\n",
            "Training Loss: 0.21744105219841003\n",
            "Training Loss: 0.17988359928131104\n",
            "Training Loss: 0.20543435215950012\n",
            "Training Loss: 0.1760750263929367\n",
            "Training Loss: 0.17077089846134186\n",
            "Training Loss: 0.1718168705701828\n",
            "Training Loss: 0.19931457936763763\n",
            "Training Loss: 0.20336537063121796\n",
            "Training Loss: 0.22423997521400452\n",
            "Training Loss: 0.2100432813167572\n",
            "Training Loss: 0.17111191153526306\n",
            "Training Loss: 0.22394323348999023\n",
            "Training Loss: 0.16390493512153625\n",
            "Training Loss: 0.16416355967521667\n",
            "Training Loss: 0.17368893325328827\n",
            "Training Loss: 0.16894206404685974\n",
            "Training Loss: 0.206608384847641\n",
            "Training Loss: 0.18004010617733002\n",
            "Training Loss: 0.2081579715013504\n",
            "Training Loss: 0.2053767740726471\n",
            "Training Loss: 0.20193085074424744\n",
            "Training Loss: 0.20270662009716034\n",
            "Training Loss: 0.16191469132900238\n",
            "Training Loss: 0.16825148463249207\n",
            "Training Loss: 0.1906217634677887\n",
            "Training Loss: 0.1583258956670761\n",
            "Training Loss: 0.1532086282968521\n",
            "Training Loss: 0.20800122618675232\n",
            "Training Loss: 0.18106940388679504\n",
            "Training Loss: 0.18020395934581757\n",
            "Training Loss: 0.17727148532867432\n",
            "Training Loss: 0.20373137295246124\n",
            "Training Loss: 0.21704423427581787\n",
            "Training Loss: 0.23564307391643524\n",
            "Training Loss: 0.1982678920030594\n",
            "Training Loss: 0.1621883511543274\n",
            "Training Loss: 0.1678129881620407\n",
            "Training Loss: 0.1749112606048584\n",
            "Training Loss: 0.18701288104057312\n",
            "Training Loss: 0.18543066084384918\n",
            "Training Loss: 0.1922883838415146\n",
            "Training Loss: 0.16791269183158875\n",
            "Training Loss: 0.23391230404376984\n",
            "Training Loss: 0.15419714152812958\n",
            "Training Loss: 0.1767553985118866\n",
            "Training Loss: 0.1639956533908844\n",
            "Training Loss: 0.1668349653482437\n",
            "Training Loss: 0.15187406539916992\n",
            "Training Loss: 0.18574027717113495\n",
            "Training Loss: 0.17964312434196472\n",
            "Training Loss: 0.18603454530239105\n",
            "Training Loss: 0.1720084697008133\n",
            "Training Loss: 0.16975772380828857\n",
            "Training Loss: 0.14652766287326813\n",
            "Training Loss: 0.16856878995895386\n",
            "Training Loss: 0.1657043695449829\n",
            "Training Loss: 0.139034241437912\n",
            "Training Loss: 0.14850838482379913\n",
            "Training Loss: 0.22302600741386414\n",
            "Training Loss: 0.12947015464305878\n",
            "Training Loss: 0.1549701988697052\n",
            "Training Loss: 0.1609359234571457\n",
            "Training Loss: 0.14736579358577728\n",
            "Training Loss: 0.2152024507522583\n",
            "Training Loss: 0.14730635285377502\n",
            "Training Loss: 0.12816594541072845\n",
            "Training Loss: 0.18999354541301727\n",
            "Training Loss: 0.17244350910186768\n",
            "Training Loss: 0.1742384433746338\n",
            "Training Loss: 0.20665891468524933\n",
            "Training Loss: 0.1542917639017105\n",
            "Training Loss: 0.15074953436851501\n",
            "Training Loss: 0.15106640756130219\n",
            "Training Loss: 0.18237417936325073\n",
            "Training Loss: 0.17036518454551697\n",
            "Training Loss: 0.13891443610191345\n",
            "Training Loss: 0.1434156894683838\n",
            "Training Loss: 0.1601925790309906\n",
            "Training Loss: 0.15334784984588623\n",
            "Training Loss: 0.16731497645378113\n",
            "Training Loss: 0.22385767102241516\n",
            "Training Loss: 0.15512894093990326\n",
            "Training Loss: 0.15995237231254578\n",
            "Training Loss: 0.18895362317562103\n",
            "Training Loss: 0.19115500152111053\n",
            "Training Loss: 0.1678832471370697\n",
            "Training Loss: 0.1330529749393463\n",
            "Training Loss: 0.1783984750509262\n",
            "Training Loss: 0.12019123882055283\n",
            "Training Loss: 0.14558424055576324\n",
            "Training Loss: 0.167729452252388\n",
            "Training Loss: 0.14755427837371826\n",
            "Training Loss: 0.15208914875984192\n",
            "Training Loss: 0.14000877737998962\n",
            "Training Loss: 0.1617787927389145\n",
            "Training Loss: 0.11982951313257217\n",
            "Training Loss: 0.14394055306911469\n",
            "Training Loss: 0.13681460916996002\n",
            "Training Loss: 0.1528434008359909\n",
            "Training Loss: 0.1609472930431366\n",
            "Training Loss: 0.15108847618103027\n",
            "Training Loss: 0.18162330985069275\n",
            "Training Loss: 0.14792540669441223\n",
            "Training Loss: 0.1896091252565384\n",
            "Training Loss: 0.18940980732440948\n",
            "Training Loss: 0.2032640427350998\n",
            "Training Loss: 0.16710197925567627\n",
            "Training Loss: 0.16021282970905304\n",
            "Training Loss: 0.17131824791431427\n",
            "Training Loss: 0.1635713130235672\n",
            "Training Loss: 0.1739567220211029\n",
            "Training Loss: 0.1561610996723175\n",
            "Training Loss: 0.20232748985290527\n",
            "Training Loss: 0.16431453824043274\n",
            "Training Loss: 0.19988438487052917\n",
            "Training Loss: 0.14402176439762115\n",
            "Training Loss: 0.15911968052387238\n",
            "Training Loss: 0.1618417203426361\n",
            "Training Loss: 0.13823051750659943\n",
            "Training Loss: 0.1508033573627472\n",
            "Training Loss: 0.15322303771972656\n",
            "Training Loss: 0.1417851746082306\n",
            "Training Loss: 0.17396971583366394\n",
            "Training Loss: 0.18572288751602173\n",
            "Training Loss: 0.15166068077087402\n",
            "Training Loss: 0.17004594206809998\n",
            "Training Loss: 0.15295778214931488\n",
            "Training Loss: 0.16003389656543732\n",
            "Training Loss: 0.15344898402690887\n",
            "Training Loss: 0.15464098751544952\n",
            "Training Loss: 0.17226363718509674\n",
            "Training Loss: 0.13567133247852325\n",
            "Training Loss: 0.16230112314224243\n",
            "Training Loss: 0.18797890841960907\n",
            "Training Loss: 0.1329900026321411\n",
            "Training Loss: 0.17869052290916443\n",
            "Training Loss: 0.14791418612003326\n",
            "Training Loss: 0.18457435071468353\n",
            "Training Loss: 0.17712648212909698\n",
            "Training Loss: 0.1814659982919693\n",
            "Training Loss: 0.13034889101982117\n",
            "Training Loss: 0.138423889875412\n",
            "Training Loss: 0.1808890551328659\n",
            "Training Loss: 0.145363911986351\n",
            "Training Loss: 0.16783840954303741\n",
            "Training Loss: 0.1681886464357376\n",
            "Training Loss: 0.13001294434070587\n",
            "Training Loss: 0.1416970193386078\n",
            "Training Loss: 0.1908763349056244\n",
            "Training Loss: 0.11844736337661743\n",
            "Training Loss: 0.14532732963562012\n",
            "Training Loss: 0.14951437711715698\n",
            "Training Loss: 0.18151290714740753\n",
            "Training Loss: 0.13800279796123505\n",
            "Training Loss: 0.16214792430400848\n",
            "Training Loss: 0.1613033264875412\n",
            "Training Loss: 0.13621172308921814\n",
            "Training Loss: 0.10137219727039337\n",
            "Training Loss: 0.20577695965766907\n",
            "Training Loss: 0.16485968232154846\n",
            "Training Loss: 0.14075523614883423\n",
            "Training Loss: 0.14772114157676697\n",
            "Training Loss: 0.1714257001876831\n",
            "Training Loss: 0.15235063433647156\n",
            "Training Loss: 0.17535383999347687\n",
            "Training Loss: 0.1643325388431549\n",
            "Training Loss: 0.18118134140968323\n",
            "Training Loss: 0.12707902491092682\n",
            "Training Loss: 0.16067592799663544\n",
            "Training Loss: 0.1549776792526245\n",
            "Training Loss: 0.16748860478401184\n",
            "Training Loss: 0.1290285736322403\n",
            "Training Loss: 0.14681030809879303\n",
            "Training Loss: 0.14544154703617096\n",
            "Training Loss: 0.1559208482503891\n",
            "Training Loss: 0.13735425472259521\n",
            "Training Loss: 0.1402435600757599\n",
            "Training Loss: 0.11926883459091187\n",
            "Training Loss: 0.1751047670841217\n",
            "Training Loss: 0.1786309778690338\n",
            "Training Loss: 0.18466554582118988\n",
            "Training Loss: 0.1465306580066681\n",
            "Training Loss: 0.1533469259738922\n",
            "Training Loss: 0.14715591073036194\n",
            "Training Loss: 0.14103290438652039\n",
            "Training Loss: 0.18750420212745667\n",
            "Training Loss: 0.2153661549091339\n",
            "Training Loss: 0.15751828253269196\n",
            "Training Loss: 0.17482593655586243\n",
            "Training Loss: 0.22041253745555878\n",
            "Training Loss: 0.1532515287399292\n",
            "Training Loss: 0.1335144340991974\n",
            "Training Loss: 0.12711894512176514\n",
            "Training Loss: 0.1379622519016266\n",
            "Training Loss: 0.15485909581184387\n",
            "Training Loss: 0.13757646083831787\n",
            "Training Loss: 0.16431966423988342\n",
            "Training Loss: 0.14967003464698792\n",
            "Training Loss: 0.15323396027088165\n",
            "Training Loss: 0.13886712491512299\n",
            "Training Loss: 0.20176556706428528\n",
            "Training Loss: 0.12066139280796051\n",
            "Training Loss: 0.17040973901748657\n",
            "Training Loss: 0.14193961024284363\n",
            "Training Loss: 0.14724460244178772\n",
            "Training Loss: 0.14679479598999023\n",
            "Training Loss: 0.11949652433395386\n",
            "Training Loss: 0.13292357325553894\n",
            "Training Loss: 0.13261185586452484\n",
            "Training Loss: 0.1251983344554901\n",
            "Training Loss: 0.16156254708766937\n",
            "Training Loss: 0.15088611841201782\n",
            "Training Loss: 0.15582063794136047\n",
            "Training Loss: 0.15691295266151428\n",
            "Training Loss: 0.1316027194261551\n",
            "Training Loss: 0.13983076810836792\n",
            "Training Loss: 0.12499921023845673\n",
            "Training Loss: 0.1260915994644165\n",
            "Training Loss: 0.18603402376174927\n",
            "Training Loss: 0.1404372751712799\n",
            "Training Loss: 0.1402284801006317\n",
            "Training Loss: 0.15450164675712585\n",
            "Training Loss: 0.14247332513332367\n",
            "Training Loss: 0.12456442415714264\n",
            "Training Loss: 0.13882683217525482\n",
            "Training Loss: 0.12889058887958527\n",
            "Training Loss: 0.14321008324623108\n",
            "Training Loss: 0.1785001903772354\n",
            "Training Loss: 0.15194071829319\n",
            "Training Loss: 0.1283545196056366\n",
            "Training Loss: 0.1250028908252716\n",
            "Training Loss: 0.12410751730203629\n",
            "Training Loss: 0.156973198056221\n",
            "Training Loss: 0.13103468716144562\n",
            "Training Loss: 0.15422917902469635\n",
            "Training Loss: 0.11846127361059189\n",
            "Training Loss: 0.1488141119480133\n",
            "Training Loss: 0.13653209805488586\n",
            "Training Loss: 0.14278723299503326\n",
            "Training Loss: 0.12684056162834167\n",
            "Training Loss: 0.10536855459213257\n",
            "Training Loss: 0.12384655326604843\n",
            "Training Loss: 0.18128663301467896\n",
            "Training Loss: 0.11895624548196793\n",
            "Training Loss: 0.1407291144132614\n",
            "Training Loss: 0.12367703020572662\n",
            "Training Loss: 0.15300318598747253\n",
            "Training Loss: 0.1516166627407074\n",
            "Training Loss: 0.10448076575994492\n",
            "Training Loss: 0.18372194468975067\n",
            "Training Loss: 0.13784432411193848\n",
            "Training Loss: 0.15406402945518494\n",
            "Training Loss: 0.13350412249565125\n",
            "Training Loss: 0.138419970870018\n",
            "Training Loss: 0.14327874779701233\n",
            "Training Loss: 0.11355383694171906\n",
            "Training Loss: 0.14790652692317963\n",
            "Training Loss: 0.15180370211601257\n",
            "Training Loss: 0.1300307959318161\n",
            "Training Loss: 0.1536128968000412\n",
            "Training Loss: 0.1601226031780243\n",
            "Training Loss: 0.11191848665475845\n",
            "Training Loss: 0.17113420367240906\n",
            "Training Loss: 0.16924628615379333\n",
            "Training Loss: 0.14650802314281464\n",
            "Training Loss: 0.17093029618263245\n",
            "Training Loss: 0.10458189249038696\n",
            "Training Loss: 0.1340034306049347\n",
            "Training Loss: 0.12670668959617615\n",
            "Training Loss: 0.10205457359552383\n",
            "Training Loss: 0.14186401665210724\n",
            "Training Loss: 0.12613573670387268\n",
            "Training Loss: 0.13953416049480438\n",
            "Training Loss: 0.15037348866462708\n",
            "Training Loss: 0.11698098480701447\n",
            "Training Loss: 0.14888712763786316\n",
            "Training Loss: 0.1344204545021057\n",
            "Training Loss: 0.12263227999210358\n",
            "Training Loss: 0.15376313030719757\n",
            "Training Loss: 0.09466658532619476\n",
            "Accuracy on Test Data: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n86rl85pJ8dB"
      },
      "source": [
        "## **Exercice 3 : Network class**\n",
        "\n",
        "Vous allez recoder l'exercice 2 en ajoutant deux modifications: <br/>\n",
        "1)\n",
        "Pour l'instant, on traite nos couches séparément, et non comme un \"block\" unifié. Par exemple, il faut donner les parametres de chaque couche a l'optimizer (par exemple : `optim.SGD(list(layer1.parameters())+list(layer2.parameters()), lr=learning_rate`)\n",
        "\n",
        "Allez sur ce lien et regardez comment définir une classe pour votre réseau (https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html). Utilisez la et modifiez votre ligne de l'optimiseur ainsi que la gestion de votre forward pass.\n",
        "\n",
        "2)\n",
        "Imaginez qu'on ait 1000 couches, ce n'est pas pratique des les appeler une par une lors de la forward pass. Utilisez la fonction suivante pour palier au problème :\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O4MhPDz-2gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4be539f-c233-4632-a5c1-c53f7f958ec1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Classe Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 3)\n",
        "        self.fc2 = nn.Tanh()\n",
        "        self.fc3 = nn.Linear(3, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Copiez collez votre solution en modifiant l'otim.SGD et la forward pass\n",
        "\n",
        "## Créez les layers ici\n",
        "model = Net()\n",
        "\n",
        "# Déclarez les parametres de l'expérience (batch_size, learning rate, ...)\n",
        "batch_size = 10\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Déclarez la loss et l'optimisation (SGD)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# Split test/train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)\n",
        "\n",
        "# Training\n",
        "while True:\n",
        "  X_batch, y_batch = batch(X_train, y_train, batch_size = batch_size)\n",
        "\n",
        "  y_batch = np.array(y_batch)\n",
        "  X_batch = np.array(X_batch)\n",
        "\n",
        "  X_batch_tensor = torch.tensor(X_batch, dtype=torch.float32)\n",
        "  y_batch_tensor = torch.tensor(y_batch, dtype=torch.long)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  y_pred = model(X_batch_tensor)\n",
        "  loss = loss_func(y_pred, y_batch_tensor)\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"Training Loss: {loss.item()}\")\n",
        "  if loss.item() < 0.1:\n",
        "        break\n",
        "\n",
        "# Eval sur test\n",
        "with torch.no_grad():\n",
        "    test_inputs = torch.tensor(X_test, dtype=torch.float32)\n",
        "    test_labels = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    test_output = model(test_inputs)\n",
        "\n",
        "    predicted = np.argmax(test_output.numpy(), axis = 1)\n",
        "    accuracy = np.mean(predicted == test_labels.numpy())\n",
        "\n",
        "    print(f\"Accuracy on Test Data: {accuracy * 100}%\")"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.8814164996147156\n",
            "Training Loss: 0.5630083680152893\n",
            "Training Loss: 0.7347114682197571\n",
            "Training Loss: 0.6768612265586853\n",
            "Training Loss: 0.7999709844589233\n",
            "Training Loss: 0.673324704170227\n",
            "Training Loss: 0.6215248107910156\n",
            "Training Loss: 0.7251855134963989\n",
            "Training Loss: 0.7602086663246155\n",
            "Training Loss: 0.8131636381149292\n",
            "Training Loss: 0.7131611704826355\n",
            "Training Loss: 0.7103308439254761\n",
            "Training Loss: 0.6438497304916382\n",
            "Training Loss: 0.747871994972229\n",
            "Training Loss: 0.6469854712486267\n",
            "Training Loss: 0.7046647071838379\n",
            "Training Loss: 0.7064049243927002\n",
            "Training Loss: 0.6152964234352112\n",
            "Training Loss: 0.7090757489204407\n",
            "Training Loss: 0.7740894556045532\n",
            "Training Loss: 0.7290576100349426\n",
            "Training Loss: 0.6537407636642456\n",
            "Training Loss: 0.6444079279899597\n",
            "Training Loss: 0.7002242803573608\n",
            "Training Loss: 0.6435907483100891\n",
            "Training Loss: 0.6797136068344116\n",
            "Training Loss: 0.7041635513305664\n",
            "Training Loss: 0.6117070317268372\n",
            "Training Loss: 0.7050996422767639\n",
            "Training Loss: 0.6688932180404663\n",
            "Training Loss: 0.664190948009491\n",
            "Training Loss: 0.7276656627655029\n",
            "Training Loss: 0.6131256818771362\n",
            "Training Loss: 0.7741755247116089\n",
            "Training Loss: 0.6957190632820129\n",
            "Training Loss: 0.6917668581008911\n",
            "Training Loss: 0.6908498406410217\n",
            "Training Loss: 0.6981196403503418\n",
            "Training Loss: 0.7102240920066833\n",
            "Training Loss: 0.6559971570968628\n",
            "Training Loss: 0.6746553182601929\n",
            "Training Loss: 0.6733529567718506\n",
            "Training Loss: 0.7251091003417969\n",
            "Training Loss: 0.7017382979393005\n",
            "Training Loss: 0.6908466219902039\n",
            "Training Loss: 0.6686697602272034\n",
            "Training Loss: 0.6924899816513062\n",
            "Training Loss: 0.6384384632110596\n",
            "Training Loss: 0.6996862888336182\n",
            "Training Loss: 0.6613574624061584\n",
            "Training Loss: 0.751322329044342\n",
            "Training Loss: 0.6970353126525879\n",
            "Training Loss: 0.6733280420303345\n",
            "Training Loss: 0.6615198254585266\n",
            "Training Loss: 0.6632603406906128\n",
            "Training Loss: 0.6904071569442749\n",
            "Training Loss: 0.6435319781303406\n",
            "Training Loss: 0.7616464495658875\n",
            "Training Loss: 0.6718355417251587\n",
            "Training Loss: 0.6838286519050598\n",
            "Training Loss: 0.7048629522323608\n",
            "Training Loss: 0.6887038946151733\n",
            "Training Loss: 0.6822854280471802\n",
            "Training Loss: 0.6745563745498657\n",
            "Training Loss: 0.6984382271766663\n",
            "Training Loss: 0.670699954032898\n",
            "Training Loss: 0.6877968311309814\n",
            "Training Loss: 0.6794325709342957\n",
            "Training Loss: 0.6466922760009766\n",
            "Training Loss: 0.6784713864326477\n",
            "Training Loss: 0.6971452832221985\n",
            "Training Loss: 0.6623591780662537\n",
            "Training Loss: 0.6928107142448425\n",
            "Training Loss: 0.7441965341567993\n",
            "Training Loss: 0.6868906617164612\n",
            "Training Loss: 0.6637905836105347\n",
            "Training Loss: 0.702139675617218\n",
            "Training Loss: 0.6867482662200928\n",
            "Training Loss: 0.6657596826553345\n",
            "Training Loss: 0.668300449848175\n",
            "Training Loss: 0.6636985540390015\n",
            "Training Loss: 0.6515072584152222\n",
            "Training Loss: 0.6821184754371643\n",
            "Training Loss: 0.6484837532043457\n",
            "Training Loss: 0.7076969146728516\n",
            "Training Loss: 0.6602771282196045\n",
            "Training Loss: 0.6610721349716187\n",
            "Training Loss: 0.6701087951660156\n",
            "Training Loss: 0.697236955165863\n",
            "Training Loss: 0.7209242582321167\n",
            "Training Loss: 0.6727712750434875\n",
            "Training Loss: 0.6564682126045227\n",
            "Training Loss: 0.6777199506759644\n",
            "Training Loss: 0.6824408769607544\n",
            "Training Loss: 0.7217332720756531\n",
            "Training Loss: 0.7009638547897339\n",
            "Training Loss: 0.6925737857818604\n",
            "Training Loss: 0.6648256182670593\n",
            "Training Loss: 0.682839572429657\n",
            "Training Loss: 0.6425830721855164\n",
            "Training Loss: 0.6902698278427124\n",
            "Training Loss: 0.710226833820343\n",
            "Training Loss: 0.6540628671646118\n",
            "Training Loss: 0.6523088216781616\n",
            "Training Loss: 0.6639150381088257\n",
            "Training Loss: 0.6493343114852905\n",
            "Training Loss: 0.6106653213500977\n",
            "Training Loss: 0.6874881386756897\n",
            "Training Loss: 0.6823356747627258\n",
            "Training Loss: 0.7005218267440796\n",
            "Training Loss: 0.676383376121521\n",
            "Training Loss: 0.6947274208068848\n",
            "Training Loss: 0.7047988176345825\n",
            "Training Loss: 0.6429948806762695\n",
            "Training Loss: 0.7077380418777466\n",
            "Training Loss: 0.6641941070556641\n",
            "Training Loss: 0.6669076681137085\n",
            "Training Loss: 0.6288846731185913\n",
            "Training Loss: 0.631797194480896\n",
            "Training Loss: 0.6334443688392639\n",
            "Training Loss: 0.7435529828071594\n",
            "Training Loss: 0.7412071824073792\n",
            "Training Loss: 0.6798025369644165\n",
            "Training Loss: 0.6663066744804382\n",
            "Training Loss: 0.6230205297470093\n",
            "Training Loss: 0.7413078546524048\n",
            "Training Loss: 0.6871787309646606\n",
            "Training Loss: 0.6922568678855896\n",
            "Training Loss: 0.6963754892349243\n",
            "Training Loss: 0.6701815128326416\n",
            "Training Loss: 0.6888074278831482\n",
            "Training Loss: 0.6577902436256409\n",
            "Training Loss: 0.6630438566207886\n",
            "Training Loss: 0.6784216165542603\n",
            "Training Loss: 0.6855103373527527\n",
            "Training Loss: 0.6482651829719543\n",
            "Training Loss: 0.6652025580406189\n",
            "Training Loss: 0.7059463262557983\n",
            "Training Loss: 0.6570687890052795\n",
            "Training Loss: 0.6482681035995483\n",
            "Training Loss: 0.6548002362251282\n",
            "Training Loss: 0.5948663353919983\n",
            "Training Loss: 0.6638210415840149\n",
            "Training Loss: 0.6586942076683044\n",
            "Training Loss: 0.8162567019462585\n",
            "Training Loss: 0.6853259801864624\n",
            "Training Loss: 0.7093688249588013\n",
            "Training Loss: 0.6592556834220886\n",
            "Training Loss: 0.6644867658615112\n",
            "Training Loss: 0.6454787254333496\n",
            "Training Loss: 0.6849711537361145\n",
            "Training Loss: 0.6708442568778992\n",
            "Training Loss: 0.6878029704093933\n",
            "Training Loss: 0.658930242061615\n",
            "Training Loss: 0.673201322555542\n",
            "Training Loss: 0.632102370262146\n",
            "Training Loss: 0.6553425192832947\n",
            "Training Loss: 0.6464198231697083\n",
            "Training Loss: 0.7257530689239502\n",
            "Training Loss: 0.6531091928482056\n",
            "Training Loss: 0.7065812945365906\n",
            "Training Loss: 0.6822220087051392\n",
            "Training Loss: 0.645146369934082\n",
            "Training Loss: 0.6849861145019531\n",
            "Training Loss: 0.6480542421340942\n",
            "Training Loss: 0.7021806240081787\n",
            "Training Loss: 0.6717237234115601\n",
            "Training Loss: 0.6326512098312378\n",
            "Training Loss: 0.6769319772720337\n",
            "Training Loss: 0.651380181312561\n",
            "Training Loss: 0.6532638072967529\n",
            "Training Loss: 0.6485225558280945\n",
            "Training Loss: 0.6676209568977356\n",
            "Training Loss: 0.6790193915367126\n",
            "Training Loss: 0.6508222222328186\n",
            "Training Loss: 0.7217389345169067\n",
            "Training Loss: 0.6371837854385376\n",
            "Training Loss: 0.6143621206283569\n",
            "Training Loss: 0.6386248469352722\n",
            "Training Loss: 0.6641238927841187\n",
            "Training Loss: 0.6600196957588196\n",
            "Training Loss: 0.7113146781921387\n",
            "Training Loss: 0.6539666056632996\n",
            "Training Loss: 0.7029814720153809\n",
            "Training Loss: 0.6661953926086426\n",
            "Training Loss: 0.6820486187934875\n",
            "Training Loss: 0.6768983602523804\n",
            "Training Loss: 0.6595643758773804\n",
            "Training Loss: 0.6375236511230469\n",
            "Training Loss: 0.6297328472137451\n",
            "Training Loss: 0.6488693952560425\n",
            "Training Loss: 0.6358925104141235\n",
            "Training Loss: 0.6393237709999084\n",
            "Training Loss: 0.6742302179336548\n",
            "Training Loss: 0.6536002159118652\n",
            "Training Loss: 0.6432821750640869\n",
            "Training Loss: 0.6700552701950073\n",
            "Training Loss: 0.6295614838600159\n",
            "Training Loss: 0.688967227935791\n",
            "Training Loss: 0.6588268280029297\n",
            "Training Loss: 0.6753474473953247\n",
            "Training Loss: 0.6455385088920593\n",
            "Training Loss: 0.6373915672302246\n",
            "Training Loss: 0.6444947123527527\n",
            "Training Loss: 0.6654280424118042\n",
            "Training Loss: 0.6412426829338074\n",
            "Training Loss: 0.673244833946228\n",
            "Training Loss: 0.6512378454208374\n",
            "Training Loss: 0.6551908850669861\n",
            "Training Loss: 0.6503672003746033\n",
            "Training Loss: 0.6524439454078674\n",
            "Training Loss: 0.6632003784179688\n",
            "Training Loss: 0.6618048548698425\n",
            "Training Loss: 0.6641721725463867\n",
            "Training Loss: 0.6564492583274841\n",
            "Training Loss: 0.6505424380302429\n",
            "Training Loss: 0.6560322046279907\n",
            "Training Loss: 0.6681913137435913\n",
            "Training Loss: 0.6626490950584412\n",
            "Training Loss: 0.6874884366989136\n",
            "Training Loss: 0.6406200528144836\n",
            "Training Loss: 0.6516846418380737\n",
            "Training Loss: 0.6732381582260132\n",
            "Training Loss: 0.6801384091377258\n",
            "Training Loss: 0.6755324006080627\n",
            "Training Loss: 0.6486088037490845\n",
            "Training Loss: 0.6987261772155762\n",
            "Training Loss: 0.6467365026473999\n",
            "Training Loss: 0.6935552358627319\n",
            "Training Loss: 0.6460246443748474\n",
            "Training Loss: 0.6889380216598511\n",
            "Training Loss: 0.6705381274223328\n",
            "Training Loss: 0.670462965965271\n",
            "Training Loss: 0.635689377784729\n",
            "Training Loss: 0.6764765381813049\n",
            "Training Loss: 0.6383751630783081\n",
            "Training Loss: 0.6184011101722717\n",
            "Training Loss: 0.6649103760719299\n",
            "Training Loss: 0.6195641160011292\n",
            "Training Loss: 0.6486219167709351\n",
            "Training Loss: 0.6319941878318787\n",
            "Training Loss: 0.601620078086853\n",
            "Training Loss: 0.6269623041152954\n",
            "Training Loss: 0.6591070890426636\n",
            "Training Loss: 0.637283980846405\n",
            "Training Loss: 0.5884906649589539\n",
            "Training Loss: 0.6775025129318237\n",
            "Training Loss: 0.6523969769477844\n",
            "Training Loss: 0.6417335271835327\n",
            "Training Loss: 0.6459973454475403\n",
            "Training Loss: 0.6313878297805786\n",
            "Training Loss: 0.6540816426277161\n",
            "Training Loss: 0.6431761384010315\n",
            "Training Loss: 0.6383141875267029\n",
            "Training Loss: 0.6014231443405151\n",
            "Training Loss: 0.6283071637153625\n",
            "Training Loss: 0.6879420876502991\n",
            "Training Loss: 0.6363276839256287\n",
            "Training Loss: 0.641948401927948\n",
            "Training Loss: 0.643545389175415\n",
            "Training Loss: 0.6365812420845032\n",
            "Training Loss: 0.6684197187423706\n",
            "Training Loss: 0.6555142998695374\n",
            "Training Loss: 0.6436585187911987\n",
            "Training Loss: 0.6307370066642761\n",
            "Training Loss: 0.6792216897010803\n",
            "Training Loss: 0.5997816324234009\n",
            "Training Loss: 0.6472077369689941\n",
            "Training Loss: 0.654702365398407\n",
            "Training Loss: 0.6636853814125061\n",
            "Training Loss: 0.6567537188529968\n",
            "Training Loss: 0.6112232208251953\n",
            "Training Loss: 0.6118128299713135\n",
            "Training Loss: 0.6126784086227417\n",
            "Training Loss: 0.733526349067688\n",
            "Training Loss: 0.682477593421936\n",
            "Training Loss: 0.5972156524658203\n",
            "Training Loss: 0.6540461778640747\n",
            "Training Loss: 0.6716200709342957\n",
            "Training Loss: 0.6193686127662659\n",
            "Training Loss: 0.638892650604248\n",
            "Training Loss: 0.6267566084861755\n",
            "Training Loss: 0.6755290031433105\n",
            "Training Loss: 0.6720340847969055\n",
            "Training Loss: 0.6081157922744751\n",
            "Training Loss: 0.6409557461738586\n",
            "Training Loss: 0.6210528612136841\n",
            "Training Loss: 0.6497610211372375\n",
            "Training Loss: 0.6123474836349487\n",
            "Training Loss: 0.6754703521728516\n",
            "Training Loss: 0.656515896320343\n",
            "Training Loss: 0.6131739020347595\n",
            "Training Loss: 0.694564938545227\n",
            "Training Loss: 0.6427741050720215\n",
            "Training Loss: 0.6141189336776733\n",
            "Training Loss: 0.6173349618911743\n",
            "Training Loss: 0.5850520133972168\n",
            "Training Loss: 0.670133113861084\n",
            "Training Loss: 0.6402304768562317\n",
            "Training Loss: 0.6412926912307739\n",
            "Training Loss: 0.6361592411994934\n",
            "Training Loss: 0.6238937973976135\n",
            "Training Loss: 0.5865274667739868\n",
            "Training Loss: 0.6017498970031738\n",
            "Training Loss: 0.6141597032546997\n",
            "Training Loss: 0.630193293094635\n",
            "Training Loss: 0.6097002029418945\n",
            "Training Loss: 0.6562126278877258\n",
            "Training Loss: 0.611420750617981\n",
            "Training Loss: 0.5868967771530151\n",
            "Training Loss: 0.5978262424468994\n",
            "Training Loss: 0.6570204496383667\n",
            "Training Loss: 0.6172069311141968\n",
            "Training Loss: 0.6891758441925049\n",
            "Training Loss: 0.6158533692359924\n",
            "Training Loss: 0.6608436703681946\n",
            "Training Loss: 0.642765998840332\n",
            "Training Loss: 0.6287634968757629\n",
            "Training Loss: 0.6095880270004272\n",
            "Training Loss: 0.6766000986099243\n",
            "Training Loss: 0.5699461698532104\n",
            "Training Loss: 0.6049012541770935\n",
            "Training Loss: 0.6292589902877808\n",
            "Training Loss: 0.6555877327919006\n",
            "Training Loss: 0.5785802006721497\n",
            "Training Loss: 0.633489191532135\n",
            "Training Loss: 0.6450746655464172\n",
            "Training Loss: 0.5966557860374451\n",
            "Training Loss: 0.6546264886856079\n",
            "Training Loss: 0.6028617024421692\n",
            "Training Loss: 0.6931699514389038\n",
            "Training Loss: 0.616536021232605\n",
            "Training Loss: 0.6791266202926636\n",
            "Training Loss: 0.6163538694381714\n",
            "Training Loss: 0.6860247850418091\n",
            "Training Loss: 0.5999473333358765\n",
            "Training Loss: 0.6711889505386353\n",
            "Training Loss: 0.674750030040741\n",
            "Training Loss: 0.6406444311141968\n",
            "Training Loss: 0.6507915258407593\n",
            "Training Loss: 0.5975165367126465\n",
            "Training Loss: 0.5539907217025757\n",
            "Training Loss: 0.6337133646011353\n",
            "Training Loss: 0.5914961695671082\n",
            "Training Loss: 0.59183269739151\n",
            "Training Loss: 0.5848225951194763\n",
            "Training Loss: 0.6761624813079834\n",
            "Training Loss: 0.5874465703964233\n",
            "Training Loss: 0.5870792269706726\n",
            "Training Loss: 0.624925971031189\n",
            "Training Loss: 0.554292619228363\n",
            "Training Loss: 0.6166865825653076\n",
            "Training Loss: 0.6401515603065491\n",
            "Training Loss: 0.5914254784584045\n",
            "Training Loss: 0.6516700983047485\n",
            "Training Loss: 0.5748640298843384\n",
            "Training Loss: 0.5764185190200806\n",
            "Training Loss: 0.7053960561752319\n",
            "Training Loss: 0.5936036109924316\n",
            "Training Loss: 0.5304057598114014\n",
            "Training Loss: 0.5678101181983948\n",
            "Training Loss: 0.5391414761543274\n",
            "Training Loss: 0.6021344065666199\n",
            "Training Loss: 0.617506206035614\n",
            "Training Loss: 0.6325806379318237\n",
            "Training Loss: 0.5958370566368103\n",
            "Training Loss: 0.5617971420288086\n",
            "Training Loss: 0.6360143423080444\n",
            "Training Loss: 0.6502026915550232\n",
            "Training Loss: 0.5622745156288147\n",
            "Training Loss: 0.6080914735794067\n",
            "Training Loss: 0.5668818950653076\n",
            "Training Loss: 0.6119982600212097\n",
            "Training Loss: 0.6737592220306396\n",
            "Training Loss: 0.5202698707580566\n",
            "Training Loss: 0.5728340148925781\n",
            "Training Loss: 0.5815756916999817\n",
            "Training Loss: 0.6461717486381531\n",
            "Training Loss: 0.6218553781509399\n",
            "Training Loss: 0.5934536457061768\n",
            "Training Loss: 0.5999501347541809\n",
            "Training Loss: 0.6177247762680054\n",
            "Training Loss: 0.6776341199874878\n",
            "Training Loss: 0.5939310789108276\n",
            "Training Loss: 0.7083085179328918\n",
            "Training Loss: 0.6572243571281433\n",
            "Training Loss: 0.5785019993782043\n",
            "Training Loss: 0.5715086460113525\n",
            "Training Loss: 0.5629842281341553\n",
            "Training Loss: 0.6510097980499268\n",
            "Training Loss: 0.6749297976493835\n",
            "Training Loss: 0.5658560395240784\n",
            "Training Loss: 0.5458519458770752\n",
            "Training Loss: 0.5907496213912964\n",
            "Training Loss: 0.5837653875350952\n",
            "Training Loss: 0.549066424369812\n",
            "Training Loss: 0.5754958391189575\n",
            "Training Loss: 0.523743212223053\n",
            "Training Loss: 0.6334525346755981\n",
            "Training Loss: 0.5878240466117859\n",
            "Training Loss: 0.5844308733940125\n",
            "Training Loss: 0.6124781966209412\n",
            "Training Loss: 0.587822437286377\n",
            "Training Loss: 0.5630350708961487\n",
            "Training Loss: 0.544974684715271\n",
            "Training Loss: 0.6915212869644165\n",
            "Training Loss: 0.6205536127090454\n",
            "Training Loss: 0.6525743007659912\n",
            "Training Loss: 0.5475696325302124\n",
            "Training Loss: 0.6010846495628357\n",
            "Training Loss: 0.5555518865585327\n",
            "Training Loss: 0.6518221497535706\n",
            "Training Loss: 0.6122325658798218\n",
            "Training Loss: 0.5854446887969971\n",
            "Training Loss: 0.5320030450820923\n",
            "Training Loss: 0.7144201993942261\n",
            "Training Loss: 0.641676127910614\n",
            "Training Loss: 0.6001192927360535\n",
            "Training Loss: 0.5903419852256775\n",
            "Training Loss: 0.5415328145027161\n",
            "Training Loss: 0.6171990633010864\n",
            "Training Loss: 0.5228248834609985\n",
            "Training Loss: 0.5808095335960388\n",
            "Training Loss: 0.5715610980987549\n",
            "Training Loss: 0.6244978308677673\n",
            "Training Loss: 0.5578535795211792\n",
            "Training Loss: 0.6145740747451782\n",
            "Training Loss: 0.607461154460907\n",
            "Training Loss: 0.6348782777786255\n",
            "Training Loss: 0.6437254548072815\n",
            "Training Loss: 0.6025189161300659\n",
            "Training Loss: 0.6256765127182007\n",
            "Training Loss: 0.5956010818481445\n",
            "Training Loss: 0.5953649282455444\n",
            "Training Loss: 0.5728304982185364\n",
            "Training Loss: 0.6975170373916626\n",
            "Training Loss: 0.5514031648635864\n",
            "Training Loss: 0.6738716959953308\n",
            "Training Loss: 0.5530402064323425\n",
            "Training Loss: 0.5627632141113281\n",
            "Training Loss: 0.6295786499977112\n",
            "Training Loss: 0.587364673614502\n",
            "Training Loss: 0.5099917054176331\n",
            "Training Loss: 0.5266786813735962\n",
            "Training Loss: 0.5670698285102844\n",
            "Training Loss: 0.6026455163955688\n",
            "Training Loss: 0.6512205600738525\n",
            "Training Loss: 0.5864151120185852\n",
            "Training Loss: 0.5241349935531616\n",
            "Training Loss: 0.5800450444221497\n",
            "Training Loss: 0.646335244178772\n",
            "Training Loss: 0.4865225851535797\n",
            "Training Loss: 0.6152211427688599\n",
            "Training Loss: 0.7357566356658936\n",
            "Training Loss: 0.613676905632019\n",
            "Training Loss: 0.550756573677063\n",
            "Training Loss: 0.5759947896003723\n",
            "Training Loss: 0.6038283109664917\n",
            "Training Loss: 0.6637054085731506\n",
            "Training Loss: 0.5878031849861145\n",
            "Training Loss: 0.6838821172714233\n",
            "Training Loss: 0.6210501194000244\n",
            "Training Loss: 0.5487000346183777\n",
            "Training Loss: 0.5771994590759277\n",
            "Training Loss: 0.6671708226203918\n",
            "Training Loss: 0.5586024522781372\n",
            "Training Loss: 0.584470272064209\n",
            "Training Loss: 0.5083421468734741\n",
            "Training Loss: 0.5100429654121399\n",
            "Training Loss: 0.5890169739723206\n",
            "Training Loss: 0.6783370971679688\n",
            "Training Loss: 0.6074231266975403\n",
            "Training Loss: 0.5640336275100708\n",
            "Training Loss: 0.5939279794692993\n",
            "Training Loss: 0.5824269652366638\n",
            "Training Loss: 0.5160843133926392\n",
            "Training Loss: 0.6455515027046204\n",
            "Training Loss: 0.5460453033447266\n",
            "Training Loss: 0.5208415985107422\n",
            "Training Loss: 0.6652954816818237\n",
            "Training Loss: 0.6939237117767334\n",
            "Training Loss: 0.5660853981971741\n",
            "Training Loss: 0.615164577960968\n",
            "Training Loss: 0.6294840574264526\n",
            "Training Loss: 0.5918319821357727\n",
            "Training Loss: 0.6309406161308289\n",
            "Training Loss: 0.544952392578125\n",
            "Training Loss: 0.5879018902778625\n",
            "Training Loss: 0.6662145256996155\n",
            "Training Loss: 0.5400282144546509\n",
            "Training Loss: 0.6619247794151306\n",
            "Training Loss: 0.5466891527175903\n",
            "Training Loss: 0.45236149430274963\n",
            "Training Loss: 0.6388974785804749\n",
            "Training Loss: 0.6121966242790222\n",
            "Training Loss: 0.6425617933273315\n",
            "Training Loss: 0.5249592065811157\n",
            "Training Loss: 0.5793983340263367\n",
            "Training Loss: 0.6349984407424927\n",
            "Training Loss: 0.6622591614723206\n",
            "Training Loss: 0.5485814213752747\n",
            "Training Loss: 0.5401027798652649\n",
            "Training Loss: 0.6591205596923828\n",
            "Training Loss: 0.5832249522209167\n",
            "Training Loss: 0.5214213728904724\n",
            "Training Loss: 0.6624568700790405\n",
            "Training Loss: 0.6018284559249878\n",
            "Training Loss: 0.5456241369247437\n",
            "Training Loss: 0.5507088303565979\n",
            "Training Loss: 0.6119838953018188\n",
            "Training Loss: 0.5624800324440002\n",
            "Training Loss: 0.63847416639328\n",
            "Training Loss: 0.5981131792068481\n",
            "Training Loss: 0.5690206289291382\n",
            "Training Loss: 0.5232073068618774\n",
            "Training Loss: 0.6450281143188477\n",
            "Training Loss: 0.5465906262397766\n",
            "Training Loss: 0.5279651284217834\n",
            "Training Loss: 0.6083787679672241\n",
            "Training Loss: 0.609225869178772\n",
            "Training Loss: 0.5894793272018433\n",
            "Training Loss: 0.6501500606536865\n",
            "Training Loss: 0.6309849619865417\n",
            "Training Loss: 0.5877496004104614\n",
            "Training Loss: 0.5250464677810669\n",
            "Training Loss: 0.570437490940094\n",
            "Training Loss: 0.6807953119277954\n",
            "Training Loss: 0.5343536138534546\n",
            "Training Loss: 0.5672698616981506\n",
            "Training Loss: 0.5731373429298401\n",
            "Training Loss: 0.6326227784156799\n",
            "Training Loss: 0.5467674136161804\n",
            "Training Loss: 0.549791693687439\n",
            "Training Loss: 0.47534066438674927\n",
            "Training Loss: 0.5193063616752625\n",
            "Training Loss: 0.5986822843551636\n",
            "Training Loss: 0.5720616579055786\n",
            "Training Loss: 0.49519142508506775\n",
            "Training Loss: 0.5717755556106567\n",
            "Training Loss: 0.47223392128944397\n",
            "Training Loss: 0.4864129424095154\n",
            "Training Loss: 0.6122084856033325\n",
            "Training Loss: 0.539808452129364\n",
            "Training Loss: 0.6027340292930603\n",
            "Training Loss: 0.5605452656745911\n",
            "Training Loss: 0.4631815552711487\n",
            "Training Loss: 0.6341938376426697\n",
            "Training Loss: 0.6252573728561401\n",
            "Training Loss: 0.4322569966316223\n",
            "Training Loss: 0.556930661201477\n",
            "Training Loss: 0.5495585799217224\n",
            "Training Loss: 0.6215068697929382\n",
            "Training Loss: 0.6357800364494324\n",
            "Training Loss: 0.5449801683425903\n",
            "Training Loss: 0.6111990213394165\n",
            "Training Loss: 0.5328795313835144\n",
            "Training Loss: 0.6252371072769165\n",
            "Training Loss: 0.4718192219734192\n",
            "Training Loss: 0.5323209762573242\n",
            "Training Loss: 0.5960034132003784\n",
            "Training Loss: 0.6415960788726807\n",
            "Training Loss: 0.5437014698982239\n",
            "Training Loss: 0.5196858644485474\n",
            "Training Loss: 0.48212820291519165\n",
            "Training Loss: 0.591254711151123\n",
            "Training Loss: 0.5769490003585815\n",
            "Training Loss: 0.42657750844955444\n",
            "Training Loss: 0.5986323356628418\n",
            "Training Loss: 0.5273718237876892\n",
            "Training Loss: 0.522707462310791\n",
            "Training Loss: 0.5572406649589539\n",
            "Training Loss: 0.6238215565681458\n",
            "Training Loss: 0.46741217374801636\n",
            "Training Loss: 0.552174985408783\n",
            "Training Loss: 0.5415058732032776\n",
            "Training Loss: 0.6279074549674988\n",
            "Training Loss: 0.5676437020301819\n",
            "Training Loss: 0.549136757850647\n",
            "Training Loss: 0.4320853352546692\n",
            "Training Loss: 0.4951515793800354\n",
            "Training Loss: 0.6315264105796814\n",
            "Training Loss: 0.4839615225791931\n",
            "Training Loss: 0.4426657557487488\n",
            "Training Loss: 0.545242428779602\n",
            "Training Loss: 0.6051236391067505\n",
            "Training Loss: 0.49882978200912476\n",
            "Training Loss: 0.6585506200790405\n",
            "Training Loss: 0.7147015333175659\n",
            "Training Loss: 0.5954384803771973\n",
            "Training Loss: 0.41473907232284546\n",
            "Training Loss: 0.4909583032131195\n",
            "Training Loss: 0.6315599679946899\n",
            "Training Loss: 0.6236635446548462\n",
            "Training Loss: 0.6089407801628113\n",
            "Training Loss: 0.5247876048088074\n",
            "Training Loss: 0.558795690536499\n",
            "Training Loss: 0.5148951411247253\n",
            "Training Loss: 0.6865261793136597\n",
            "Training Loss: 0.5038430094718933\n",
            "Training Loss: 0.673971951007843\n",
            "Training Loss: 0.5681456327438354\n",
            "Training Loss: 0.42938241362571716\n",
            "Training Loss: 0.610794723033905\n",
            "Training Loss: 0.5755642652511597\n",
            "Training Loss: 0.5838006138801575\n",
            "Training Loss: 0.5007079243659973\n",
            "Training Loss: 0.5497655272483826\n",
            "Training Loss: 0.5331649780273438\n",
            "Training Loss: 0.6392103433609009\n",
            "Training Loss: 0.6244751214981079\n",
            "Training Loss: 0.48111242055892944\n",
            "Training Loss: 0.5294511914253235\n",
            "Training Loss: 0.6360709071159363\n",
            "Training Loss: 0.491830974817276\n",
            "Training Loss: 0.5270212888717651\n",
            "Training Loss: 0.6269048452377319\n",
            "Training Loss: 0.5904918909072876\n",
            "Training Loss: 0.6002331972122192\n",
            "Training Loss: 0.5463522672653198\n",
            "Training Loss: 0.48847946524620056\n",
            "Training Loss: 0.47712811827659607\n",
            "Training Loss: 0.5770179033279419\n",
            "Training Loss: 0.5614039301872253\n",
            "Training Loss: 0.5693507194519043\n",
            "Training Loss: 0.6876969933509827\n",
            "Training Loss: 0.5299767255783081\n",
            "Training Loss: 0.5477217435836792\n",
            "Training Loss: 0.4155823588371277\n",
            "Training Loss: 0.44634896516799927\n",
            "Training Loss: 0.41593846678733826\n",
            "Training Loss: 0.5147486925125122\n",
            "Training Loss: 0.46776413917541504\n",
            "Training Loss: 0.46852582693099976\n",
            "Training Loss: 0.5096030235290527\n",
            "Training Loss: 0.6377474069595337\n",
            "Training Loss: 0.643105685710907\n",
            "Training Loss: 0.4549063742160797\n",
            "Training Loss: 0.4638175368309021\n",
            "Training Loss: 0.4342193603515625\n",
            "Training Loss: 0.478503942489624\n",
            "Training Loss: 0.563911497592926\n",
            "Training Loss: 0.5241347551345825\n",
            "Training Loss: 0.4597571790218353\n",
            "Training Loss: 0.5395143628120422\n",
            "Training Loss: 0.5754856467247009\n",
            "Training Loss: 0.47913917899131775\n",
            "Training Loss: 0.5858792066574097\n",
            "Training Loss: 0.5576438903808594\n",
            "Training Loss: 0.4787364602088928\n",
            "Training Loss: 0.6134347915649414\n",
            "Training Loss: 0.44300001859664917\n",
            "Training Loss: 0.5834134221076965\n",
            "Training Loss: 0.5914651155471802\n",
            "Training Loss: 0.41466718912124634\n",
            "Training Loss: 0.43334275484085083\n",
            "Training Loss: 0.6079879999160767\n",
            "Training Loss: 0.5160700082778931\n",
            "Training Loss: 0.5409232974052429\n",
            "Training Loss: 0.5507816076278687\n",
            "Training Loss: 0.6044877767562866\n",
            "Training Loss: 0.6488045454025269\n",
            "Training Loss: 0.5110899209976196\n",
            "Training Loss: 0.4596799314022064\n",
            "Training Loss: 0.5799129605293274\n",
            "Training Loss: 0.6812716126441956\n",
            "Training Loss: 0.4536464214324951\n",
            "Training Loss: 0.5221794843673706\n",
            "Training Loss: 0.616883397102356\n",
            "Training Loss: 0.5586934685707092\n",
            "Training Loss: 0.5069672465324402\n",
            "Training Loss: 0.6061528325080872\n",
            "Training Loss: 0.5378835797309875\n",
            "Training Loss: 0.721711277961731\n",
            "Training Loss: 0.39317846298217773\n",
            "Training Loss: 0.4497634768486023\n",
            "Training Loss: 0.4931887984275818\n",
            "Training Loss: 0.5006141066551208\n",
            "Training Loss: 0.6552473306655884\n",
            "Training Loss: 0.5165799260139465\n",
            "Training Loss: 0.451315313577652\n",
            "Training Loss: 0.4573882520198822\n",
            "Training Loss: 0.5733076930046082\n",
            "Training Loss: 0.46179088950157166\n",
            "Training Loss: 0.46308884024620056\n",
            "Training Loss: 0.6160105466842651\n",
            "Training Loss: 0.5217008590698242\n",
            "Training Loss: 0.5453363656997681\n",
            "Training Loss: 0.5611181855201721\n",
            "Training Loss: 0.618738055229187\n",
            "Training Loss: 0.4354282319545746\n",
            "Training Loss: 0.6076827049255371\n",
            "Training Loss: 0.5485429167747498\n",
            "Training Loss: 0.4931797981262207\n",
            "Training Loss: 0.438558429479599\n",
            "Training Loss: 0.5450975298881531\n",
            "Training Loss: 0.4753319323062897\n",
            "Training Loss: 0.4808548390865326\n",
            "Training Loss: 0.6170596480369568\n",
            "Training Loss: 0.6050202250480652\n",
            "Training Loss: 0.5909309983253479\n",
            "Training Loss: 0.4946119785308838\n",
            "Training Loss: 0.5093110799789429\n",
            "Training Loss: 0.4720214903354645\n",
            "Training Loss: 0.44825249910354614\n",
            "Training Loss: 0.5210500359535217\n",
            "Training Loss: 0.6756571531295776\n",
            "Training Loss: 0.5719576478004456\n",
            "Training Loss: 0.49723178148269653\n",
            "Training Loss: 0.5258408188819885\n",
            "Training Loss: 0.6639612317085266\n",
            "Training Loss: 0.40185609459877014\n",
            "Training Loss: 0.4868716299533844\n",
            "Training Loss: 0.5670364499092102\n",
            "Training Loss: 0.40594062209129333\n",
            "Training Loss: 0.5176358222961426\n",
            "Training Loss: 0.6373189687728882\n",
            "Training Loss: 0.5355384349822998\n",
            "Training Loss: 0.471078097820282\n",
            "Training Loss: 0.5338772535324097\n",
            "Training Loss: 0.5413822531700134\n",
            "Training Loss: 0.618010401725769\n",
            "Training Loss: 0.5148534774780273\n",
            "Training Loss: 0.6052507162094116\n",
            "Training Loss: 0.5267575979232788\n",
            "Training Loss: 0.7101232409477234\n",
            "Training Loss: 0.5351629853248596\n",
            "Training Loss: 0.40584665536880493\n",
            "Training Loss: 0.5262117385864258\n",
            "Training Loss: 0.45795005559921265\n",
            "Training Loss: 0.5524234175682068\n",
            "Training Loss: 0.39399874210357666\n",
            "Training Loss: 0.5002407431602478\n",
            "Training Loss: 0.6246501207351685\n",
            "Training Loss: 0.4287521243095398\n",
            "Training Loss: 0.6763370633125305\n",
            "Training Loss: 0.4970759451389313\n",
            "Training Loss: 0.5008481740951538\n",
            "Training Loss: 0.5858207941055298\n",
            "Training Loss: 0.6287258267402649\n",
            "Training Loss: 0.5191140174865723\n",
            "Training Loss: 0.45296621322631836\n",
            "Training Loss: 0.540619969367981\n",
            "Training Loss: 0.572078287601471\n",
            "Training Loss: 0.5489867925643921\n",
            "Training Loss: 0.6885913014411926\n",
            "Training Loss: 0.4934096336364746\n",
            "Training Loss: 0.5261248350143433\n",
            "Training Loss: 0.5855709910392761\n",
            "Training Loss: 0.4617699086666107\n",
            "Training Loss: 0.49911555647850037\n",
            "Training Loss: 0.518683135509491\n",
            "Training Loss: 0.40696215629577637\n",
            "Training Loss: 0.5768235325813293\n",
            "Training Loss: 0.44098082184791565\n",
            "Training Loss: 0.4351731240749359\n",
            "Training Loss: 0.4079974591732025\n",
            "Training Loss: 0.5117137432098389\n",
            "Training Loss: 0.5106713771820068\n",
            "Training Loss: 0.6054369211196899\n",
            "Training Loss: 0.5218887329101562\n",
            "Training Loss: 0.578231155872345\n",
            "Training Loss: 0.5034180283546448\n",
            "Training Loss: 0.5451978445053101\n",
            "Training Loss: 0.6953441500663757\n",
            "Training Loss: 0.5298305153846741\n",
            "Training Loss: 0.44693049788475037\n",
            "Training Loss: 0.6045355200767517\n",
            "Training Loss: 0.49555841088294983\n",
            "Training Loss: 0.4788711667060852\n",
            "Training Loss: 0.5647193789482117\n",
            "Training Loss: 0.5079494714736938\n",
            "Training Loss: 0.497907817363739\n",
            "Training Loss: 0.3712223470211029\n",
            "Training Loss: 0.5218151807785034\n",
            "Training Loss: 0.5159543752670288\n",
            "Training Loss: 0.5839018225669861\n",
            "Training Loss: 0.5507526993751526\n",
            "Training Loss: 0.5454533696174622\n",
            "Training Loss: 0.5906031131744385\n",
            "Training Loss: 0.49181729555130005\n",
            "Training Loss: 0.5116811394691467\n",
            "Training Loss: 0.5195695161819458\n",
            "Training Loss: 0.5402880907058716\n",
            "Training Loss: 0.44431981444358826\n",
            "Training Loss: 0.5173043012619019\n",
            "Training Loss: 0.41192689538002014\n",
            "Training Loss: 0.4674040675163269\n",
            "Training Loss: 0.4776081144809723\n",
            "Training Loss: 0.4642269015312195\n",
            "Training Loss: 0.5275266170501709\n",
            "Training Loss: 0.4041566848754883\n",
            "Training Loss: 0.5092629790306091\n",
            "Training Loss: 0.5171960592269897\n",
            "Training Loss: 0.3828240633010864\n",
            "Training Loss: 0.5992516279220581\n",
            "Training Loss: 0.4825366139411926\n",
            "Training Loss: 0.5677038431167603\n",
            "Training Loss: 0.6481798887252808\n",
            "Training Loss: 0.5214739441871643\n",
            "Training Loss: 0.6063988208770752\n",
            "Training Loss: 0.5617527961730957\n",
            "Training Loss: 0.39663130044937134\n",
            "Training Loss: 0.42612186074256897\n",
            "Training Loss: 0.46301454305648804\n",
            "Training Loss: 0.6244286298751831\n",
            "Training Loss: 0.4590694010257721\n",
            "Training Loss: 0.5417920351028442\n",
            "Training Loss: 0.4541413187980652\n",
            "Training Loss: 0.559202253818512\n",
            "Training Loss: 0.6847299933433533\n",
            "Training Loss: 0.537070095539093\n",
            "Training Loss: 0.41337713599205017\n",
            "Training Loss: 0.3699244558811188\n",
            "Training Loss: 0.6465712785720825\n",
            "Training Loss: 0.49217286705970764\n",
            "Training Loss: 0.5151541829109192\n",
            "Training Loss: 0.4819226861000061\n",
            "Training Loss: 0.4428366720676422\n",
            "Training Loss: 0.47998756170272827\n",
            "Training Loss: 0.477978378534317\n",
            "Training Loss: 0.6151443719863892\n",
            "Training Loss: 0.5244327783584595\n",
            "Training Loss: 0.5030497312545776\n",
            "Training Loss: 0.42841631174087524\n",
            "Training Loss: 0.5980985760688782\n",
            "Training Loss: 0.5006455183029175\n",
            "Training Loss: 0.5468584895133972\n",
            "Training Loss: 0.4463421702384949\n",
            "Training Loss: 0.3817822337150574\n",
            "Training Loss: 0.4249214231967926\n",
            "Training Loss: 0.4467322826385498\n",
            "Training Loss: 0.5698146820068359\n",
            "Training Loss: 0.6822440028190613\n",
            "Training Loss: 0.42744749784469604\n",
            "Training Loss: 0.33550888299942017\n",
            "Training Loss: 0.4508598744869232\n",
            "Training Loss: 0.5749741196632385\n",
            "Training Loss: 0.37960290908813477\n",
            "Training Loss: 0.425870805978775\n",
            "Training Loss: 0.5318995714187622\n",
            "Training Loss: 0.4766176640987396\n",
            "Training Loss: 0.6201942563056946\n",
            "Training Loss: 0.5138149261474609\n",
            "Training Loss: 0.39096537232398987\n",
            "Training Loss: 0.5461398363113403\n",
            "Training Loss: 0.49528664350509644\n",
            "Training Loss: 0.49186262488365173\n",
            "Training Loss: 0.37477922439575195\n",
            "Training Loss: 0.48091062903404236\n",
            "Training Loss: 0.3782862722873688\n",
            "Training Loss: 0.6704076528549194\n",
            "Training Loss: 0.5330560803413391\n",
            "Training Loss: 0.3769758939743042\n",
            "Training Loss: 0.5220564603805542\n",
            "Training Loss: 0.40012043714523315\n",
            "Training Loss: 0.4519309401512146\n",
            "Training Loss: 0.47802668809890747\n",
            "Training Loss: 0.5375107526779175\n",
            "Training Loss: 0.45107778906822205\n",
            "Training Loss: 0.4302147924900055\n",
            "Training Loss: 0.43886128067970276\n",
            "Training Loss: 0.42516231536865234\n",
            "Training Loss: 0.506583571434021\n",
            "Training Loss: 0.4652481973171234\n",
            "Training Loss: 0.6091187596321106\n",
            "Training Loss: 0.49231892824172974\n",
            "Training Loss: 0.44284576177597046\n",
            "Training Loss: 0.6303882598876953\n",
            "Training Loss: 0.5974284410476685\n",
            "Training Loss: 0.4696948528289795\n",
            "Training Loss: 0.3861114978790283\n",
            "Training Loss: 0.366740882396698\n",
            "Training Loss: 0.37764036655426025\n",
            "Training Loss: 0.6285809278488159\n",
            "Training Loss: 0.39028578996658325\n",
            "Training Loss: 0.6155025362968445\n",
            "Training Loss: 0.5260023474693298\n",
            "Training Loss: 0.5251199007034302\n",
            "Training Loss: 0.48681163787841797\n",
            "Training Loss: 0.44472208619117737\n",
            "Training Loss: 0.4832685589790344\n",
            "Training Loss: 0.34430569410324097\n",
            "Training Loss: 0.38954633474349976\n",
            "Training Loss: 0.48942217230796814\n",
            "Training Loss: 0.5623214244842529\n",
            "Training Loss: 0.4833085536956787\n",
            "Training Loss: 0.5724822878837585\n",
            "Training Loss: 0.5464929342269897\n",
            "Training Loss: 0.5045750141143799\n",
            "Training Loss: 0.43723058700561523\n",
            "Training Loss: 0.5996579527854919\n",
            "Training Loss: 0.6387091279029846\n",
            "Training Loss: 0.37379032373428345\n",
            "Training Loss: 0.3966488242149353\n",
            "Training Loss: 0.4442273676395416\n",
            "Training Loss: 0.5303584337234497\n",
            "Training Loss: 0.39791661500930786\n",
            "Training Loss: 0.4810273051261902\n",
            "Training Loss: 0.37686532735824585\n",
            "Training Loss: 0.3190513253211975\n",
            "Training Loss: 0.36482298374176025\n",
            "Training Loss: 0.378085196018219\n",
            "Training Loss: 0.664854109287262\n",
            "Training Loss: 0.45008260011672974\n",
            "Training Loss: 0.4410347044467926\n",
            "Training Loss: 0.6229848265647888\n",
            "Training Loss: 0.5695542097091675\n",
            "Training Loss: 0.5137091279029846\n",
            "Training Loss: 0.6078943014144897\n",
            "Training Loss: 0.35886403918266296\n",
            "Training Loss: 0.40702953934669495\n",
            "Training Loss: 0.5086120367050171\n",
            "Training Loss: 0.4684935212135315\n",
            "Training Loss: 0.458036333322525\n",
            "Training Loss: 0.5891343355178833\n",
            "Training Loss: 0.38139182329177856\n",
            "Training Loss: 0.4556788504123688\n",
            "Training Loss: 0.5544611811637878\n",
            "Training Loss: 0.5425727367401123\n",
            "Training Loss: 0.5166485905647278\n",
            "Training Loss: 0.6762522459030151\n",
            "Training Loss: 0.5471118688583374\n",
            "Training Loss: 0.34147703647613525\n",
            "Training Loss: 0.6571643352508545\n",
            "Training Loss: 0.4391215741634369\n",
            "Training Loss: 0.5587505102157593\n",
            "Training Loss: 0.4466828405857086\n",
            "Training Loss: 0.43594083189964294\n",
            "Training Loss: 0.3625718951225281\n",
            "Training Loss: 0.3852262496948242\n",
            "Training Loss: 0.2951567769050598\n",
            "Training Loss: 0.40539246797561646\n",
            "Training Loss: 0.3743026852607727\n",
            "Training Loss: 0.39988479018211365\n",
            "Training Loss: 0.3185666501522064\n",
            "Training Loss: 0.4279441237449646\n",
            "Training Loss: 0.3752405345439911\n",
            "Training Loss: 0.4153391718864441\n",
            "Training Loss: 0.44927850365638733\n",
            "Training Loss: 0.5633643865585327\n",
            "Training Loss: 0.5708743929862976\n",
            "Training Loss: 0.3910680413246155\n",
            "Training Loss: 0.46061110496520996\n",
            "Training Loss: 0.4680849611759186\n",
            "Training Loss: 0.3693736791610718\n",
            "Training Loss: 0.38026800751686096\n",
            "Training Loss: 0.31230178475379944\n",
            "Training Loss: 0.5559182167053223\n",
            "Training Loss: 0.3974512815475464\n",
            "Training Loss: 0.5368798971176147\n",
            "Training Loss: 0.4854664206504822\n",
            "Training Loss: 0.6119867563247681\n",
            "Training Loss: 0.48029565811157227\n",
            "Training Loss: 0.599610447883606\n",
            "Training Loss: 0.4014894962310791\n",
            "Training Loss: 0.4976126253604889\n",
            "Training Loss: 0.40794849395751953\n",
            "Training Loss: 0.7124105095863342\n",
            "Training Loss: 0.47057920694351196\n",
            "Training Loss: 0.7752268314361572\n",
            "Training Loss: 0.5175844430923462\n",
            "Training Loss: 0.49731913208961487\n",
            "Training Loss: 0.48025447130203247\n",
            "Training Loss: 0.5490810871124268\n",
            "Training Loss: 0.6676573753356934\n",
            "Training Loss: 0.5234445333480835\n",
            "Training Loss: 0.5880445241928101\n",
            "Training Loss: 0.3620489239692688\n",
            "Training Loss: 0.41571393609046936\n",
            "Training Loss: 0.37553438544273376\n",
            "Training Loss: 0.5006553530693054\n",
            "Training Loss: 0.49291810393333435\n",
            "Training Loss: 0.5329419374465942\n",
            "Training Loss: 0.5817156434059143\n",
            "Training Loss: 0.36082953214645386\n",
            "Training Loss: 0.3727627396583557\n",
            "Training Loss: 0.4253372251987457\n",
            "Training Loss: 0.5625743865966797\n",
            "Training Loss: 0.3820323050022125\n",
            "Training Loss: 0.5528802871704102\n",
            "Training Loss: 0.5235058665275574\n",
            "Training Loss: 0.4340745806694031\n",
            "Training Loss: 0.4544323980808258\n",
            "Training Loss: 0.4121396541595459\n",
            "Training Loss: 0.5104520916938782\n",
            "Training Loss: 0.5428475141525269\n",
            "Training Loss: 0.5530930757522583\n",
            "Training Loss: 0.31012123823165894\n",
            "Training Loss: 0.5701171159744263\n",
            "Training Loss: 0.49799567461013794\n",
            "Training Loss: 0.47931766510009766\n",
            "Training Loss: 0.5175066590309143\n",
            "Training Loss: 0.38155603408813477\n",
            "Training Loss: 0.4286721348762512\n",
            "Training Loss: 0.4067944884300232\n",
            "Training Loss: 0.47496557235717773\n",
            "Training Loss: 0.4423678517341614\n",
            "Training Loss: 0.5103920698165894\n",
            "Training Loss: 0.37973642349243164\n",
            "Training Loss: 0.39676836133003235\n",
            "Training Loss: 0.48252105712890625\n",
            "Training Loss: 0.3406132757663727\n",
            "Training Loss: 0.3346182703971863\n",
            "Training Loss: 0.4429861009120941\n",
            "Training Loss: 0.46811968088150024\n",
            "Training Loss: 0.5391167402267456\n",
            "Training Loss: 0.5586048364639282\n",
            "Training Loss: 0.4159802496433258\n",
            "Training Loss: 0.5944312810897827\n",
            "Training Loss: 0.6162415146827698\n",
            "Training Loss: 0.513717532157898\n",
            "Training Loss: 0.4374074935913086\n",
            "Training Loss: 0.3952035903930664\n",
            "Training Loss: 0.439491331577301\n",
            "Training Loss: 0.43378114700317383\n",
            "Training Loss: 0.5964395999908447\n",
            "Training Loss: 0.5515743494033813\n",
            "Training Loss: 0.5308555364608765\n",
            "Training Loss: 0.4691019058227539\n",
            "Training Loss: 0.4412473738193512\n",
            "Training Loss: 0.4304254949092865\n",
            "Training Loss: 0.5587009787559509\n",
            "Training Loss: 0.3205769956111908\n",
            "Training Loss: 0.4217822551727295\n",
            "Training Loss: 0.6882190108299255\n",
            "Training Loss: 0.5490835309028625\n",
            "Training Loss: 0.48011961579322815\n",
            "Training Loss: 0.32128793001174927\n",
            "Training Loss: 0.34861820936203003\n",
            "Training Loss: 0.3978562355041504\n",
            "Training Loss: 0.35145559906959534\n",
            "Training Loss: 0.3491191864013672\n",
            "Training Loss: 0.43354734778404236\n",
            "Training Loss: 0.49692219495773315\n",
            "Training Loss: 0.6073040962219238\n",
            "Training Loss: 0.26599448919296265\n",
            "Training Loss: 0.5223934054374695\n",
            "Training Loss: 0.49720272421836853\n",
            "Training Loss: 0.6978610157966614\n",
            "Training Loss: 0.6558836698532104\n",
            "Training Loss: 0.3424106538295746\n",
            "Training Loss: 0.596738874912262\n",
            "Training Loss: 0.4676057696342468\n",
            "Training Loss: 0.38139307498931885\n",
            "Training Loss: 0.4846748411655426\n",
            "Training Loss: 0.4480131268501282\n",
            "Training Loss: 0.47655749320983887\n",
            "Training Loss: 0.35179126262664795\n",
            "Training Loss: 0.5405699610710144\n",
            "Training Loss: 0.37201258540153503\n",
            "Training Loss: 0.49087661504745483\n",
            "Training Loss: 0.387983113527298\n",
            "Training Loss: 0.6030593514442444\n",
            "Training Loss: 0.4435111880302429\n",
            "Training Loss: 0.37541815638542175\n",
            "Training Loss: 0.34108027815818787\n",
            "Training Loss: 0.4657815396785736\n",
            "Training Loss: 0.3989107012748718\n",
            "Training Loss: 0.6121285557746887\n",
            "Training Loss: 0.536123514175415\n",
            "Training Loss: 0.3541080951690674\n",
            "Training Loss: 0.40668660402297974\n",
            "Training Loss: 0.38035961985588074\n",
            "Training Loss: 0.5203589200973511\n",
            "Training Loss: 0.480959951877594\n",
            "Training Loss: 0.5889542102813721\n",
            "Training Loss: 0.5406044125556946\n",
            "Training Loss: 0.5533558130264282\n",
            "Training Loss: 0.577059268951416\n",
            "Training Loss: 0.38335222005844116\n",
            "Training Loss: 0.5248721241950989\n",
            "Training Loss: 0.426534503698349\n",
            "Training Loss: 0.4373672604560852\n",
            "Training Loss: 0.5925725698471069\n",
            "Training Loss: 0.39071381092071533\n",
            "Training Loss: 0.24151034653186798\n",
            "Training Loss: 0.5107595324516296\n",
            "Training Loss: 0.49323949217796326\n",
            "Training Loss: 0.43342477083206177\n",
            "Training Loss: 0.34199148416519165\n",
            "Training Loss: 0.4586276113986969\n",
            "Training Loss: 0.34649592638015747\n",
            "Training Loss: 0.4982635974884033\n",
            "Training Loss: 0.5871718525886536\n",
            "Training Loss: 0.4726645052433014\n",
            "Training Loss: 0.39528888463974\n",
            "Training Loss: 0.46399641036987305\n",
            "Training Loss: 0.3700037896633148\n",
            "Training Loss: 0.3316105604171753\n",
            "Training Loss: 0.5182985067367554\n",
            "Training Loss: 0.4074315130710602\n",
            "Training Loss: 0.36247971653938293\n",
            "Training Loss: 0.3303164839744568\n",
            "Training Loss: 0.6841911673545837\n",
            "Training Loss: 0.30592620372772217\n",
            "Training Loss: 0.5819093585014343\n",
            "Training Loss: 0.44160228967666626\n",
            "Training Loss: 0.38310056924819946\n",
            "Training Loss: 0.43287307024002075\n",
            "Training Loss: 0.47463783621788025\n",
            "Training Loss: 0.5614384412765503\n",
            "Training Loss: 0.3401400148868561\n",
            "Training Loss: 0.45493775606155396\n",
            "Training Loss: 0.4210807681083679\n",
            "Training Loss: 0.39803820848464966\n",
            "Training Loss: 0.34083691239356995\n",
            "Training Loss: 0.2805805206298828\n",
            "Training Loss: 0.5564260482788086\n",
            "Training Loss: 0.30749931931495667\n",
            "Training Loss: 0.5656484365463257\n",
            "Training Loss: 0.5435999035835266\n",
            "Training Loss: 0.3036777973175049\n",
            "Training Loss: 0.45234736800193787\n",
            "Training Loss: 0.43959856033325195\n",
            "Training Loss: 0.42891979217529297\n",
            "Training Loss: 0.22486034035682678\n",
            "Training Loss: 0.5778265595436096\n",
            "Training Loss: 0.4561391770839691\n",
            "Training Loss: 0.5199277400970459\n",
            "Training Loss: 0.40328526496887207\n",
            "Training Loss: 0.2622489929199219\n",
            "Training Loss: 0.38107219338417053\n",
            "Training Loss: 0.4758376479148865\n",
            "Training Loss: 0.46631431579589844\n",
            "Training Loss: 0.365916907787323\n",
            "Training Loss: 0.49892187118530273\n",
            "Training Loss: 0.4947277009487152\n",
            "Training Loss: 0.33085066080093384\n",
            "Training Loss: 0.4862096905708313\n",
            "Training Loss: 0.47873467206954956\n",
            "Training Loss: 0.45778122544288635\n",
            "Training Loss: 0.4448397755622864\n",
            "Training Loss: 0.41298651695251465\n",
            "Training Loss: 0.4348899722099304\n",
            "Training Loss: 0.3746871054172516\n",
            "Training Loss: 0.43501392006874084\n",
            "Training Loss: 0.5974446535110474\n",
            "Training Loss: 0.31783053278923035\n",
            "Training Loss: 0.5727585554122925\n",
            "Training Loss: 0.5782621502876282\n",
            "Training Loss: 0.4882168173789978\n",
            "Training Loss: 0.4547182619571686\n",
            "Training Loss: 0.4058558940887451\n",
            "Training Loss: 0.4322373867034912\n",
            "Training Loss: 0.5596049427986145\n",
            "Training Loss: 0.40072137117385864\n",
            "Training Loss: 0.22796344757080078\n",
            "Training Loss: 0.2939395010471344\n",
            "Training Loss: 0.43589216470718384\n",
            "Training Loss: 0.279601514339447\n",
            "Training Loss: 0.42369207739830017\n",
            "Training Loss: 0.5128505229949951\n",
            "Training Loss: 0.45752352476119995\n",
            "Training Loss: 0.2992956340312958\n",
            "Training Loss: 0.4126889705657959\n",
            "Training Loss: 0.3951837420463562\n",
            "Training Loss: 0.3618656098842621\n",
            "Training Loss: 0.5972973108291626\n",
            "Training Loss: 0.6019558310508728\n",
            "Training Loss: 0.7358325719833374\n",
            "Training Loss: 0.39742571115493774\n",
            "Training Loss: 0.6663678288459778\n",
            "Training Loss: 0.39810264110565186\n",
            "Training Loss: 0.30052974820137024\n",
            "Training Loss: 0.35267525911331177\n",
            "Training Loss: 0.5196632146835327\n",
            "Training Loss: 0.5032473802566528\n",
            "Training Loss: 0.4278181195259094\n",
            "Training Loss: 0.3878510594367981\n",
            "Training Loss: 0.5835881233215332\n",
            "Training Loss: 0.5914542078971863\n",
            "Training Loss: 0.44981294870376587\n",
            "Training Loss: 0.4571910798549652\n",
            "Training Loss: 0.32514843344688416\n",
            "Training Loss: 0.3291204869747162\n",
            "Training Loss: 0.4040261209011078\n",
            "Training Loss: 0.3143026530742645\n",
            "Training Loss: 0.4762107729911804\n",
            "Training Loss: 0.3967093527317047\n",
            "Training Loss: 0.36129969358444214\n",
            "Training Loss: 0.2989431619644165\n",
            "Training Loss: 0.3146710991859436\n",
            "Training Loss: 0.37231484055519104\n",
            "Training Loss: 0.5066050291061401\n",
            "Training Loss: 0.3415502905845642\n",
            "Training Loss: 0.516949474811554\n",
            "Training Loss: 0.4015991687774658\n",
            "Training Loss: 0.5828835368156433\n",
            "Training Loss: 0.4839392304420471\n",
            "Training Loss: 0.4614867568016052\n",
            "Training Loss: 0.3119138777256012\n",
            "Training Loss: 0.5086221694946289\n",
            "Training Loss: 0.6232329607009888\n",
            "Training Loss: 0.6168181300163269\n",
            "Training Loss: 0.5359672904014587\n",
            "Training Loss: 0.39501652121543884\n",
            "Training Loss: 0.548362672328949\n",
            "Training Loss: 0.34202003479003906\n",
            "Training Loss: 0.45762476325035095\n",
            "Training Loss: 0.4402490258216858\n",
            "Training Loss: 0.46578627824783325\n",
            "Training Loss: 0.36320024728775024\n",
            "Training Loss: 0.5697811841964722\n",
            "Training Loss: 0.32662108540534973\n",
            "Training Loss: 0.4691241383552551\n",
            "Training Loss: 0.3721342980861664\n",
            "Training Loss: 0.4475094676017761\n",
            "Training Loss: 0.42878788709640503\n",
            "Training Loss: 0.5591657161712646\n",
            "Training Loss: 0.4275413453578949\n",
            "Training Loss: 0.4333038330078125\n",
            "Training Loss: 0.2821618616580963\n",
            "Training Loss: 0.38475722074508667\n",
            "Training Loss: 0.4262538552284241\n",
            "Training Loss: 0.24460360407829285\n",
            "Training Loss: 0.4380587041378021\n",
            "Training Loss: 0.6233068704605103\n",
            "Training Loss: 0.4245356619358063\n",
            "Training Loss: 0.3132142126560211\n",
            "Training Loss: 0.4673750400543213\n",
            "Training Loss: 0.2932918965816498\n",
            "Training Loss: 0.4812173843383789\n",
            "Training Loss: 0.3785526156425476\n",
            "Training Loss: 0.31248873472213745\n",
            "Training Loss: 0.4098307192325592\n",
            "Training Loss: 0.27372297644615173\n",
            "Training Loss: 0.36617714166641235\n",
            "Training Loss: 0.4967251420021057\n",
            "Training Loss: 0.40595585107803345\n",
            "Training Loss: 0.44952115416526794\n",
            "Training Loss: 0.40753379464149475\n",
            "Training Loss: 0.30007612705230713\n",
            "Training Loss: 0.509618878364563\n",
            "Training Loss: 0.42808833718299866\n",
            "Training Loss: 0.3522043526172638\n",
            "Training Loss: 0.500373125076294\n",
            "Training Loss: 0.40165239572525024\n",
            "Training Loss: 0.6286357045173645\n",
            "Training Loss: 0.4528135657310486\n",
            "Training Loss: 0.41540002822875977\n",
            "Training Loss: 0.4093925356864929\n",
            "Training Loss: 0.37156182527542114\n",
            "Training Loss: 0.5511530637741089\n",
            "Training Loss: 0.4214909076690674\n",
            "Training Loss: 0.45250844955444336\n",
            "Training Loss: 0.3599514663219452\n",
            "Training Loss: 0.40854400396347046\n",
            "Training Loss: 0.5569331049919128\n",
            "Training Loss: 0.4644535481929779\n",
            "Training Loss: 0.49760985374450684\n",
            "Training Loss: 0.45621195435523987\n",
            "Training Loss: 0.5145691633224487\n",
            "Training Loss: 0.43312376737594604\n",
            "Training Loss: 0.4140821099281311\n",
            "Training Loss: 0.34595629572868347\n",
            "Training Loss: 0.4047737717628479\n",
            "Training Loss: 0.35803717374801636\n",
            "Training Loss: 0.5688148736953735\n",
            "Training Loss: 0.3456839621067047\n",
            "Training Loss: 0.3498845100402832\n",
            "Training Loss: 0.2710880637168884\n",
            "Training Loss: 0.27314046025276184\n",
            "Training Loss: 0.4039991497993469\n",
            "Training Loss: 0.40818238258361816\n",
            "Training Loss: 0.4680509567260742\n",
            "Training Loss: 0.42540407180786133\n",
            "Training Loss: 0.3495810031890869\n",
            "Training Loss: 0.33305904269218445\n",
            "Training Loss: 0.511724054813385\n",
            "Training Loss: 0.2827147841453552\n",
            "Training Loss: 0.26996153593063354\n",
            "Training Loss: 0.35196512937545776\n",
            "Training Loss: 0.4530009329319\n",
            "Training Loss: 0.4389665722846985\n",
            "Training Loss: 0.46730080246925354\n",
            "Training Loss: 0.4299807548522949\n",
            "Training Loss: 0.4444473683834076\n",
            "Training Loss: 0.34424805641174316\n",
            "Training Loss: 0.19427259266376495\n",
            "Training Loss: 0.3206585645675659\n",
            "Training Loss: 0.3479316234588623\n",
            "Training Loss: 0.4252406656742096\n",
            "Training Loss: 0.5887302160263062\n",
            "Training Loss: 0.4747423231601715\n",
            "Training Loss: 0.3785821199417114\n",
            "Training Loss: 0.3123120665550232\n",
            "Training Loss: 0.43906909227371216\n",
            "Training Loss: 0.46938556432724\n",
            "Training Loss: 0.2986504137516022\n",
            "Training Loss: 0.2927403748035431\n",
            "Training Loss: 0.31787288188934326\n",
            "Training Loss: 0.3968569040298462\n",
            "Training Loss: 0.2631080448627472\n",
            "Training Loss: 0.4903877377510071\n",
            "Training Loss: 0.4291221499443054\n",
            "Training Loss: 0.34175580739974976\n",
            "Training Loss: 0.27197855710983276\n",
            "Training Loss: 0.44501009583473206\n",
            "Training Loss: 0.40236878395080566\n",
            "Training Loss: 0.43458667397499084\n",
            "Training Loss: 0.3670991361141205\n",
            "Training Loss: 0.27786847949028015\n",
            "Training Loss: 0.4151670038700104\n",
            "Training Loss: 0.47713083028793335\n",
            "Training Loss: 0.42927271127700806\n",
            "Training Loss: 0.2675689160823822\n",
            "Training Loss: 0.37535780668258667\n",
            "Training Loss: 0.45270437002182007\n",
            "Training Loss: 0.4236513674259186\n",
            "Training Loss: 0.41401809453964233\n",
            "Training Loss: 0.5413678884506226\n",
            "Training Loss: 0.5464221239089966\n",
            "Training Loss: 0.2848854660987854\n",
            "Training Loss: 0.7403535842895508\n",
            "Training Loss: 0.3954603970050812\n",
            "Training Loss: 0.2686226963996887\n",
            "Training Loss: 0.28971540927886963\n",
            "Training Loss: 0.3676729202270508\n",
            "Training Loss: 0.37002068758010864\n",
            "Training Loss: 0.29515212774276733\n",
            "Training Loss: 0.25145870447158813\n",
            "Training Loss: 0.4241648316383362\n",
            "Training Loss: 0.40198764204978943\n",
            "Training Loss: 0.3048357367515564\n",
            "Training Loss: 0.3471861183643341\n",
            "Training Loss: 0.27249854803085327\n",
            "Training Loss: 0.27675390243530273\n",
            "Training Loss: 0.3716389834880829\n",
            "Training Loss: 0.5318587422370911\n",
            "Training Loss: 0.39214271306991577\n",
            "Training Loss: 0.3948308229446411\n",
            "Training Loss: 0.4064922332763672\n",
            "Training Loss: 0.2631680369377136\n",
            "Training Loss: 0.40334129333496094\n",
            "Training Loss: 0.2961735427379608\n",
            "Training Loss: 0.29738301038742065\n",
            "Training Loss: 0.3371875286102295\n",
            "Training Loss: 0.3389347791671753\n",
            "Training Loss: 0.37709468603134155\n",
            "Training Loss: 0.2812005579471588\n",
            "Training Loss: 0.2696714401245117\n",
            "Training Loss: 0.5023335814476013\n",
            "Training Loss: 0.31627339124679565\n",
            "Training Loss: 0.26168209314346313\n",
            "Training Loss: 0.39654311537742615\n",
            "Training Loss: 0.2586686611175537\n",
            "Training Loss: 0.47189751267433167\n",
            "Training Loss: 0.4513433575630188\n",
            "Training Loss: 0.40338459610939026\n",
            "Training Loss: 0.21545056998729706\n",
            "Training Loss: 0.45783543586730957\n",
            "Training Loss: 0.2753455638885498\n",
            "Training Loss: 0.5056626796722412\n",
            "Training Loss: 0.34084588289260864\n",
            "Training Loss: 0.2731765806674957\n",
            "Training Loss: 0.3834199011325836\n",
            "Training Loss: 0.48929905891418457\n",
            "Training Loss: 0.3035169243812561\n",
            "Training Loss: 0.42660197615623474\n",
            "Training Loss: 0.3410959839820862\n",
            "Training Loss: 0.41177549958229065\n",
            "Training Loss: 0.5178285837173462\n",
            "Training Loss: 0.2955404222011566\n",
            "Training Loss: 0.4221906065940857\n",
            "Training Loss: 0.3364954888820648\n",
            "Training Loss: 0.26998183131217957\n",
            "Training Loss: 0.3759516775608063\n",
            "Training Loss: 0.3552708029747009\n",
            "Training Loss: 0.24889712035655975\n",
            "Training Loss: 0.375497043132782\n",
            "Training Loss: 0.44736915826797485\n",
            "Training Loss: 0.3675491213798523\n",
            "Training Loss: 0.3075905442237854\n",
            "Training Loss: 0.37347421050071716\n",
            "Training Loss: 0.24086269736289978\n",
            "Training Loss: 0.4228028357028961\n",
            "Training Loss: 0.36861157417297363\n",
            "Training Loss: 0.28068190813064575\n",
            "Training Loss: 0.2831980586051941\n",
            "Training Loss: 0.3260616064071655\n",
            "Training Loss: 0.4095703959465027\n",
            "Training Loss: 0.27196773886680603\n",
            "Training Loss: 0.2411685287952423\n",
            "Training Loss: 0.22817650437355042\n",
            "Training Loss: 0.24438457190990448\n",
            "Training Loss: 0.4057418704032898\n",
            "Training Loss: 0.29558631777763367\n",
            "Training Loss: 0.38391542434692383\n",
            "Training Loss: 0.5097826719284058\n",
            "Training Loss: 0.22125522792339325\n",
            "Training Loss: 0.2633694112300873\n",
            "Training Loss: 0.42734432220458984\n",
            "Training Loss: 0.3924137353897095\n",
            "Training Loss: 0.42615851759910583\n",
            "Training Loss: 0.3754864037036896\n",
            "Training Loss: 0.3998042941093445\n",
            "Training Loss: 0.35184234380722046\n",
            "Training Loss: 0.4495593011379242\n",
            "Training Loss: 0.2706339359283447\n",
            "Training Loss: 0.3723004162311554\n",
            "Training Loss: 0.38759762048721313\n",
            "Training Loss: 0.38943883776664734\n",
            "Training Loss: 0.4478076994419098\n",
            "Training Loss: 0.3952220380306244\n",
            "Training Loss: 0.37323302030563354\n",
            "Training Loss: 0.29586759209632874\n",
            "Training Loss: 0.239587664604187\n",
            "Training Loss: 0.5418431162834167\n",
            "Training Loss: 0.26321709156036377\n",
            "Training Loss: 0.6444429159164429\n",
            "Training Loss: 0.29890725016593933\n",
            "Training Loss: 0.289320707321167\n",
            "Training Loss: 0.2805306315422058\n",
            "Training Loss: 0.5219334959983826\n",
            "Training Loss: 0.4410766661167145\n",
            "Training Loss: 0.30192244052886963\n",
            "Training Loss: 0.5673755407333374\n",
            "Training Loss: 0.3591708242893219\n",
            "Training Loss: 0.3594895303249359\n",
            "Training Loss: 0.33743980526924133\n",
            "Training Loss: 0.2832241356372833\n",
            "Training Loss: 0.21147525310516357\n",
            "Training Loss: 0.31657785177230835\n",
            "Training Loss: 0.6098716855049133\n",
            "Training Loss: 0.4666743278503418\n",
            "Training Loss: 0.5143588185310364\n",
            "Training Loss: 0.3229869604110718\n",
            "Training Loss: 0.30750009417533875\n",
            "Training Loss: 0.49121397733688354\n",
            "Training Loss: 0.3047807812690735\n",
            "Training Loss: 0.5095492005348206\n",
            "Training Loss: 0.3079224228858948\n",
            "Training Loss: 0.20170199871063232\n",
            "Training Loss: 0.30816763639450073\n",
            "Training Loss: 0.2936360836029053\n",
            "Training Loss: 0.34654414653778076\n",
            "Training Loss: 0.41846078634262085\n",
            "Training Loss: 0.3850801885128021\n",
            "Training Loss: 0.23360195755958557\n",
            "Training Loss: 0.4149937629699707\n",
            "Training Loss: 0.32998359203338623\n",
            "Training Loss: 0.43655627965927124\n",
            "Training Loss: 0.3311190903186798\n",
            "Training Loss: 0.3754092752933502\n",
            "Training Loss: 0.2806549370288849\n",
            "Training Loss: 0.3848258852958679\n",
            "Training Loss: 0.3624041676521301\n",
            "Training Loss: 0.36992883682250977\n",
            "Training Loss: 0.2777988016605377\n",
            "Training Loss: 0.22559861838817596\n",
            "Training Loss: 0.25845393538475037\n",
            "Training Loss: 0.30825671553611755\n",
            "Training Loss: 0.39039739966392517\n",
            "Training Loss: 0.30772164463996887\n",
            "Training Loss: 0.23205061256885529\n",
            "Training Loss: 0.2717798948287964\n",
            "Training Loss: 0.20799346268177032\n",
            "Training Loss: 0.23665830492973328\n",
            "Training Loss: 0.2248498648405075\n",
            "Training Loss: 0.4315827488899231\n",
            "Training Loss: 0.25773105025291443\n",
            "Training Loss: 0.4066386818885803\n",
            "Training Loss: 0.29879873991012573\n",
            "Training Loss: 0.3955244719982147\n",
            "Training Loss: 0.39079171419143677\n",
            "Training Loss: 0.3639703392982483\n",
            "Training Loss: 0.32564589381217957\n",
            "Training Loss: 0.25687527656555176\n",
            "Training Loss: 0.2485247552394867\n",
            "Training Loss: 0.24858073890209198\n",
            "Training Loss: 0.32419726252555847\n",
            "Training Loss: 0.40838608145713806\n",
            "Training Loss: 0.3712266683578491\n",
            "Training Loss: 0.3188015818595886\n",
            "Training Loss: 0.26769158244132996\n",
            "Training Loss: 0.30544883012771606\n",
            "Training Loss: 0.24668240547180176\n",
            "Training Loss: 0.5097996592521667\n",
            "Training Loss: 0.49252063035964966\n",
            "Training Loss: 0.629684567451477\n",
            "Training Loss: 0.31902578473091125\n",
            "Training Loss: 0.28314584493637085\n",
            "Training Loss: 0.38910046219825745\n",
            "Training Loss: 0.4077053964138031\n",
            "Training Loss: 0.5299090147018433\n",
            "Training Loss: 0.40549761056900024\n",
            "Training Loss: 0.24948935210704803\n",
            "Training Loss: 0.25632745027542114\n",
            "Training Loss: 0.3747127950191498\n",
            "Training Loss: 0.5164647102355957\n",
            "Training Loss: 0.3498697578907013\n",
            "Training Loss: 0.2334899604320526\n",
            "Training Loss: 0.25802481174468994\n",
            "Training Loss: 0.29401278495788574\n",
            "Training Loss: 0.19755251705646515\n",
            "Training Loss: 0.3497060239315033\n",
            "Training Loss: 0.2423413246870041\n",
            "Training Loss: 0.38887661695480347\n",
            "Training Loss: 0.39180177450180054\n",
            "Training Loss: 0.4803653359413147\n",
            "Training Loss: 0.4101738929748535\n",
            "Training Loss: 0.39875155687332153\n",
            "Training Loss: 0.4735165536403656\n",
            "Training Loss: 0.3232141137123108\n",
            "Training Loss: 0.32733258605003357\n",
            "Training Loss: 0.25025826692581177\n",
            "Training Loss: 0.2548409104347229\n",
            "Training Loss: 0.2739143967628479\n",
            "Training Loss: 0.25368067622184753\n",
            "Training Loss: 0.3521896004676819\n",
            "Training Loss: 0.6680529117584229\n",
            "Training Loss: 0.23984971642494202\n",
            "Training Loss: 0.2703051269054413\n",
            "Training Loss: 0.3523945212364197\n",
            "Training Loss: 0.2302546501159668\n",
            "Training Loss: 0.3985476493835449\n",
            "Training Loss: 0.37652748823165894\n",
            "Training Loss: 0.2533629834651947\n",
            "Training Loss: 0.38549500703811646\n",
            "Training Loss: 0.2378791868686676\n",
            "Training Loss: 0.28865617513656616\n",
            "Training Loss: 0.3805675506591797\n",
            "Training Loss: 0.5187972784042358\n",
            "Training Loss: 0.33359888195991516\n",
            "Training Loss: 0.25979748368263245\n",
            "Training Loss: 0.2550598978996277\n",
            "Training Loss: 0.2718486189842224\n",
            "Training Loss: 0.40989693999290466\n",
            "Training Loss: 0.4581245481967926\n",
            "Training Loss: 0.3593667149543762\n",
            "Training Loss: 0.24374163150787354\n",
            "Training Loss: 0.21820513904094696\n",
            "Training Loss: 0.2772122919559479\n",
            "Training Loss: 0.24469177424907684\n",
            "Training Loss: 0.2599044442176819\n",
            "Training Loss: 0.3669334053993225\n",
            "Training Loss: 0.2592192590236664\n",
            "Training Loss: 0.25560981035232544\n",
            "Training Loss: 0.5050889253616333\n",
            "Training Loss: 0.25102269649505615\n",
            "Training Loss: 0.48577994108200073\n",
            "Training Loss: 0.2498476803302765\n",
            "Training Loss: 0.4012625813484192\n",
            "Training Loss: 0.4931524395942688\n",
            "Training Loss: 0.26084452867507935\n",
            "Training Loss: 0.368661493062973\n",
            "Training Loss: 0.21072474122047424\n",
            "Training Loss: 0.23403210937976837\n",
            "Training Loss: 0.6000595092773438\n",
            "Training Loss: 0.36550766229629517\n",
            "Training Loss: 0.40535768866539\n",
            "Training Loss: 0.25568243861198425\n",
            "Training Loss: 0.2720893323421478\n",
            "Training Loss: 0.39275509119033813\n",
            "Training Loss: 0.3973778784275055\n",
            "Training Loss: 0.35161787271499634\n",
            "Training Loss: 0.2699565589427948\n",
            "Training Loss: 0.34050995111465454\n",
            "Training Loss: 0.21003243327140808\n",
            "Training Loss: 0.32891935110092163\n",
            "Training Loss: 0.25583523511886597\n",
            "Training Loss: 0.23271575570106506\n",
            "Training Loss: 0.34132450819015503\n",
            "Training Loss: 0.41079267859458923\n",
            "Training Loss: 0.2033703774213791\n",
            "Training Loss: 0.3714218735694885\n",
            "Training Loss: 0.34809693694114685\n",
            "Training Loss: 0.40814918279647827\n",
            "Training Loss: 0.18841847777366638\n",
            "Training Loss: 0.36931878328323364\n",
            "Training Loss: 0.492311954498291\n",
            "Training Loss: 0.3571780025959015\n",
            "Training Loss: 0.208783820271492\n",
            "Training Loss: 0.4424766004085541\n",
            "Training Loss: 0.2105601578950882\n",
            "Training Loss: 0.6711880564689636\n",
            "Training Loss: 0.3074144721031189\n",
            "Training Loss: 0.4054839015007019\n",
            "Training Loss: 0.39142078161239624\n",
            "Training Loss: 0.2048853635787964\n",
            "Training Loss: 0.33218178153038025\n",
            "Training Loss: 0.43077516555786133\n",
            "Training Loss: 0.23997636139392853\n",
            "Training Loss: 0.4841303825378418\n",
            "Training Loss: 0.22049441933631897\n",
            "Training Loss: 0.4983576834201813\n",
            "Training Loss: 0.7241432666778564\n",
            "Training Loss: 0.5027215480804443\n",
            "Training Loss: 0.38978904485702515\n",
            "Training Loss: 0.2744021713733673\n",
            "Training Loss: 0.19845327734947205\n",
            "Training Loss: 0.18283171951770782\n",
            "Training Loss: 0.4231959879398346\n",
            "Training Loss: 0.5044217705726624\n",
            "Training Loss: 0.33320721983909607\n",
            "Training Loss: 0.24605360627174377\n",
            "Training Loss: 0.22688286006450653\n",
            "Training Loss: 0.436380535364151\n",
            "Training Loss: 0.2146700918674469\n",
            "Training Loss: 0.40297070145606995\n",
            "Training Loss: 0.42110776901245117\n",
            "Training Loss: 0.36167222261428833\n",
            "Training Loss: 0.3783974051475525\n",
            "Training Loss: 0.19977262616157532\n",
            "Training Loss: 0.2325134575366974\n",
            "Training Loss: 0.21016812324523926\n",
            "Training Loss: 0.4964768886566162\n",
            "Training Loss: 0.39575818181037903\n",
            "Training Loss: 0.24410808086395264\n",
            "Training Loss: 0.4777752459049225\n",
            "Training Loss: 0.38518938422203064\n",
            "Training Loss: 0.20564040541648865\n",
            "Training Loss: 0.36257171630859375\n",
            "Training Loss: 0.2108997106552124\n",
            "Training Loss: 0.3403443694114685\n",
            "Training Loss: 0.23918624222278595\n",
            "Training Loss: 0.3556237816810608\n",
            "Training Loss: 0.4102909564971924\n",
            "Training Loss: 0.23068007826805115\n",
            "Training Loss: 0.23310792446136475\n",
            "Training Loss: 0.2020891010761261\n",
            "Training Loss: 0.21959403157234192\n",
            "Training Loss: 0.19988945126533508\n",
            "Training Loss: 0.21785250306129456\n",
            "Training Loss: 0.4248730540275574\n",
            "Training Loss: 0.21407786011695862\n",
            "Training Loss: 0.36640021204948425\n",
            "Training Loss: 0.3611575961112976\n",
            "Training Loss: 0.2225251942873001\n",
            "Training Loss: 0.4189555048942566\n",
            "Training Loss: 0.5091356039047241\n",
            "Training Loss: 0.21367380023002625\n",
            "Training Loss: 0.35648661851882935\n",
            "Training Loss: 0.23197436332702637\n",
            "Training Loss: 0.3644666075706482\n",
            "Training Loss: 0.2417871505022049\n",
            "Training Loss: 0.253964364528656\n",
            "Training Loss: 0.36304163932800293\n",
            "Training Loss: 0.16739580035209656\n",
            "Training Loss: 0.3898254930973053\n",
            "Training Loss: 0.20800814032554626\n",
            "Training Loss: 0.23975427448749542\n",
            "Training Loss: 0.3765231966972351\n",
            "Training Loss: 0.31989237666130066\n",
            "Training Loss: 0.5035344362258911\n",
            "Training Loss: 0.3643670678138733\n",
            "Training Loss: 0.47750407457351685\n",
            "Training Loss: 0.22524745762348175\n",
            "Training Loss: 0.3981756865978241\n",
            "Training Loss: 0.38885390758514404\n",
            "Training Loss: 0.3631429076194763\n",
            "Training Loss: 0.18230578303337097\n",
            "Training Loss: 0.46919959783554077\n",
            "Training Loss: 0.21635565161705017\n",
            "Training Loss: 0.2104051411151886\n",
            "Training Loss: 0.16127097606658936\n",
            "Training Loss: 0.2624337077140808\n",
            "Training Loss: 0.3577585816383362\n",
            "Training Loss: 0.22515051066875458\n",
            "Training Loss: 0.41918596625328064\n",
            "Training Loss: 0.314630925655365\n",
            "Training Loss: 0.4700607657432556\n",
            "Training Loss: 0.22752316296100616\n",
            "Training Loss: 0.4359130859375\n",
            "Training Loss: 0.26828649640083313\n",
            "Training Loss: 0.18887144327163696\n",
            "Training Loss: 0.2228202074766159\n",
            "Training Loss: 0.19459620118141174\n",
            "Training Loss: 0.23053601384162903\n",
            "Training Loss: 0.2340703308582306\n",
            "Training Loss: 0.23597684502601624\n",
            "Training Loss: 0.5468665361404419\n",
            "Training Loss: 0.4560447335243225\n",
            "Training Loss: 0.231485515832901\n",
            "Training Loss: 0.5342363119125366\n",
            "Training Loss: 0.6237941384315491\n",
            "Training Loss: 0.16886378824710846\n",
            "Training Loss: 0.37086760997772217\n",
            "Training Loss: 0.3267408311367035\n",
            "Training Loss: 0.3411700129508972\n",
            "Training Loss: 0.3391522765159607\n",
            "Training Loss: 0.3526157736778259\n",
            "Training Loss: 0.3521359860897064\n",
            "Training Loss: 0.2043425589799881\n",
            "Training Loss: 0.22268684208393097\n",
            "Training Loss: 0.22919699549674988\n",
            "Training Loss: 0.24895629286766052\n",
            "Training Loss: 0.30698245763778687\n",
            "Training Loss: 0.26206034421920776\n",
            "Training Loss: 0.1722402274608612\n",
            "Training Loss: 0.47667282819747925\n",
            "Training Loss: 0.6141309142112732\n",
            "Training Loss: 0.5753031373023987\n",
            "Training Loss: 0.33702802658081055\n",
            "Training Loss: 0.3482256829738617\n",
            "Training Loss: 0.26985296607017517\n",
            "Training Loss: 0.40405115485191345\n",
            "Training Loss: 0.24780826270580292\n",
            "Training Loss: 0.2555510401725769\n",
            "Training Loss: 0.20534369349479675\n",
            "Training Loss: 0.19948959350585938\n",
            "Training Loss: 0.3252595067024231\n",
            "Training Loss: 0.48260074853897095\n",
            "Training Loss: 0.20016738772392273\n",
            "Training Loss: 0.23986610770225525\n",
            "Training Loss: 0.46423596143722534\n",
            "Training Loss: 0.47490406036376953\n",
            "Training Loss: 0.20800049602985382\n",
            "Training Loss: 0.2148422747850418\n",
            "Training Loss: 0.33482447266578674\n",
            "Training Loss: 0.33080607652664185\n",
            "Training Loss: 0.19569212198257446\n",
            "Training Loss: 0.1998116672039032\n",
            "Training Loss: 0.35307103395462036\n",
            "Training Loss: 0.17826297879219055\n",
            "Training Loss: 0.39768701791763306\n",
            "Training Loss: 0.31835800409317017\n",
            "Training Loss: 0.18023881316184998\n",
            "Training Loss: 0.35088804364204407\n",
            "Training Loss: 0.4750167429447174\n",
            "Training Loss: 0.3637433350086212\n",
            "Training Loss: 0.257281094789505\n",
            "Training Loss: 0.3509806990623474\n",
            "Training Loss: 0.23887355625629425\n",
            "Training Loss: 0.43724289536476135\n",
            "Training Loss: 0.24190554022789001\n",
            "Training Loss: 0.18335291743278503\n",
            "Training Loss: 0.2798812985420227\n",
            "Training Loss: 0.3070806860923767\n",
            "Training Loss: 0.5268341898918152\n",
            "Training Loss: 0.23936161398887634\n",
            "Training Loss: 0.33330392837524414\n",
            "Training Loss: 0.39205366373062134\n",
            "Training Loss: 0.3799673914909363\n",
            "Training Loss: 0.2141536921262741\n",
            "Training Loss: 0.3565087616443634\n",
            "Training Loss: 0.5290917754173279\n",
            "Training Loss: 0.20169420540332794\n",
            "Training Loss: 0.31202906370162964\n",
            "Training Loss: 0.22407957911491394\n",
            "Training Loss: 0.30832523107528687\n",
            "Training Loss: 0.1957719773054123\n",
            "Training Loss: 0.405102014541626\n",
            "Training Loss: 0.20276837050914764\n",
            "Training Loss: 0.3478812575340271\n",
            "Training Loss: 0.18470105528831482\n",
            "Training Loss: 0.3364019989967346\n",
            "Training Loss: 0.22261984646320343\n",
            "Training Loss: 0.33634740114212036\n",
            "Training Loss: 0.3498193323612213\n",
            "Training Loss: 0.1902424395084381\n",
            "Training Loss: 0.371802419424057\n",
            "Training Loss: 0.15025246143341064\n",
            "Training Loss: 0.48936596512794495\n",
            "Training Loss: 0.24893227219581604\n",
            "Training Loss: 0.320544958114624\n",
            "Training Loss: 0.34818559885025024\n",
            "Training Loss: 0.3728920817375183\n",
            "Training Loss: 0.18415160477161407\n",
            "Training Loss: 0.3814104199409485\n",
            "Training Loss: 0.36111873388290405\n",
            "Training Loss: 0.22809270024299622\n",
            "Training Loss: 0.18933600187301636\n",
            "Training Loss: 0.2012116014957428\n",
            "Training Loss: 0.24738284945487976\n",
            "Training Loss: 0.1744115650653839\n",
            "Training Loss: 0.19779916107654572\n",
            "Training Loss: 0.37506237626075745\n",
            "Training Loss: 0.21902890503406525\n",
            "Training Loss: 0.1862519234418869\n",
            "Training Loss: 0.2205464094877243\n",
            "Training Loss: 0.24928946793079376\n",
            "Training Loss: 0.5041097402572632\n",
            "Training Loss: 0.3680632412433624\n",
            "Training Loss: 0.6435976028442383\n",
            "Training Loss: 0.23452897369861603\n",
            "Training Loss: 0.18128472566604614\n",
            "Training Loss: 0.21130815148353577\n",
            "Training Loss: 0.18424496054649353\n",
            "Training Loss: 0.14946766197681427\n",
            "Training Loss: 0.22742049396038055\n",
            "Training Loss: 0.3816073536872864\n",
            "Training Loss: 0.3702966272830963\n",
            "Training Loss: 0.3379737436771393\n",
            "Training Loss: 0.3751046061515808\n",
            "Training Loss: 0.23956671357154846\n",
            "Training Loss: 0.49241581559181213\n",
            "Training Loss: 0.3527471721172333\n",
            "Training Loss: 0.21062779426574707\n",
            "Training Loss: 0.13481837511062622\n",
            "Training Loss: 0.3718697428703308\n",
            "Training Loss: 0.37015438079833984\n",
            "Training Loss: 0.49505382776260376\n",
            "Training Loss: 0.20718154311180115\n",
            "Training Loss: 0.4738251566886902\n",
            "Training Loss: 0.35593074560165405\n",
            "Training Loss: 0.45589494705200195\n",
            "Training Loss: 0.2079245150089264\n",
            "Training Loss: 0.23368997871875763\n",
            "Training Loss: 0.22615616023540497\n",
            "Training Loss: 0.18759557604789734\n",
            "Training Loss: 0.47084569931030273\n",
            "Training Loss: 0.45631083846092224\n",
            "Training Loss: 0.33163416385650635\n",
            "Training Loss: 0.23078326880931854\n",
            "Training Loss: 0.4869305491447449\n",
            "Training Loss: 0.17732800543308258\n",
            "Training Loss: 0.20214667916297913\n",
            "Training Loss: 0.20175138115882874\n",
            "Training Loss: 0.31883955001831055\n",
            "Training Loss: 0.23280660808086395\n",
            "Training Loss: 0.22571544349193573\n",
            "Training Loss: 0.2070857733488083\n",
            "Training Loss: 0.3663587272167206\n",
            "Training Loss: 0.18284355103969574\n",
            "Training Loss: 0.3589652180671692\n",
            "Training Loss: 0.4806607663631439\n",
            "Training Loss: 0.20850133895874023\n",
            "Training Loss: 0.3569095730781555\n",
            "Training Loss: 0.3422895073890686\n",
            "Training Loss: 0.19545352458953857\n",
            "Training Loss: 0.16801020503044128\n",
            "Training Loss: 0.2184310257434845\n",
            "Training Loss: 0.19618937373161316\n",
            "Training Loss: 0.1993398815393448\n",
            "Training Loss: 0.22542643547058105\n",
            "Training Loss: 0.3963225483894348\n",
            "Training Loss: 0.47425204515457153\n",
            "Training Loss: 0.21760089695453644\n",
            "Training Loss: 0.48926448822021484\n",
            "Training Loss: 0.3369041383266449\n",
            "Training Loss: 0.31926894187927246\n",
            "Training Loss: 0.358413428068161\n",
            "Training Loss: 0.47195640206336975\n",
            "Training Loss: 0.2105897217988968\n",
            "Training Loss: 0.3253055214881897\n",
            "Training Loss: 0.3176165521144867\n",
            "Training Loss: 0.12514999508857727\n",
            "Training Loss: 0.33347415924072266\n",
            "Training Loss: 0.21045105159282684\n",
            "Training Loss: 0.2208804339170456\n",
            "Training Loss: 0.53355473279953\n",
            "Training Loss: 0.20837418735027313\n",
            "Training Loss: 0.19763723015785217\n",
            "Training Loss: 0.32307273149490356\n",
            "Training Loss: 0.33080071210861206\n",
            "Training Loss: 0.20873188972473145\n",
            "Training Loss: 0.18860049545764923\n",
            "Training Loss: 0.17561271786689758\n",
            "Training Loss: 0.35428375005722046\n",
            "Training Loss: 0.3890768885612488\n",
            "Training Loss: 0.35667750239372253\n",
            "Training Loss: 0.18119770288467407\n",
            "Training Loss: 0.34223222732543945\n",
            "Training Loss: 0.19503463804721832\n",
            "Training Loss: 0.20714184641838074\n",
            "Training Loss: 0.33653706312179565\n",
            "Training Loss: 0.19000355899333954\n",
            "Training Loss: 0.19300231337547302\n",
            "Training Loss: 0.3356977701187134\n",
            "Training Loss: 0.49652808904647827\n",
            "Training Loss: 0.34306851029396057\n",
            "Training Loss: 0.23886200785636902\n",
            "Training Loss: 0.1902592033147812\n",
            "Training Loss: 0.3534262776374817\n",
            "Training Loss: 0.47291845083236694\n",
            "Training Loss: 0.18455515801906586\n",
            "Training Loss: 0.3361068367958069\n",
            "Training Loss: 0.20975005626678467\n",
            "Training Loss: 0.2038956582546234\n",
            "Training Loss: 0.32162389159202576\n",
            "Training Loss: 0.18580970168113708\n",
            "Training Loss: 0.1695723682641983\n",
            "Training Loss: 0.3149401843547821\n",
            "Training Loss: 0.336112916469574\n",
            "Training Loss: 0.1818397343158722\n",
            "Training Loss: 0.15835587680339813\n",
            "Training Loss: 0.6557024717330933\n",
            "Training Loss: 0.4921252727508545\n",
            "Training Loss: 0.35254985094070435\n",
            "Training Loss: 0.31614696979522705\n",
            "Training Loss: 0.1696341186761856\n",
            "Training Loss: 0.18757064640522003\n",
            "Training Loss: 0.3518042266368866\n",
            "Training Loss: 0.13432778418064117\n",
            "Training Loss: 0.34920424222946167\n",
            "Training Loss: 0.20860309898853302\n",
            "Training Loss: 0.35911256074905396\n",
            "Training Loss: 0.34764620661735535\n",
            "Training Loss: 0.2690271735191345\n",
            "Training Loss: 0.16486068069934845\n",
            "Training Loss: 0.16870014369487762\n",
            "Training Loss: 0.3266608417034149\n",
            "Training Loss: 0.5063072443008423\n",
            "Training Loss: 0.3860412538051605\n",
            "Training Loss: 0.2093939483165741\n",
            "Training Loss: 0.332819402217865\n",
            "Training Loss: 0.20223744213581085\n",
            "Training Loss: 0.3624524772167206\n",
            "Training Loss: 0.22310838103294373\n",
            "Training Loss: 0.20017841458320618\n",
            "Training Loss: 0.32455572485923767\n",
            "Training Loss: 0.17830920219421387\n",
            "Training Loss: 0.48489731550216675\n",
            "Training Loss: 0.19033043086528778\n",
            "Training Loss: 0.16831593215465546\n",
            "Training Loss: 0.38677236437797546\n",
            "Training Loss: 0.17455026507377625\n",
            "Training Loss: 0.32534059882164\n",
            "Training Loss: 0.34669485688209534\n",
            "Training Loss: 0.22169458866119385\n",
            "Training Loss: 0.32108235359191895\n",
            "Training Loss: 0.3426257073879242\n",
            "Training Loss: 0.44603481888771057\n",
            "Training Loss: 0.4829235076904297\n",
            "Training Loss: 0.18523328006267548\n",
            "Training Loss: 0.18035700917243958\n",
            "Training Loss: 0.1786188930273056\n",
            "Training Loss: 0.19410184025764465\n",
            "Training Loss: 0.1845410168170929\n",
            "Training Loss: 0.5073539614677429\n",
            "Training Loss: 0.18034102022647858\n",
            "Training Loss: 0.48732978105545044\n",
            "Training Loss: 0.4992617666721344\n",
            "Training Loss: 0.19767706096172333\n",
            "Training Loss: 0.20735566318035126\n",
            "Training Loss: 0.1740991771221161\n",
            "Training Loss: 0.19616028666496277\n",
            "Training Loss: 0.33634835481643677\n",
            "Training Loss: 0.18634341657161713\n",
            "Training Loss: 0.2141953408718109\n",
            "Training Loss: 0.16969726979732513\n",
            "Training Loss: 0.3152446448802948\n",
            "Training Loss: 0.1986916959285736\n",
            "Training Loss: 0.31192582845687866\n",
            "Training Loss: 0.3349805474281311\n",
            "Training Loss: 0.6551091074943542\n",
            "Training Loss: 0.1586158722639084\n",
            "Training Loss: 0.49011874198913574\n",
            "Training Loss: 0.18107591569423676\n",
            "Training Loss: 0.1848704069852829\n",
            "Training Loss: 0.20042863488197327\n",
            "Training Loss: 0.3301800787448883\n",
            "Training Loss: 0.13667871057987213\n",
            "Training Loss: 0.3718841075897217\n",
            "Training Loss: 0.3485448360443115\n",
            "Training Loss: 0.359405517578125\n",
            "Training Loss: 0.3280024528503418\n",
            "Training Loss: 0.18083682656288147\n",
            "Training Loss: 0.31482863426208496\n",
            "Training Loss: 0.3064989447593689\n",
            "Training Loss: 0.47615742683410645\n",
            "Training Loss: 0.18393972516059875\n",
            "Training Loss: 0.3260122239589691\n",
            "Training Loss: 0.32049864530563354\n",
            "Training Loss: 0.1746157705783844\n",
            "Training Loss: 0.3611227571964264\n",
            "Training Loss: 0.34974101185798645\n",
            "Training Loss: 0.189323291182518\n",
            "Training Loss: 0.3482694923877716\n",
            "Training Loss: 0.3179927468299866\n",
            "Training Loss: 0.20195868611335754\n",
            "Training Loss: 0.17267942428588867\n",
            "Training Loss: 0.14565509557724\n",
            "Training Loss: 0.34733152389526367\n",
            "Training Loss: 0.48438701033592224\n",
            "Training Loss: 0.1515711545944214\n",
            "Training Loss: 0.18014296889305115\n",
            "Training Loss: 0.33344292640686035\n",
            "Training Loss: 0.1997443437576294\n",
            "Training Loss: 0.2020384520292282\n",
            "Training Loss: 0.1801685392856598\n",
            "Training Loss: 0.18002387881278992\n",
            "Training Loss: 0.16483430564403534\n",
            "Training Loss: 0.49580955505371094\n",
            "Training Loss: 0.15298768877983093\n",
            "Training Loss: 0.1952522099018097\n",
            "Training Loss: 0.35077765583992004\n",
            "Training Loss: 0.3289077579975128\n",
            "Training Loss: 0.4840511381626129\n",
            "Training Loss: 0.18159706890583038\n",
            "Training Loss: 0.19090121984481812\n",
            "Training Loss: 0.6651581525802612\n",
            "Training Loss: 0.16337399184703827\n",
            "Training Loss: 0.1613929569721222\n",
            "Training Loss: 0.12563525140285492\n",
            "Training Loss: 0.17994971573352814\n",
            "Training Loss: 0.4868059754371643\n",
            "Training Loss: 0.21168012917041779\n",
            "Training Loss: 0.4982597231864929\n",
            "Training Loss: 0.3145069181919098\n",
            "Training Loss: 0.3406325876712799\n",
            "Training Loss: 0.19072508811950684\n",
            "Training Loss: 0.18006691336631775\n",
            "Training Loss: 0.31675010919570923\n",
            "Training Loss: 0.3418135344982147\n",
            "Training Loss: 0.17356668412685394\n",
            "Training Loss: 0.1568707525730133\n",
            "Training Loss: 0.3411979675292969\n",
            "Training Loss: 0.4865110516548157\n",
            "Training Loss: 0.2170518934726715\n",
            "Training Loss: 0.3325398564338684\n",
            "Training Loss: 0.17365728318691254\n",
            "Training Loss: 0.17890317738056183\n",
            "Training Loss: 0.15511217713356018\n",
            "Training Loss: 0.14967945218086243\n",
            "Training Loss: 0.17724508047103882\n",
            "Training Loss: 0.3442794978618622\n",
            "Training Loss: 0.13609787821769714\n",
            "Training Loss: 0.19818677008152008\n",
            "Training Loss: 0.33311039209365845\n",
            "Training Loss: 0.33144864439964294\n",
            "Training Loss: 0.16423092782497406\n",
            "Training Loss: 0.3467709720134735\n",
            "Training Loss: 0.1844787895679474\n",
            "Training Loss: 0.4935382902622223\n",
            "Training Loss: 0.626961350440979\n",
            "Training Loss: 0.1918638050556183\n",
            "Training Loss: 0.3308861255645752\n",
            "Training Loss: 0.29405656456947327\n",
            "Training Loss: 0.48626041412353516\n",
            "Training Loss: 0.3566579222679138\n",
            "Training Loss: 0.161880761384964\n",
            "Training Loss: 0.1871212273836136\n",
            "Training Loss: 0.5103948712348938\n",
            "Training Loss: 0.29873010516166687\n",
            "Training Loss: 0.175184965133667\n",
            "Training Loss: 0.4881463646888733\n",
            "Training Loss: 0.3466738760471344\n",
            "Training Loss: 0.32753926515579224\n",
            "Training Loss: 0.15571677684783936\n",
            "Training Loss: 0.2982098460197449\n",
            "Training Loss: 0.4625675678253174\n",
            "Training Loss: 0.20980267226696014\n",
            "Training Loss: 0.34973347187042236\n",
            "Training Loss: 0.15162380039691925\n",
            "Training Loss: 0.1821144074201584\n",
            "Training Loss: 0.2919246256351471\n",
            "Training Loss: 0.139962300658226\n",
            "Training Loss: 0.4927060604095459\n",
            "Training Loss: 0.4923945367336273\n",
            "Training Loss: 0.32092756032943726\n",
            "Training Loss: 0.46954289078712463\n",
            "Training Loss: 0.34482455253601074\n",
            "Training Loss: 0.3104894757270813\n",
            "Training Loss: 0.465512216091156\n",
            "Training Loss: 0.46491581201553345\n",
            "Training Loss: 0.19675195217132568\n",
            "Training Loss: 0.32069435715675354\n",
            "Training Loss: 0.3473415672779083\n",
            "Training Loss: 0.2008363902568817\n",
            "Training Loss: 0.1776847094297409\n",
            "Training Loss: 0.16441503167152405\n",
            "Training Loss: 0.34474366903305054\n",
            "Training Loss: 0.4866895079612732\n",
            "Training Loss: 0.17685146629810333\n",
            "Training Loss: 0.14069941639900208\n",
            "Training Loss: 0.335470974445343\n",
            "Training Loss: 0.3518126606941223\n",
            "Training Loss: 0.34308481216430664\n",
            "Training Loss: 0.18105359375476837\n",
            "Training Loss: 0.3393194377422333\n",
            "Training Loss: 0.3214492201805115\n",
            "Training Loss: 0.16127550601959229\n",
            "Training Loss: 0.46746987104415894\n",
            "Training Loss: 0.29901692271232605\n",
            "Training Loss: 0.16921278834342957\n",
            "Training Loss: 0.3037866950035095\n",
            "Training Loss: 0.3474372923374176\n",
            "Training Loss: 0.1952163577079773\n",
            "Training Loss: 0.3203316330909729\n",
            "Training Loss: 0.16844816505908966\n",
            "Training Loss: 0.16272275149822235\n",
            "Training Loss: 0.1744602769613266\n",
            "Training Loss: 0.17502637207508087\n",
            "Training Loss: 0.2879330813884735\n",
            "Training Loss: 0.47547370195388794\n",
            "Training Loss: 0.3124140202999115\n",
            "Training Loss: 0.47421330213546753\n",
            "Training Loss: 0.3196151554584503\n",
            "Training Loss: 0.30487972497940063\n",
            "Training Loss: 0.3123357594013214\n",
            "Training Loss: 0.3081011474132538\n",
            "Training Loss: 0.45515957474708557\n",
            "Training Loss: 0.45130428671836853\n",
            "Training Loss: 0.30553609132766724\n",
            "Training Loss: 0.31598925590515137\n",
            "Training Loss: 0.3261548578739166\n",
            "Training Loss: 0.3225058317184448\n",
            "Training Loss: 0.20567914843559265\n",
            "Training Loss: 0.1413022130727768\n",
            "Training Loss: 0.19738999009132385\n",
            "Training Loss: 0.29817256331443787\n",
            "Training Loss: 0.15417596697807312\n",
            "Training Loss: 0.7475491762161255\n",
            "Training Loss: 0.31267350912094116\n",
            "Training Loss: 0.32349926233291626\n",
            "Training Loss: 0.34605851769447327\n",
            "Training Loss: 0.28763365745544434\n",
            "Training Loss: 0.1650354415178299\n",
            "Training Loss: 0.32056865096092224\n",
            "Training Loss: 0.318577378988266\n",
            "Training Loss: 0.15899324417114258\n",
            "Training Loss: 0.18889787793159485\n",
            "Training Loss: 0.16870775818824768\n",
            "Training Loss: 0.3023006319999695\n",
            "Training Loss: 0.16844502091407776\n",
            "Training Loss: 0.16331413388252258\n",
            "Training Loss: 0.18041640520095825\n",
            "Training Loss: 0.20202398300170898\n",
            "Training Loss: 0.17654447257518768\n",
            "Training Loss: 0.3332669734954834\n",
            "Training Loss: 0.320477157831192\n",
            "Training Loss: 0.48537731170654297\n",
            "Training Loss: 0.4731108248233795\n",
            "Training Loss: 0.19156602025032043\n",
            "Training Loss: 0.16767098009586334\n",
            "Training Loss: 0.32147055864334106\n",
            "Training Loss: 0.18865913152694702\n",
            "Training Loss: 0.1578066647052765\n",
            "Training Loss: 0.6419360041618347\n",
            "Training Loss: 0.49167099595069885\n",
            "Training Loss: 0.3110921084880829\n",
            "Training Loss: 0.3536851108074188\n",
            "Training Loss: 0.19488194584846497\n",
            "Training Loss: 0.35729312896728516\n",
            "Training Loss: 0.17552438378334045\n",
            "Training Loss: 0.14915437996387482\n",
            "Training Loss: 0.28025010228157043\n",
            "Training Loss: 0.153150737285614\n",
            "Training Loss: 0.6121164560317993\n",
            "Training Loss: 0.203611820936203\n",
            "Training Loss: 0.17284521460533142\n",
            "Training Loss: 0.18310979008674622\n",
            "Training Loss: 0.3640449643135071\n",
            "Training Loss: 0.1752282679080963\n",
            "Training Loss: 0.16286590695381165\n",
            "Training Loss: 0.3097778260707855\n",
            "Training Loss: 0.5048579573631287\n",
            "Training Loss: 0.4878292679786682\n",
            "Training Loss: 0.4882277548313141\n",
            "Training Loss: 0.3241974413394928\n",
            "Training Loss: 0.3346227705478668\n",
            "Training Loss: 0.1683642864227295\n",
            "Training Loss: 0.4492267966270447\n",
            "Training Loss: 0.3561495244503021\n",
            "Training Loss: 0.33141547441482544\n",
            "Training Loss: 0.32525619864463806\n",
            "Training Loss: 0.16545309126377106\n",
            "Training Loss: 0.19186465442180634\n",
            "Training Loss: 0.15249282121658325\n",
            "Training Loss: 0.3227892220020294\n",
            "Training Loss: 0.48211079835891724\n",
            "Training Loss: 0.1704380363225937\n",
            "Training Loss: 0.17366458475589752\n",
            "Training Loss: 0.193879634141922\n",
            "Training Loss: 0.3100318908691406\n",
            "Training Loss: 0.2852991819381714\n",
            "Training Loss: 0.13691624999046326\n",
            "Training Loss: 0.1820542961359024\n",
            "Training Loss: 0.3249524235725403\n",
            "Training Loss: 0.1629292070865631\n",
            "Training Loss: 0.3172057569026947\n",
            "Training Loss: 0.16532252728939056\n",
            "Training Loss: 0.16544891893863678\n",
            "Training Loss: 0.325910747051239\n",
            "Training Loss: 0.45237261056900024\n",
            "Training Loss: 0.15835393965244293\n",
            "Training Loss: 0.35251548886299133\n",
            "Training Loss: 0.32877203822135925\n",
            "Training Loss: 0.17278876900672913\n",
            "Training Loss: 0.15256622433662415\n",
            "Training Loss: 0.15283244848251343\n",
            "Training Loss: 0.13142013549804688\n",
            "Training Loss: 0.1673518717288971\n",
            "Training Loss: 0.16235807538032532\n",
            "Training Loss: 0.32205119729042053\n",
            "Training Loss: 0.13350822031497955\n",
            "Training Loss: 0.315899133682251\n",
            "Training Loss: 0.18292614817619324\n",
            "Training Loss: 0.30485671758651733\n",
            "Training Loss: 0.1596342772245407\n",
            "Training Loss: 0.44523364305496216\n",
            "Training Loss: 0.3286801278591156\n",
            "Training Loss: 0.15141765773296356\n",
            "Training Loss: 0.1407821774482727\n",
            "Training Loss: 0.17558901011943817\n",
            "Training Loss: 0.32084983587265015\n",
            "Training Loss: 0.16253231465816498\n",
            "Training Loss: 0.321895569562912\n",
            "Training Loss: 0.32664546370506287\n",
            "Training Loss: 0.3114437758922577\n",
            "Training Loss: 0.3252917230129242\n",
            "Training Loss: 0.13687458634376526\n",
            "Training Loss: 0.15952180325984955\n",
            "Training Loss: 0.1355513632297516\n",
            "Training Loss: 0.49298423528671265\n",
            "Training Loss: 0.313571035861969\n",
            "Training Loss: 0.3018417954444885\n",
            "Training Loss: 0.6313667297363281\n",
            "Training Loss: 0.3281376361846924\n",
            "Training Loss: 0.29776841402053833\n",
            "Training Loss: 0.15683197975158691\n",
            "Training Loss: 0.3293466567993164\n",
            "Training Loss: 0.1585572063922882\n",
            "Training Loss: 0.30987676978111267\n",
            "Training Loss: 0.3450961112976074\n",
            "Training Loss: 0.17656570672988892\n",
            "Training Loss: 0.33627474308013916\n",
            "Training Loss: 0.46483463048934937\n",
            "Training Loss: 0.14184972643852234\n",
            "Training Loss: 0.2858605682849884\n",
            "Training Loss: 0.13227705657482147\n",
            "Training Loss: 0.15671436488628387\n",
            "Training Loss: 0.2915119528770447\n",
            "Training Loss: 0.3487241566181183\n",
            "Training Loss: 0.34090253710746765\n",
            "Training Loss: 0.2100868672132492\n",
            "Training Loss: 0.14897756278514862\n",
            "Training Loss: 0.1700841635465622\n",
            "Training Loss: 0.15804754197597504\n",
            "Training Loss: 0.177191823720932\n",
            "Training Loss: 0.1667175590991974\n",
            "Training Loss: 0.1627543568611145\n",
            "Training Loss: 0.3229818344116211\n",
            "Training Loss: 0.30927082896232605\n",
            "Training Loss: 0.31152480840682983\n",
            "Training Loss: 0.6385583877563477\n",
            "Training Loss: 0.3318512439727783\n",
            "Training Loss: 0.1529727280139923\n",
            "Training Loss: 0.14509092271327972\n",
            "Training Loss: 0.2826221287250519\n",
            "Training Loss: 0.14096486568450928\n",
            "Training Loss: 0.15382884442806244\n",
            "Training Loss: 0.3215181231498718\n",
            "Training Loss: 0.2784087359905243\n",
            "Training Loss: 0.15577517449855804\n",
            "Training Loss: 0.18086257576942444\n",
            "Training Loss: 0.11888062953948975\n",
            "Training Loss: 0.18114833533763885\n",
            "Training Loss: 0.3507848381996155\n",
            "Training Loss: 0.15577712655067444\n",
            "Training Loss: 0.33050400018692017\n",
            "Training Loss: 0.16394612193107605\n",
            "Training Loss: 0.1331237256526947\n",
            "Training Loss: 0.3231199383735657\n",
            "Training Loss: 0.48453760147094727\n",
            "Training Loss: 0.14700473845005035\n",
            "Training Loss: 0.31019237637519836\n",
            "Training Loss: 0.31187838315963745\n",
            "Training Loss: 0.13041964173316956\n",
            "Training Loss: 0.10512964427471161\n",
            "Training Loss: 0.15199656784534454\n",
            "Training Loss: 0.17645524442195892\n",
            "Training Loss: 0.12368736416101456\n",
            "Training Loss: 0.3542214334011078\n",
            "Training Loss: 0.305866003036499\n",
            "Training Loss: 0.3045221269130707\n",
            "Training Loss: 0.4961822032928467\n",
            "Training Loss: 0.3149788975715637\n",
            "Training Loss: 0.1502181589603424\n",
            "Training Loss: 0.48101192712783813\n",
            "Training Loss: 0.3234308063983917\n",
            "Training Loss: 0.31271275877952576\n",
            "Training Loss: 0.3258865475654602\n",
            "Training Loss: 0.3075363039970398\n",
            "Training Loss: 0.29902949929237366\n",
            "Training Loss: 0.3126180171966553\n",
            "Training Loss: 0.16396409273147583\n",
            "Training Loss: 0.3192017376422882\n",
            "Training Loss: 0.1409132182598114\n",
            "Training Loss: 0.12689556181430817\n",
            "Training Loss: 0.2854349613189697\n",
            "Training Loss: 0.32285207509994507\n",
            "Training Loss: 0.2926216423511505\n",
            "Training Loss: 0.47188234329223633\n",
            "Training Loss: 0.29185664653778076\n",
            "Training Loss: 0.305001825094223\n",
            "Training Loss: 0.2889173626899719\n",
            "Training Loss: 0.161539226770401\n",
            "Training Loss: 0.12181468307971954\n",
            "Training Loss: 0.21672005951404572\n",
            "Training Loss: 0.13826292753219604\n",
            "Training Loss: 0.17372778058052063\n",
            "Training Loss: 0.1289280354976654\n",
            "Training Loss: 0.17573226988315582\n",
            "Training Loss: 0.14617736637592316\n",
            "Training Loss: 0.14770463109016418\n",
            "Training Loss: 0.18121488392353058\n",
            "Training Loss: 0.3335949778556824\n",
            "Training Loss: 0.4688993990421295\n",
            "Training Loss: 0.14856299757957458\n",
            "Training Loss: 0.36787259578704834\n",
            "Training Loss: 0.14964531362056732\n",
            "Training Loss: 0.1316109299659729\n",
            "Training Loss: 0.1344575732946396\n",
            "Training Loss: 0.30586081743240356\n",
            "Training Loss: 0.15336313843727112\n",
            "Training Loss: 0.4754510819911957\n",
            "Training Loss: 0.139616459608078\n",
            "Training Loss: 0.3253762423992157\n",
            "Training Loss: 0.13978630304336548\n",
            "Training Loss: 0.4849581718444824\n",
            "Training Loss: 0.14241322875022888\n",
            "Training Loss: 0.16992391645908356\n",
            "Training Loss: 0.3211018741130829\n",
            "Training Loss: 0.1406622678041458\n",
            "Training Loss: 0.4825187623500824\n",
            "Training Loss: 0.2864702045917511\n",
            "Training Loss: 0.1649247109889984\n",
            "Training Loss: 0.15585288405418396\n",
            "Training Loss: 0.327025443315506\n",
            "Training Loss: 0.283603310585022\n",
            "Training Loss: 0.12845447659492493\n",
            "Training Loss: 0.30202600359916687\n",
            "Training Loss: 0.16320866346359253\n",
            "Training Loss: 0.1354837268590927\n",
            "Training Loss: 0.1463599056005478\n",
            "Training Loss: 0.1358983814716339\n",
            "Training Loss: 0.49194803833961487\n",
            "Training Loss: 0.13637056946754456\n",
            "Training Loss: 0.141621932387352\n",
            "Training Loss: 0.2996615469455719\n",
            "Training Loss: 0.14379268884658813\n",
            "Training Loss: 0.15780654549598694\n",
            "Training Loss: 0.30399709939956665\n",
            "Training Loss: 0.11642546951770782\n",
            "Training Loss: 0.31434303522109985\n",
            "Training Loss: 0.31253373622894287\n",
            "Training Loss: 0.1317259818315506\n",
            "Training Loss: 0.14454470574855804\n",
            "Training Loss: 0.49618053436279297\n",
            "Training Loss: 0.3281593918800354\n",
            "Training Loss: 0.14310860633850098\n",
            "Training Loss: 0.29807227849960327\n",
            "Training Loss: 0.1626185029745102\n",
            "Training Loss: 0.314524382352829\n",
            "Training Loss: 0.4667207598686218\n",
            "Training Loss: 0.29701846837997437\n",
            "Training Loss: 0.10030756145715714\n",
            "Training Loss: 0.3193776607513428\n",
            "Training Loss: 0.2929290533065796\n",
            "Training Loss: 0.4695133566856384\n",
            "Training Loss: 0.4660571217536926\n",
            "Training Loss: 0.4709182679653168\n",
            "Training Loss: 0.4804140627384186\n",
            "Training Loss: 0.13492612540721893\n",
            "Training Loss: 0.43890342116355896\n",
            "Training Loss: 0.16873790323734283\n",
            "Training Loss: 0.3423902094364166\n",
            "Training Loss: 0.33126693964004517\n",
            "Training Loss: 0.15336598455905914\n",
            "Training Loss: 0.3207055330276489\n",
            "Training Loss: 0.19303275644779205\n",
            "Training Loss: 0.1674068421125412\n",
            "Training Loss: 0.3056778609752655\n",
            "Training Loss: 0.31945639848709106\n",
            "Training Loss: 0.4779632091522217\n",
            "Training Loss: 0.14189830422401428\n",
            "Training Loss: 0.3536004424095154\n",
            "Training Loss: 0.2906726598739624\n",
            "Training Loss: 0.4719272553920746\n",
            "Training Loss: 0.42074981331825256\n",
            "Training Loss: 0.18220379948616028\n",
            "Training Loss: 0.2947891354560852\n",
            "Training Loss: 0.1633104383945465\n",
            "Training Loss: 0.3482751250267029\n",
            "Training Loss: 0.14781633019447327\n",
            "Training Loss: 0.2843683958053589\n",
            "Training Loss: 0.4890348017215729\n",
            "Training Loss: 0.31511151790618896\n",
            "Training Loss: 0.1718299835920334\n",
            "Training Loss: 0.4575462341308594\n",
            "Training Loss: 0.13691584765911102\n",
            "Training Loss: 0.15085594356060028\n",
            "Training Loss: 0.3019278645515442\n",
            "Training Loss: 0.2606534957885742\n",
            "Training Loss: 0.12915576994419098\n",
            "Training Loss: 0.4565427303314209\n",
            "Training Loss: 0.29477259516716003\n",
            "Training Loss: 0.32345181703567505\n",
            "Training Loss: 0.3300451338291168\n",
            "Training Loss: 0.3198568522930145\n",
            "Training Loss: 0.31947755813598633\n",
            "Training Loss: 0.1863257884979248\n",
            "Training Loss: 0.3097238838672638\n",
            "Training Loss: 0.13792584836483002\n",
            "Training Loss: 0.1737395077943802\n",
            "Training Loss: 0.29577744007110596\n",
            "Training Loss: 0.3283690810203552\n",
            "Training Loss: 0.1908227503299713\n",
            "Training Loss: 0.30242544412612915\n",
            "Training Loss: 0.2942858338356018\n",
            "Training Loss: 0.15304845571517944\n",
            "Training Loss: 0.161697655916214\n",
            "Training Loss: 0.31923797726631165\n",
            "Training Loss: 0.32104989886283875\n",
            "Training Loss: 0.45694416761398315\n",
            "Training Loss: 0.31353142857551575\n",
            "Training Loss: 0.14977677166461945\n",
            "Training Loss: 0.1487007439136505\n",
            "Training Loss: 0.1262778341770172\n",
            "Training Loss: 0.15974529087543488\n",
            "Training Loss: 0.15254972875118256\n",
            "Training Loss: 0.13675843179225922\n",
            "Training Loss: 0.28467026352882385\n",
            "Training Loss: 0.3004445433616638\n",
            "Training Loss: 0.3138737678527832\n",
            "Training Loss: 0.15018413960933685\n",
            "Training Loss: 0.4711504876613617\n",
            "Training Loss: 0.28721100091934204\n",
            "Training Loss: 0.15442240238189697\n",
            "Training Loss: 0.28667181730270386\n",
            "Training Loss: 0.12478828430175781\n",
            "Training Loss: 0.1719520390033722\n",
            "Training Loss: 0.34460359811782837\n",
            "Training Loss: 0.31659287214279175\n",
            "Training Loss: 0.283551961183548\n",
            "Training Loss: 0.12629368901252747\n",
            "Training Loss: 0.48523736000061035\n",
            "Training Loss: 0.29542607069015503\n",
            "Training Loss: 0.1356443613767624\n",
            "Training Loss: 0.309552937746048\n",
            "Training Loss: 0.31532156467437744\n",
            "Training Loss: 0.307415246963501\n",
            "Training Loss: 0.16702178120613098\n",
            "Training Loss: 0.1602376401424408\n",
            "Training Loss: 0.4588702619075775\n",
            "Training Loss: 0.28254541754722595\n",
            "Training Loss: 0.13743478059768677\n",
            "Training Loss: 0.3409360945224762\n",
            "Training Loss: 0.29914453625679016\n",
            "Training Loss: 0.12243707478046417\n",
            "Training Loss: 0.1677805781364441\n",
            "Training Loss: 0.325684130191803\n",
            "Training Loss: 0.12946905195713043\n",
            "Training Loss: 0.459739625453949\n",
            "Training Loss: 0.4682233929634094\n",
            "Training Loss: 0.1640636920928955\n",
            "Training Loss: 0.11703252792358398\n",
            "Training Loss: 0.15609994530677795\n",
            "Training Loss: 0.304415762424469\n",
            "Training Loss: 0.14204402267932892\n",
            "Training Loss: 0.32677456736564636\n",
            "Training Loss: 0.2838752269744873\n",
            "Training Loss: 0.35355955362319946\n",
            "Training Loss: 0.3267771303653717\n",
            "Training Loss: 0.32455623149871826\n",
            "Training Loss: 0.14866185188293457\n",
            "Training Loss: 0.46961888670921326\n",
            "Training Loss: 0.4810810089111328\n",
            "Training Loss: 0.11762438714504242\n",
            "Training Loss: 0.11628220230340958\n",
            "Training Loss: 0.12789711356163025\n",
            "Training Loss: 0.13555561006069183\n",
            "Training Loss: 0.12663212418556213\n",
            "Training Loss: 0.4925772249698639\n",
            "Training Loss: 0.46051615476608276\n",
            "Training Loss: 0.1462836116552353\n",
            "Training Loss: 0.29587996006011963\n",
            "Training Loss: 0.4822293221950531\n",
            "Training Loss: 0.1367243230342865\n",
            "Training Loss: 0.29505521059036255\n",
            "Training Loss: 0.29604193568229675\n",
            "Training Loss: 0.16621394455432892\n",
            "Training Loss: 0.16942623257637024\n",
            "Training Loss: 0.4535180926322937\n",
            "Training Loss: 0.14643800258636475\n",
            "Training Loss: 0.3125927746295929\n",
            "Training Loss: 0.3191958963871002\n",
            "Training Loss: 0.13253210484981537\n",
            "Training Loss: 0.33193540573120117\n",
            "Training Loss: 0.16434985399246216\n",
            "Training Loss: 0.13566796481609344\n",
            "Training Loss: 0.6353856921195984\n",
            "Training Loss: 0.4654563069343567\n",
            "Training Loss: 0.30892276763916016\n",
            "Training Loss: 0.30157309770584106\n",
            "Training Loss: 0.1509583741426468\n",
            "Training Loss: 0.14556285738945007\n",
            "Training Loss: 0.14061637222766876\n",
            "Training Loss: 0.13829585909843445\n",
            "Training Loss: 0.10344822704792023\n",
            "Training Loss: 0.27269265055656433\n",
            "Training Loss: 0.2801451086997986\n",
            "Training Loss: 0.15403608977794647\n",
            "Training Loss: 0.16745416820049286\n",
            "Training Loss: 0.46757739782333374\n",
            "Training Loss: 0.13977506756782532\n",
            "Training Loss: 0.2892325222492218\n",
            "Training Loss: 0.13612259924411774\n",
            "Training Loss: 0.2730990946292877\n",
            "Training Loss: 0.11751073598861694\n",
            "Training Loss: 0.33875173330307007\n",
            "Training Loss: 0.31249189376831055\n",
            "Training Loss: 0.2750925123691559\n",
            "Training Loss: 0.117994524538517\n",
            "Training Loss: 0.15557432174682617\n",
            "Training Loss: 0.32159900665283203\n",
            "Training Loss: 0.3167459964752197\n",
            "Training Loss: 0.30245059728622437\n",
            "Training Loss: 0.12186294794082642\n",
            "Training Loss: 0.48367753624916077\n",
            "Training Loss: 0.17584720253944397\n",
            "Training Loss: 0.13891743123531342\n",
            "Training Loss: 0.1104753166437149\n",
            "Training Loss: 0.2826518714427948\n",
            "Training Loss: 0.3200856149196625\n",
            "Training Loss: 0.4657432436943054\n",
            "Training Loss: 0.1980302631855011\n",
            "Training Loss: 0.14721275866031647\n",
            "Training Loss: 0.28100576996803284\n",
            "Training Loss: 0.1365666538476944\n",
            "Training Loss: 0.12668141722679138\n",
            "Training Loss: 0.30748748779296875\n",
            "Training Loss: 0.1277310699224472\n",
            "Training Loss: 0.6309041380882263\n",
            "Training Loss: 0.12676939368247986\n",
            "Training Loss: 0.16445767879486084\n",
            "Training Loss: 0.18176265060901642\n",
            "Training Loss: 0.15998493134975433\n",
            "Training Loss: 0.10996679216623306\n",
            "Training Loss: 0.1301693320274353\n",
            "Training Loss: 0.32529041171073914\n",
            "Training Loss: 0.13829079270362854\n",
            "Training Loss: 0.29317083954811096\n",
            "Training Loss: 0.2919187545776367\n",
            "Training Loss: 0.14824146032333374\n",
            "Training Loss: 0.49271702766418457\n",
            "Training Loss: 0.31271693110466003\n",
            "Training Loss: 0.14048992097377777\n",
            "Training Loss: 0.14540758728981018\n",
            "Training Loss: 0.30683162808418274\n",
            "Training Loss: 0.1228242889046669\n",
            "Training Loss: 0.4843461513519287\n",
            "Training Loss: 0.15329691767692566\n",
            "Training Loss: 0.4890344738960266\n",
            "Training Loss: 0.28476688265800476\n",
            "Training Loss: 0.2818979322910309\n",
            "Training Loss: 0.29296040534973145\n",
            "Training Loss: 0.310983270406723\n",
            "Training Loss: 0.1473206728696823\n",
            "Training Loss: 0.13290317356586456\n",
            "Training Loss: 0.13912127912044525\n",
            "Training Loss: 0.47838014364242554\n",
            "Training Loss: 0.13651694357395172\n",
            "Training Loss: 0.27134376764297485\n",
            "Training Loss: 0.26769277453422546\n",
            "Training Loss: 0.14131291210651398\n",
            "Training Loss: 0.13370737433433533\n",
            "Training Loss: 0.31098252534866333\n",
            "Training Loss: 0.49747833609580994\n",
            "Training Loss: 0.13649477064609528\n",
            "Training Loss: 0.12845462560653687\n",
            "Training Loss: 0.3099299669265747\n",
            "Training Loss: 0.33162087202072144\n",
            "Training Loss: 0.12433550506830215\n",
            "Training Loss: 0.16012676060199738\n",
            "Training Loss: 0.4750441908836365\n",
            "Training Loss: 0.8012974858283997\n",
            "Training Loss: 0.28600379824638367\n",
            "Training Loss: 0.3153051435947418\n",
            "Training Loss: 0.4728434085845947\n",
            "Training Loss: 0.6169492602348328\n",
            "Training Loss: 0.47387418150901794\n",
            "Training Loss: 0.3351782262325287\n",
            "Training Loss: 0.1661381721496582\n",
            "Training Loss: 0.11653392016887665\n",
            "Training Loss: 0.3201918303966522\n",
            "Training Loss: 0.4079411029815674\n",
            "Training Loss: 0.15377222001552582\n",
            "Training Loss: 0.14407703280448914\n",
            "Training Loss: 0.30298876762390137\n",
            "Training Loss: 0.15370652079582214\n",
            "Training Loss: 0.3235416114330292\n",
            "Training Loss: 0.298308789730072\n",
            "Training Loss: 0.0978703424334526\n",
            "Accuracy on Test Data: 88.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnasFXCbNLBb"
      },
      "source": [
        "## **Exercice 4 : Dataloader**\n",
        "\n",
        "En général, il est compliqué de créer soi même ses batchs, surtout si le dataset devient complexe. Nous allons utiliser un dataloader de pytorch, qui lui même crée les batch. Lisez **seulement la section \"dataset\" de cette ressource** :https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel#dataset et créer votre classe dataset. Voici une esquisse de code pour vous aider :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R55rAnKVOFY8"
      },
      "source": [
        "import torch\n",
        "\n",
        "X, y = make_circles(n_samples=200, noise=0.05, factor=0.5, random_state=1)\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, X, y):\n",
        "        # a completer\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return # a completer\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select one sample at position index\n",
        "        # Return in type : np.array\n",
        "\n",
        "        # a compléter\n",
        "        return x, y\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = # splitting a faire\n",
        "train_dataset = Dataset(X_train, y_train)\n",
        "test_dataset = Dataset(X_test, y_test)\n",
        "\n",
        "training_generator = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "test_generator = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "print('Train set length', len(train_dataset))\n",
        "print('Test set length', len(test_dataset))\n",
        "\n",
        "net = Network()\n",
        "\n",
        "# Déclarez les parametres de l'expérience (batch_size, learning rate, ...)\n",
        "# Déclarez la loss et l'optimisation (SGD)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  for j, sample in enumerate(training_generator):\n",
        "    x, y = sample # Generator automatically transform it to tensor\n",
        "    # A compléter\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "for j, sample in enumerate(test_generator):\n",
        "  x, y = sample # Generator automatically transform it to tensor\n",
        "  # A compléter\n",
        "\n",
        "print('\\n Accuracy is ' , # A compléter)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}